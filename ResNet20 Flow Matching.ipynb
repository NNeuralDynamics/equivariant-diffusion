{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d1f399-0abf-4b59-9630-1df4fdce2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./weightflow')  # e.g., './repo_name'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict, namedtuple\n",
    "from typing import NamedTuple\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import logging\n",
    "import copy\n",
    "import traceback\n",
    "from utils.data import sample_gaussian_wsos\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Set up device and logging\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, \n",
    "    format='%(asctime)s %(levelname)s: %(message)s', \n",
    "    level=logging.INFO, \n",
    "    datefmt='%I:%M:%S'\n",
    ")\n",
    "\n",
    "# Simple Bunch class for data containers\n",
    "class Bunch:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "# RESNET MODEL DEFINITION #\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet20():\n",
    "    return ResNet(BasicBlock, [3,3,3])\n",
    "\n",
    "# WEIGHT MATCHING #\n",
    "\n",
    "\n",
    "# PermutationSpec class for defining permutable dimensions\n",
    "class PermutationSpec(NamedTuple):\n",
    "    perm_to_axes: dict\n",
    "    axes_to_perm: dict\n",
    "\n",
    "def permutation_spec_from_axes_to_perm(axes_to_perm: dict) -> PermutationSpec:\n",
    "    perm_to_axes = defaultdict(list)\n",
    "    for wk, axis_perms in axes_to_perm.items():\n",
    "        for axis, perm in enumerate(axis_perms):\n",
    "            if perm is not None:\n",
    "                perm_to_axes[perm].append((wk, axis))\n",
    "    return PermutationSpec(perm_to_axes=dict(perm_to_axes), axes_to_perm=axes_to_perm)\n",
    "\n",
    "def resnet_permutation_spec() -> PermutationSpec:\n",
    "    conv = lambda name, p_in, p_out: {f\"{name}.weight\": (p_out, p_in, None, None)}\n",
    "    \n",
    "    dense = lambda name, p_in, p_out: {\n",
    "        f\"{name}.weight\": (p_out, p_in),\n",
    "        f\"{name}.bias\": (p_out,)\n",
    "    }\n",
    "\n",
    "    easyblock = lambda name, p: {\n",
    "        **conv(f\"{name}.conv1\", p, f\"P_{name}_inner\"),\n",
    "        **conv(f\"{name}.conv2\", f\"P_{name}_inner\", p),\n",
    "    }\n",
    "\n",
    "    shortcutblock = lambda name, p_in, p_out: {\n",
    "        **conv(f\"{name}.conv1\", p_in, f\"P_{name}_inner\"),\n",
    "        **conv(f\"{name}.conv2\", f\"P_{name}_inner\", p_out),\n",
    "        **conv(f\"{name}.shortcut.0\", p_in, p_out),\n",
    "    }\n",
    "\n",
    "    return permutation_spec_from_axes_to_perm({\n",
    "        **conv(\"conv1\", None, \"P_bg0\"),\n",
    "\n",
    "        **easyblock(\"layer1.0\", \"P_bg0\"),\n",
    "        **easyblock(\"layer1.1\", \"P_bg0\"),\n",
    "        **easyblock(\"layer1.2\", \"P_bg0\"),\n",
    "\n",
    "        **shortcutblock(\"layer2.0\", \"P_bg0\", \"P_bg1\"),\n",
    "        **easyblock(\"layer2.1\", \"P_bg1\"),\n",
    "        **easyblock(\"layer2.2\", \"P_bg1\"),\n",
    "\n",
    "        **shortcutblock(\"layer3.0\", \"P_bg1\", \"P_bg2\"),\n",
    "        **easyblock(\"layer3.1\", \"P_bg2\"),\n",
    "        **easyblock(\"layer3.2\", \"P_bg2\"),\n",
    "\n",
    "        **dense(\"linear\", \"P_bg2\", None),\n",
    "    })\n",
    "\n",
    "def get_permuted_param(ps: PermutationSpec, perm, k: str, params, except_axis=None):\n",
    "    \"\"\"Get parameter k from params, with permutations applied.\"\"\"\n",
    "    w = params[k]\n",
    "    for axis, p in enumerate(ps.axes_to_perm[k]):\n",
    "        # Skip the axis we're trying to permute\n",
    "        if axis == except_axis:\n",
    "            continue\n",
    "\n",
    "        # None indicates no permutation for that axis\n",
    "        if p is not None:\n",
    "            w = torch.index_select(w, axis, torch.tensor(perm[p], device=w.device))\n",
    "\n",
    "    return w\n",
    "\n",
    "def apply_permutation(ps: PermutationSpec, perm, params):\n",
    "    \"\"\"Apply permutation to params\"\"\"\n",
    "    return {k: get_permuted_param(ps, perm, k, params) for k in params.keys()}\n",
    "\n",
    "def weight_matching(ps: PermutationSpec, params_a, params_b, max_iter=100, init_perm=None, silent=True, device=None):\n",
    "    \"\"\"Find permutation of params_b to make them match params_a.\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    params_a = {k: v.to(device) for k, v in params_a.items()}\n",
    "    params_b = {k: v.to(device) for k, v in params_b.items()}\n",
    "\n",
    "    # Get permutation sizes\n",
    "    perm_sizes = {p: params_a[axes[0][0]].shape[axes[0][1]] \n",
    "                  for p, axes in ps.perm_to_axes.items()}\n",
    "    \n",
    "    # Initialize permutations to identity if none provided\n",
    "    if init_perm is None:\n",
    "        perm = {p: torch.arange(n, device=device) for p, n in perm_sizes.items()}\n",
    "    else:\n",
    "        perm = {p: v.to(device) for p, v in init_perm.items()}\n",
    "        \n",
    "    perm_names = list(perm.keys())\n",
    "    \n",
    "    # Use a random number generator with a fixed seed for reproducibility\n",
    "    rng = np.random.RandomState(42)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        progress = False\n",
    "        \n",
    "        # Shuffle the order of permutations to update\n",
    "        for p_ix in rng.permutation(len(perm_names)):\n",
    "            p = perm_names[p_ix]\n",
    "            n = perm_sizes[p]\n",
    "            \n",
    "            # Initialize cost matrix\n",
    "            A = torch.zeros((n, n), device=device)\n",
    "            \n",
    "            # Fill in cost matrix based on all parameters affected by this permutation\n",
    "            for wk, axis in ps.perm_to_axes[p]:\n",
    "                w_a = params_a[wk]\n",
    "                w_b = get_permuted_param(ps, perm, wk, params_b, except_axis=axis)\n",
    "\n",
    "                w_a = w_a.moveaxis(axis, 0).reshape((n, -1))\n",
    "                w_b = w_b.moveaxis(axis, 0).reshape((n, -1))\n",
    "\n",
    "                A += w_a @ w_b.T\n",
    "\n",
    "            # Solve the linear assignment problem\n",
    "            ri, ci = linear_sum_assignment(A.detach().cpu().numpy(), maximize=True)\n",
    "            assert (ri == np.arange(len(ri))).all()\n",
    "\n",
    "            # Calculate improvement\n",
    "            eye_old = torch.eye(n, device=device)[perm[p]]\n",
    "            eye_new = torch.eye(n, device=device)[ci]\n",
    "\n",
    "            oldL = torch.tensordot(A, eye_old, dims=([0, 1], [0, 1]))\n",
    "            newL = torch.tensordot(A, eye_new, dims=([0, 1], [0, 1]))\n",
    "\n",
    "            if not silent and newL > oldL + 1e-12:\n",
    "                logging.info(f\"{iteration}/{p}: {newL.item() - oldL.item()}\")\n",
    "\n",
    "            progress = progress or newL > oldL + 1e-12\n",
    "\n",
    "            perm[p] = torch.tensor(ci, device=device)\n",
    "\n",
    "        if not progress:\n",
    "            break\n",
    "\n",
    "    return perm\n",
    "\n",
    "#  WEIGHT SPACE OBJECT  #\n",
    "\n",
    "class WeightSpaceObject:\n",
    "    def __init__(self, weights, biases):\n",
    "        self.weights = weights if isinstance(weights, tuple) else tuple(weights)\n",
    "        self.biases = biases if isinstance(biases, tuple) else tuple(biases)\n",
    "        \n",
    "    def flatten(self, device=None):\n",
    "        \"\"\"Flatten weights and biases into a single vector\"\"\"\n",
    "        flat = torch.cat(\n",
    "            [w.reshape(-1) for w in self.weights] +\n",
    "            [b.reshape(-1) for b in self.biases]\n",
    "        )\n",
    "        return flat.to(device) if device else flat\n",
    "\n",
    "    @classmethod\n",
    "    def from_flat(cls, flat, layer_shapes, bias_shapes, device=None):\n",
    "        flat = flat.to(device) if device else flat\n",
    "        sizes = [np.prod(s) for s in layer_shapes + bias_shapes]\n",
    "    \n",
    "        if flat.numel() != sum(sizes):\n",
    "            raise ValueError(f\"Expected flat vector of length {sum(sizes)}, got {flat.numel()}\")\n",
    "    \n",
    "        parts = []\n",
    "        start = 0\n",
    "        for size in sizes:\n",
    "            parts.append(flat[start:start+size])\n",
    "            start += size\n",
    "    \n",
    "        weights = [parts[i].reshape(layer_shapes[i]) for i in range(len(layer_shapes))]\n",
    "        biases = [parts[len(layer_shapes) + i].reshape(bias_shapes[i]) for i in range(len(bias_shapes))] if bias_shapes else []\n",
    "\n",
    "        return cls(weights, biases).to(device)\n",
    "\n",
    "\n",
    "    def to(self, device):\n",
    "        weights = tuple(w.to(device) for w in self.weights)\n",
    "        biases = tuple(b.to(device) for b in self.biases)\n",
    "        return WeightSpaceObject(weights, biases)\n",
    "\n",
    "    def map(self, fn):\n",
    "            new_weights = tuple(fn(w) for w in self.weights)\n",
    "            new_biases = tuple(fn(b) for b in self.biases)\n",
    "            return WeightSpaceObject(new_weights, new_biases)\n",
    "\n",
    "def create_zero_wso(template_wso):\n",
    "    zero_weights = [torch.zeros_like(w) for w in template_wso.weights]\n",
    "    zero_biases = [torch.zeros_like(b) for b in template_wso.biases]\n",
    "    return WeightSpaceObject(zero_weights, zero_biases)\n",
    "\n",
    "\n",
    "# EVALUATION UTILS #\n",
    "\n",
    "def get_test_loader(batch_size=128):\n",
    "    \"\"\"Create a test data loader for CIFAR-10\"\"\"\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # why tuning by RGB channels? \n",
    "    ])\n",
    "\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    # changed all num_workers to 0 for Jupyter reasons\n",
    "    return test_loader\n",
    "\n",
    "def evaluate(model, test_loader, device=None):\n",
    "    \"\"\"Evaluate model accuracy on test data\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# FLOW MATCHING (SIMPLIFIED) #\n",
    "\n",
    "class SimpleCFM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sourceloader,\n",
    "        targetloader,\n",
    "        model,\n",
    "        fm_type=\"vanilla\",\n",
    "        mode=\"velocity\",\n",
    "        t_dist=\"uniform\",\n",
    "        device=None,\n",
    "        normalize_pred=False,\n",
    "        geometric=False,\n",
    "    ):\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        self.t_dist = t_dist\n",
    "        self.sourceloader = sourceloader\n",
    "        self.targetloader = targetloader\n",
    "        self.model = model\n",
    "        self.fm_type = fm_type\n",
    "        self.mode = mode\n",
    "        self.sigma = 0.001\n",
    "        self.normalize_pred = normalize_pred\n",
    "        self.geometric = geometric\n",
    "        self.metrics = {\"train_loss\": [], \"time\": [], \"grad_norm\": [], \"flow_norm\": [], \"true_norm\": []}\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def sample_from_loader(self, loader):\n",
    "        \"\"\"Sample from a dataloader with proper error handling\"\"\"\n",
    "        try:\n",
    "            if not hasattr(loader, '_iterator') or loader._iterator is None:\n",
    "                loader._iterator = iter(loader)\n",
    "            try:\n",
    "                batch = next(loader._iterator)\n",
    "            except StopIteration:\n",
    "                loader._iterator = iter(loader)\n",
    "                batch = next(loader._iterator)\n",
    "            return batch[0]  # Return just the tensor, not the tuple\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Error sampling from loader: {str(e)}\")\n",
    "            # Return a default tensor if sampling fails\n",
    "            return torch.zeros(loader.batch_size, loader.dataset[0][0].shape[0], device=self.device)\n",
    "\n",
    "    # def sample_from_loader(self, loader):\n",
    "    #     \"\"\"\n",
    "    #     Simplified function to sample a single batch from a PyTorch DataLoader.\n",
    "    #     Includes debugging and logging for common issues.\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         # Check if the loader has any data\n",
    "    #         if len(loader.dataset) == 0: # Check if the dataset is empty\n",
    "    #             logging.warning(\"DataLoader's dataset is empty. Cannot sample.\") #\n",
    "    #             return None # Or raise an error as appropriate\n",
    "            \n",
    "    #         # Get the iterator for the DataLoader\n",
    "    #         # Consider num_workers=0 for initial debugging as it can provide clearer errors\n",
    "    #         # if loader.num_workers > 0 and torch.utils.data.get_worker_info() is not None:\n",
    "    #         #     logging.debug(\"Running in a DataLoader worker process. Iteration might behave differently.\") \n",
    "    \n",
    "    #         batch = next(iter(loader)) # Directly create and get the first item from the iterator\n",
    "            \n",
    "    #         # Log the shape of the sampled batch for verification\n",
    "    #         if isinstance(batch, (tuple, list)):\n",
    "    #             logging.info(f\"Successfully sampled a batch (tuple/list). First element shape: {batch[0].shape}\")\n",
    "    #         else:\n",
    "    #             logging.info(f\"Successfully sampled a batch (single tensor). Shape: {batch.shape}\")\n",
    "            \n",
    "    #         return batch\n",
    "\n",
    "    # except StopIteration:\n",
    "    #     # This occurs if the DataLoader is exhausted, meaning no more batches are available\n",
    "    #     logging.warning(\"DataLoader exhausted all samples (StopIteration). Consider resetting the iterator or using a cycle.\") #\n",
    "    #     return None # Or handle according to your needs (e.g., reset iterator for another epoch)\n",
    "    # except Exception as e:\n",
    "    #     # Catch any other unexpected errors during sampling\n",
    "    #     logging.error(f\"Error sampling from DataLoader: {e}\", exc_info=True) # Log the full traceback\n",
    "    #     return None # Return None or a default tensor as per your requirement\n",
    "\n",
    "    \n",
    "    def sample_time_and_flow(self):\n",
    "        \"\"\"Sample time, start and end points, and intermediate x_t\"\"\"\n",
    "        x0 = self.sample_from_loader(self.sourceloader)\n",
    "        x1 = self.sample_from_loader(self.targetloader)\n",
    "        \n",
    "        # Ensure consistent batch size\n",
    "        batch_size = min(x0.size(0), x1.size(0))\n",
    "        x0 = x0[:batch_size].to(self.device)\n",
    "        x1 = x1[:batch_size].to(self.device)\n",
    "        \n",
    "        if self.t_dist == \"uniform\":\n",
    "            t = torch.rand(batch_size).to(self.device)\n",
    "        elif self.t_dist == \"beta\":\n",
    "            alpha, beta = torch.tensor(1.0), torch.tensor(2.0)\n",
    "            t = torch.distributions.Beta(alpha, beta).sample((batch_size,)).to(self.device)\n",
    "        \n",
    "        t_pad = t.reshape(-1, *([1] * (x0.dim() - 1)))\n",
    "        \n",
    "        mu_t = (1 - t_pad) * x0 + t_pad * x1\n",
    "        sigma_pad = torch.tensor(self.sigma).reshape(-1, *([1] * (x0.dim() - 1))).to(self.device)\n",
    "        xt = mu_t + sigma_pad * torch.randn_like(x0).to(self.device)\n",
    "        ut = x1 - x0\n",
    "        \n",
    "        t = t.unsqueeze(-1)\n",
    "        \n",
    "        return Bunch(t=t, x0=x0, xt=xt, x1=x1, ut=ut, eps=0, lambda_t=0, batch_size=batch_size)\n",
    "    \n",
    "    def forward(self, flow):\n",
    "        \"\"\"Forward pass through the model with proper error handling\"\"\"\n",
    "        try:\n",
    "            # Forward pass directly through the model\n",
    "            flow_pred = self.model(flow.xt, flow.t)\n",
    "            return None, flow_pred\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Error in forward pass: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            # Return zero tensors as fallback\n",
    "            return None, torch.zeros_like(flow.ut)\n",
    "    \n",
    "    def loss_fn(self, flow_pred, flow):\n",
    "        \"\"\"Compute loss between predicted and true flows\"\"\"\n",
    "        if self.mode == \"target\":\n",
    "            l_flow = torch.mean((flow_pred.squeeze() - flow.x1) ** 2)\n",
    "        elif self.mode == \"velocity\":\n",
    "            l_flow = torch.mean((flow_pred.squeeze() - flow.ut) ** 2)\n",
    "        elif self.fm_type == \"ot\":\n",
    "            l_flow = torch.mean((flow_pred.squeeze() - flow.ut) ** 2)\n",
    "        else:\n",
    "            # Fallback to velocity mode if unknown\n",
    "            l_flow = torch.mean((flow_pred.squeeze() - flow.ut) ** 2)\n",
    "        return None, l_flow\n",
    "    \n",
    "    def map(self, x0, n_steps=20, return_traj=False, noise_scale=0.001):\n",
    "        \"\"\"Map points using the flow model to generate new weights\"\"\"\n",
    "        if self.best_model_state is not None:\n",
    "            current_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "           \n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        batch_size, flat_dim = x0.size()\n",
    "        traj = [] if return_traj else None\n",
    "\n",
    "        # Create time steps for Euler integration\n",
    "        times = torch.linspace(0, 1, n_steps).to(self.device)\n",
    "        dt = times[1] - times[0]  # Time step size\n",
    "\n",
    "        # Initialize result with starting point\n",
    "        xt = x0.clone()\n",
    "\n",
    "        for t in times[:-1]: \n",
    "            if return_traj:\n",
    "                traj.append(xt.detach().clone())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Create time tensor with correct shape\n",
    "                t_tensor = torch.ones(batch_size, 1).to(self.device) * t\n",
    "\n",
    "                try:\n",
    "                    pred = self.model(xt, t_tensor)\n",
    "\n",
    "                    if pred.dim() > 2:\n",
    "                        pred = pred.squeeze(-1)\n",
    "\n",
    "                    if self.mode == \"velocity\":\n",
    "                        vt = pred\n",
    "                    else:  # mode == \"target\"\n",
    "                        vt = pred - xt\n",
    "\n",
    "                    xt = xt + vt * dt\n",
    "\n",
    "                    if t > 0.8:\n",
    "                        xt = xt + torch.randn_like(xt) * noise_scale\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logging.info(f\"Error during mapping at t={t}: {str(e)}\")\n",
    "\n",
    "        if return_traj:\n",
    "            traj.append(xt.detach().clone())\n",
    "\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(current_state)\n",
    "            \n",
    "        self.model.train()\n",
    "\n",
    "        return traj if return_traj else xt\n",
    "       \n",
    "    \n",
    "    def vector_field(self, xt, t):\n",
    "        \"\"\"Compute vector field at point xt and time t\"\"\"\n",
    "        # Forward pass through model\n",
    "        _, pred = self.forward(Bunch(xt=xt, t=t, batch_size=xt.size(0)))\n",
    "        \n",
    "        if self.mode == \"velocity\":\n",
    "            vt = pred\n",
    "        elif self.mode == \"target\":\n",
    "            vt = pred - xt\n",
    "        \n",
    "        return vt\n",
    "    \n",
    "    def train(self, n_iters=10, optimizer=None, sigma=0.001, patience=1e99, log_freq=5):\n",
    "        \"\"\"Train the flow model\"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.metrics = {\"train_loss\": [], \"time\": [], \"grad_norm\": [], \"flow_norm\": [], \"true_norm\": []}\n",
    "        last_loss = 1e99\n",
    "        patience_count = 0\n",
    "        # I have a theory that the LR scheduler might be reducing too early\n",
    "        # scheduler = CosineAnnealingLR(optimizer, T_max=n_iters, eta_min=1e-4)\n",
    "        \n",
    "        pbar = tqdm(range(n_iters), desc=\"Training steps\")\n",
    "        for i in pbar:\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                flow = self.sample_time_and_flow()\n",
    "                _, flow_pred = self.forward(flow)\n",
    "                _, loss = self.loss_fn(flow_pred, flow)\n",
    "                \n",
    "                if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    # scheduler.step()\n",
    "\n",
    "                    if i % 5 == 0 or i == n_iters: # overkill\n",
    "                        # end_time = time.time()\n",
    "                        print(f\"[Iter {i}], Loss = {loss.item():.6f}, Took: [to-do]s, Saving CIFAR10_linear_cfm_{i}.pth\")\n",
    "                        \n",
    "                        # start_time = end_time\n",
    "                        \n",
    "                        checkpoint_dir = 'checkpoints'\n",
    "                        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                        ckpt_path = os.path.join(checkpoint_dir, f'CIFAR10_linear_cfm_{i}.pth') # changed!\n",
    "                        torch.save(self.model.state_dict(), ckpt_path)\n",
    "                        print(f\"Model saved to: CIFAR10_linear_cfm_{i}.pth - {i} iters took [to-do]s\")\n",
    "                \n",
    "                    # Save best model\n",
    "                    if loss.item() < self.best_loss:\n",
    "                        self.best_loss = loss.item()\n",
    "                        self.best_model_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "                else:\n",
    "                    logging.info(f\"Skipping step {i} due to invalid loss: {loss.item()}\")\n",
    "                    continue\n",
    "                \n",
    "                # early stopping\n",
    "                if loss.item() > last_loss:\n",
    "                    patience_count += 1\n",
    "                    if patience_count >= patience:\n",
    "                        logging.info(f\"Early stopping at iteration {i}\")\n",
    "                        break\n",
    "                else:\n",
    "                    patience_count = 0\n",
    "                    \n",
    "                last_loss = loss.item()\n",
    "                \n",
    "                if i % log_freq == 0:\n",
    "                    train_loss_val = loss.item()\n",
    "                    \n",
    "                    true_tensor = flow.ut if self.mode == \"velocity\" else flow.x1\n",
    "                    grad_norm = self.get_grad_norm()\n",
    "                    self.metrics[\"train_loss\"].append(train_loss_val)\n",
    "                    self.metrics[\"flow_norm\"].append(flow_pred.norm(p=2, dim=1).mean().item())\n",
    "                    self.metrics[\"time\"].append(flow.t.mean().item())\n",
    "                    self.metrics[\"true_norm\"].append(true_tensor.norm(p=2, dim=1).mean().item())\n",
    "                    self.metrics[\"grad_norm\"].append(grad_norm)\n",
    "                    \n",
    "                    pbar.set_description(f\"Iters [loss {train_loss_val:.6f}, ∇ norm {grad_norm:.6f}]\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.info(f\"Error during training iteration {i}: {str(e)}\")\n",
    "                traceback.print_exc()\n",
    "                break\n",
    "    \n",
    "    def get_grad_norm(self):\n",
    "        \"\"\"Compute gradient norm\"\"\"\n",
    "        total = 0\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total += param_norm.item() ** 2\n",
    "        total = total**0.5\n",
    "        return total\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot training metrics\"\"\"\n",
    "        labels = list(self.metrics.keys())\n",
    "        lists = list(self.metrics.values())\n",
    "        n = len(lists)\n",
    "        \n",
    "        fig, axs = plt.subplots(1, n, figsize=(3 * n, 3))\n",
    "        for i, (label, lst) in enumerate(zip(labels, lists)):\n",
    "            axs[i].plot(lst)\n",
    "            axs[i].grid()\n",
    "            axs[i].title.set_text(label)\n",
    "            if label == \"train_loss\":\n",
    "                axs[i].set_yscale(\"log\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# FLOW MODEL ARCHITECTURE #\n",
    "\n",
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim):\n",
    "#         super().__init__()\n",
    "#         self.linear = nn.Linear(in_dim, out_dim)\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.residual = (in_dim == out_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.activation(self.linear(x))\n",
    "#         if self.residual:\n",
    "#             return out + x\n",
    "#         return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.norm = nn.LayerNorm(out_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.residual = (in_dim == out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.norm( self.activation(self.linear(x)) )\n",
    "        if self.residual:\n",
    "            return out + x\n",
    "        return out\n",
    "\n",
    "class TimeConditionedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim + 1  # +1 for time\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = self.input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(ResidualBlock(prev_dim, hidden_dim))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(prev_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs, t):\n",
    "        x = torch.cat([inputs, t], dim=1)\n",
    "        x = self.hidden_layers(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# MAIN FUNCTIONS #\n",
    "\n",
    "def get_permuted_models_data(ref_point=0, model_dir=\"dummy_imagenet_resnet_models\", num_models=10, device=None):\n",
    "    \"\"\"Apply weight matching to align models with a reference model\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    ref_model = ResNet20()\n",
    "    ref_model_path = f\"{model_dir}/resnet_weights_{ref_point}.pt\"\n",
    "    \n",
    "    try:\n",
    "        ref_model.load_state_dict(torch.load(ref_model_path, map_location=device))\n",
    "        ref_model = ref_model.to(device)\n",
    "        logging.info(f\"Loaded reference model from {ref_model_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference model: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    ps = resnet_permutation_spec()\n",
    "    \n",
    "    params_a = {k: v.clone().detach() for k, v in ref_model.state_dict().items() \n",
    "               if k in ps.axes_to_perm}\n",
    "    \n",
    "    permuted_models = []\n",
    "\n",
    "    for i in tqdm(range(num_models), desc=\"Processing models\"):\n",
    "        if i == ref_point:\n",
    "            continue\n",
    "        \n",
    "        model_path = f\"{model_dir}/resnet_weights_{i}.pt\"\n",
    "        if not os.path.exists(model_path):\n",
    "            logging.info(f\"Skipping model {i} - file not found\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load model B\n",
    "            model_b = ResNet20()\n",
    "            model_b.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            model_b = model_b.to(device)\n",
    "\n",
    "            # Extract params and buffers\n",
    "            params_b = {k: v.clone().detach() for k, v in model_b.state_dict().items() \n",
    "                       if k in ps.axes_to_perm}\n",
    "            \n",
    "            # Perform weight matching directly in PyTorch\n",
    "            perm = weight_matching(ps, params_a, params_b, device=device)\n",
    "            \n",
    "            # Apply permutation\n",
    "            permuted_params_b = apply_permutation(ps, perm, params_b)\n",
    "            \n",
    "            reconstructed_model = copy.deepcopy(model_b)\n",
    "            state_dict = reconstructed_model.state_dict()\n",
    "            \n",
    "            for k in permuted_params_b:\n",
    "                state_dict[k] = permuted_params_b[k]\n",
    "            \n",
    "            reconstructed_model.load_state_dict(state_dict)\n",
    "            reconstructed_model = reconstructed_model.to(device)\n",
    "            \n",
    "            # Evaluate accuracy before adding to list\n",
    "            # test_loader = get_test_loader()\n",
    "            # accuracy = evaluate(reconstructed_model, test_loader)\n",
    "            # logging.info(f\"Model {i} accuracy after matching: {accuracy:.2f}%\")\n",
    "            \n",
    "            permuted_models.append(reconstructed_model)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing model {i}: {e}\")\n",
    "            continues\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    logging.info(f\"Processed {len(permuted_models)} models successfully\")\n",
    "    return ref_model, permuted_models\n",
    "\n",
    "def zero_like_wso(wso):\n",
    "    zero_weights = tuple(torch.zeros_like(w) for w in wso.weights)\n",
    "    zero_biases = tuple(torch.zeros_like(b) for b in wso.biases)\n",
    "    return WeightSpaceObject(zero_weights, zero_biases)\n",
    "\n",
    "def get_random_pytorch_initialized_wsos(n, device):\n",
    "    wsos = []\n",
    "    for i in range(n):\n",
    "        torch.manual_seed(i)\n",
    "        model = ResNet20().to(device)\n",
    "        weights = (\n",
    "            model.conv1.weight.data.clone(),\n",
    "            model.layer1[0].conv1.weight.data.clone(),\n",
    "            model.layer1[0].conv2.weight.data.clone(),\n",
    "            model.layer1[1].conv1.weight.data.clone(),\n",
    "            model.layer1[1].conv2.weight.data.clone(),\n",
    "            model.layer1[2].conv1.weight.data.clone(),\n",
    "            model.layer1[2].conv2.weight.data.clone(),\n",
    "            model.layer2[0].shortcut[0].weight.data.clone(),\n",
    "            model.layer2[0].conv1.weight.data.clone(),\n",
    "            model.layer2[0].conv2.weight.data.clone(),\n",
    "            model.layer2[1].conv1.weight.data.clone(),\n",
    "            model.layer2[1].conv2.weight.data.clone(),\n",
    "            model.layer2[2].conv1.weight.data.clone(),\n",
    "            model.layer2[2].conv2.weight.data.clone(),\n",
    "            model.layer3[0].shortcut[0].weight.data.clone(),\n",
    "            model.layer3[0].conv1.weight.data.clone(),\n",
    "            model.layer3[0].conv2.weight.data.clone(),\n",
    "            model.layer3[1].conv1.weight.data.clone(),\n",
    "            model.layer3[1].conv2.weight.data.clone(),\n",
    "            model.layer3[2].conv1.weight.data.clone(),\n",
    "            model.layer3[2].conv2.weight.data.clone(),\n",
    "            model.linear.weight.data.clone()\n",
    "        )\n",
    "        biases = (model.linear.bias.data.clone(),)\n",
    "        wso = WeightSpaceObject(weights, biases)\n",
    "        wsos.append(wso)\n",
    "    return wsos\n",
    "\n",
    "\n",
    "\n",
    "def train_and_generate_resnet_weights(\n",
    "    num_models=501,\n",
    "    batch_size=16,\n",
    "    n_samples=10,\n",
    "    n_iters = 10,\n",
    "    device=None\n",
    "):\n",
    "\n",
    "\n",
    "    # All weight shapes\n",
    "    layer_shapes = [\n",
    "        (16, 3, 3, 3),  # conv1\n",
    "    \n",
    "        # layer1 (3 BasicBlocks)\n",
    "        (16, 16, 3, 3), (16, 16, 3, 3),  # block 0\n",
    "        (16, 16, 3, 3), (16, 16, 3, 3),  # block 1\n",
    "        (16, 16, 3, 3), (16, 16, 3, 3),  # block 2\n",
    "    \n",
    "        # layer2 (3 BasicBlocks)\n",
    "        (32, 16, 1, 1),                  # shortcut for layer2[0]\n",
    "        (32, 16, 3, 3), (32, 32, 3, 3),  # block 0\n",
    "        (32, 32, 3, 3), (32, 32, 3, 3),  # block 1\n",
    "        (32, 32, 3, 3), (32, 32, 3, 3),  # block 2\n",
    "    \n",
    "        # layer3 (3 BasicBlocks)\n",
    "        (64, 32, 1, 1),                  # shortcut for layer3[0]\n",
    "        (64, 32, 3, 3), (64, 64, 3, 3),  # block 0\n",
    "        (64, 64, 3, 3), (64, 64, 3, 3),  # block 1\n",
    "        (64, 64, 3, 3), (64, 64, 3, 3),  # block 2\n",
    "    \n",
    "        (10, 64)  # final linear\n",
    "    ]\n",
    "\n",
    "    bias_shapes = [\n",
    "        (10,)\n",
    "    ]\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    logging.info(\"Getting permuted models...\")\n",
    "    ref_model, permuted_models = get_permuted_models_data(num_models=num_models, device=device)\n",
    "\n",
    "\n",
    "    logging.info(\"Converting models to WeightSpaceObjects...\")\n",
    "    weights_list = []\n",
    "    for model in tqdm(permuted_models):\n",
    "        weights = (\n",
    "            model.conv1.weight.data.clone(),\n",
    "            model.layer1[0].conv1.weight.data.clone(),\n",
    "            model.layer1[0].conv2.weight.data.clone(),\n",
    "            model.layer1[1].conv1.weight.data.clone(),\n",
    "            model.layer1[1].conv2.weight.data.clone(),\n",
    "            model.layer1[2].conv1.weight.data.clone(),\n",
    "            model.layer1[2].conv2.weight.data.clone(),\n",
    "            model.layer2[0].shortcut[0].weight.data.clone(),\n",
    "            model.layer2[0].conv1.weight.data.clone(),\n",
    "            model.layer2[0].conv2.weight.data.clone(),\n",
    "            model.layer2[1].conv1.weight.data.clone(),\n",
    "            model.layer2[1].conv2.weight.data.clone(),\n",
    "            model.layer2[2].conv1.weight.data.clone(),\n",
    "            model.layer2[2].conv2.weight.data.clone(),\n",
    "            model.layer3[0].shortcut[0].weight.data.clone(),\n",
    "            model.layer3[0].conv1.weight.data.clone(),\n",
    "            model.layer3[0].conv2.weight.data.clone(),\n",
    "            model.layer3[1].conv1.weight.data.clone(),\n",
    "            model.layer3[1].conv2.weight.data.clone(),\n",
    "            model.layer3[2].conv1.weight.data.clone(),\n",
    "            model.layer3[2].conv2.weight.data.clone(),\n",
    "            model.linear.weight.data.clone()\n",
    "        )\n",
    "        \n",
    "        biases = (\n",
    "            model.linear.bias.data.clone(),\n",
    "        )\n",
    "        \n",
    "        wso = WeightSpaceObject(weights, biases)\n",
    "        weights_list.append(wso)\n",
    "    \n",
    "   \n",
    "    flat_target_weights = torch.stack([wso.flatten(device) for wso in weights_list])\n",
    "    flat_dim = flat_target_weights.shape[1]\n",
    "\n",
    "    logging.info(f\"Created {len(weights_list)} target weight configurations\")\n",
    "\n",
    "    # for experimenting by generating source wsos from guassian noise\n",
    "    source_std = 0.01 #for noise we had 0.001\n",
    "\n",
    "    # Uncomment this part to get source from random noise\n",
    "    # flat_source_weights = torch.randn(len(weights_list), flat_dim, device=device) * source_std\n",
    "    mean_zero_wso = zero_like_wso(weights_list[0])\n",
    "    source_wsos = sample_gaussian_wsos(mean=mean_zero_wso, std=source_std, n=len(weights_list))\n",
    "    flat_source_weights = torch.stack([wso.flatten(device) for wso in source_wsos])\n",
    "\n",
    "    # Uncomment for experimenting by generating source wsos from kaimings Initialization\n",
    "    # source_wsos = get_random_pytorch_initialized_wsos(n=len(weights_list), device = device)\n",
    "    # flat_source_weights = torch.stack([wso.flatten(device) for wso in source_wsos])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    source_dataset = TensorDataset(flat_source_weights)\n",
    "\n",
    "    print(f\" __len__(): {source_dataset.__len__()}\")\n",
    "    print(f\"__getitem__(0): {source_dataset.__getitem__(0)}\")\n",
    "    \n",
    "    print(f\"Source dataset len: {len(source_dataset)}\")\n",
    "    print(f\"Source dataset 0 len: {len(source_dataset[0])}\")\n",
    "    print(f\"Source dataset 0 len: {len(source_dataset[0][0])}\")\n",
    "    print(f\"Source dataset 0 shape: {source_dataset[0][0].shape}\")\n",
    "\n",
    "    \n",
    "    target_dataset = TensorDataset(flat_target_weights)\n",
    "\n",
    "    print(f\"Target dataset len: {len(target_dataset)}\")\n",
    "    print(f\"Target dataset 0 len: {len(target_dataset[0])}\")\n",
    "    print(f\"Target dataset 0 len: {len(target_dataset[0][0])}\")\n",
    "    print(f\"Target dataset 0 shape: {target_dataset[0][0].shape}\")\n",
    "\n",
    "    sourceloader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True, drop_last=False) # drop_last=True\n",
    "    \n",
    "    # test = next(iter(sourceloader))\n",
    "    # print(f\"Source loader: {test.shape}\")\n",
    "    \n",
    "    targetloader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True, drop_last=False) # drop_last=True\n",
    "\n",
    "    # Flow model\n",
    "    flow_model = TimeConditionedMLP(\n",
    "        input_dim=flat_dim,\n",
    "        hidden_dims=[2048, 2048, 2048, 2048],\n",
    "        output_dim=flat_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    logging.info(f\"ResNet20 params: {sum(p.numel() for p in ResNet20().parameters()):,}\")\n",
    "    logging.info(f\"Flow model params: {sum(p.numel() for p in flow_model.parameters()):,}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(flow_model.parameters(), lr=0.0001, weight_decay = 1e-6)\n",
    "\n",
    "    # Flow Matching\n",
    "    cfm =  SimpleCFM(\n",
    "        sourceloader=sourceloader,\n",
    "        targetloader=targetloader,\n",
    "        model=flow_model,\n",
    "        fm_type=\"ot\",\n",
    "        mode=\"ot\",\n",
    "        t_dist=\"beta\",\n",
    "        device=device,\n",
    "        normalize_pred=True,\n",
    "        geometric=True,\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Training flow model on Random Noise...\")\n",
    "    cfm.train(n_iters=n_iters, optimizer=optimizer, log_freq=100)\n",
    "\n",
    "    # Sampling\n",
    "    logging.info(\"Generating new ResNet weights...\")\n",
    "    source_wsos = get_random_pytorch_initialized_wsos(n_samples, device=device)\n",
    "    random_flat = torch.stack([\n",
    "        wso.flatten(device) + torch.randn_like(wso.flatten()) * source_std\n",
    "        for wso in source_wsos\n",
    "    ])\n",
    "    \n",
    "    random_flat = torch.randn(n_samples, flat_dim, device=device) * source_std\n",
    "    new_weights_flat = cfm.map(\n",
    "        random_flat, \n",
    "        n_steps=100, # tune this?\n",
    "        noise_scale=0.0005\n",
    "    )\n",
    "\n",
    "    test_loader = get_test_loader(batch_size=128)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        new_wso = WeightSpaceObject.from_flat(\n",
    "            new_weights_flat[i], \n",
    "            layer_shapes=layer_shapes, \n",
    "            bias_shapes = bias_shapes,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        model = ResNet20()\n",
    "\n",
    "        idx = 0\n",
    "        # conv1\n",
    "        model.conv1.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "        \n",
    "        # layer1 (3 BasicBlocks)\n",
    "        for block in range(3):\n",
    "            model.layer1[block].conv1.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "            model.layer1[block].conv2.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "        \n",
    "        # layer2 (3 BasicBlocks)\n",
    "        for block in range(3):\n",
    "            if block == 0:\n",
    "                # Assign shortcut (downsample) conv for first block of layer2\n",
    "                model.layer2[block].shortcut[0].weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "            model.layer2[block].conv1.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "            model.layer2[block].conv2.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "        \n",
    "        # layer3 (3 BasicBlocks)\n",
    "        for block in range(3):\n",
    "            if block == 0:\n",
    "                # Assign shortcut (downsample) conv for first block of layer3\n",
    "                model.layer3[block].shortcut[0].weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "            model.layer3[block].conv1.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "            model.layer3[block].conv2.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "        \n",
    "        # Final linear layer\n",
    "        model.linear.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "        model.linear.bias.data = new_wso.biases[0].clone()\n",
    "\n",
    "            \n",
    "        model = model.to(device)\n",
    "\n",
    "        try:\n",
    "            acc = evaluate(model, test_loader, device)\n",
    "            logging.info(f\"Generated model {i} accuracy: {acc:.8f}%\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to evaluate model {i}: {e}\")\n",
    "\n",
    "    logging.info(\"Generation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf02a62-91e4-4612-884c-b0ad4909006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet20():\n",
    "    return ResNet(BasicBlock, [3,3,3])\n",
    "\n",
    "\n",
    "def get_train_loader(batch_size = 128):\n",
    "    \"\"\"Create a training data loader for CIFAR-10\"\"\"\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # why tuning by RGB channels? \n",
    "    ])\n",
    "\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_test\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return test_loader\n",
    "\n",
    "def train(model, train_loader, epochs=3, device = None):\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_test_loader(batch_size=128):\n",
    "    \"\"\"Create a test data loader for CIFAR-10\"\"\"\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # why tuning by RGB channels? \n",
    "    ])\n",
    "\n",
    "    test_set = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return test_loader\n",
    "\n",
    "def evaluate(model, test_loader, device=None):\n",
    "    \"\"\"Evaluate model accuracy on test data\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33a7d4-5ab1-4e04-ada6-258068623e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9005336-f067-492e-8744-f28d05c3cf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Model 75 trained 1 epochs has accuracy 41.74% and saved.\n",
      "Model 76 trained 1 epochs has accuracy 43.41% and saved.\n",
      "Model 77 trained 1 epochs has accuracy 41.70% and saved.\n",
      "Model 78 trained 1 epochs has accuracy 44.35% and saved.\n",
      "Model 79 trained 1 epochs has accuracy 44.36% and saved.\n",
      "Model 80 trained 1 epochs has accuracy 44.51% and saved.\n",
      "Model 81 trained 1 epochs has accuracy 41.43% and saved.\n",
      "Model 82 trained 1 epochs has accuracy 42.68% and saved.\n",
      "Model 83 trained 1 epochs has accuracy 42.42% and saved.\n",
      "Model 84 trained 1 epochs has accuracy 43.82% and saved.\n",
      "Model 85 trained 1 epochs has accuracy 43.35% and saved.\n",
      "Model 86 trained 1 epochs has accuracy 42.68% and saved.\n",
      "Model 87 trained 1 epochs has accuracy 44.39% and saved.\n",
      "Model 88 trained 1 epochs has accuracy 44.30% and saved.\n",
      "Model 89 trained 1 epochs has accuracy 42.89% and saved.\n",
      "Model 90 trained 1 epochs has accuracy 44.51% and saved.\n",
      "Model 91 trained 1 epochs has accuracy 43.86% and saved.\n",
      "Model 92 trained 1 epochs has accuracy 42.99% and saved.\n",
      "Model 93 trained 1 epochs has accuracy 44.89% and saved.\n",
      "Model 94 trained 1 epochs has accuracy 45.07% and saved.\n",
      "Model 95 trained 1 epochs has accuracy 44.91% and saved.\n",
      "Model 96 trained 1 epochs has accuracy 45.90% and saved.\n",
      "Model 97 trained 1 epochs has accuracy 41.62% and saved.\n",
      "Model 98 trained 1 epochs has accuracy 39.73% and saved.\n",
      "Model 99 trained 1 epochs has accuracy 43.81% and saved.\n",
      "Model 100 trained 1 epochs has accuracy 44.00% and saved.\n",
      "Model 101 trained 1 epochs has accuracy 41.52% and saved.\n",
      "Model 102 trained 1 epochs has accuracy 41.67% and saved.\n",
      "Model 103 trained 1 epochs has accuracy 44.87% and saved.\n",
      "Model 104 trained 1 epochs has accuracy 42.12% and saved.\n",
      "Model 105 trained 1 epochs has accuracy 43.18% and saved.\n",
      "Model 106 trained 1 epochs has accuracy 45.14% and saved.\n",
      "Model 107 trained 1 epochs has accuracy 45.86% and saved.\n",
      "Model 108 trained 1 epochs has accuracy 43.66% and saved.\n",
      "Model 109 trained 1 epochs has accuracy 42.25% and saved.\n",
      "Model 110 trained 1 epochs has accuracy 41.98% and saved.\n",
      "Model 111 trained 1 epochs has accuracy 44.78% and saved.\n",
      "Model 112 trained 1 epochs has accuracy 43.80% and saved.\n",
      "Model 113 trained 1 epochs has accuracy 43.17% and saved.\n",
      "Model 114 trained 1 epochs has accuracy 42.15% and saved.\n",
      "Model 115 trained 1 epochs has accuracy 41.88% and saved.\n",
      "Model 116 trained 1 epochs has accuracy 42.92% and saved.\n",
      "Model 117 trained 1 epochs has accuracy 41.26% and saved.\n",
      "Model 118 trained 1 epochs has accuracy 43.59% and saved.\n",
      "Model 119 trained 1 epochs has accuracy 43.03% and saved.\n",
      "Model 120 trained 1 epochs has accuracy 44.79% and saved.\n",
      "Model 121 trained 1 epochs has accuracy 42.47% and saved.\n",
      "Model 122 trained 1 epochs has accuracy 44.01% and saved.\n",
      "Model 123 trained 1 epochs has accuracy 44.13% and saved.\n",
      "Model 124 trained 1 epochs has accuracy 45.49% and saved.\n",
      "Model 125 trained 1 epochs has accuracy 42.94% and saved.\n",
      "Model 126 trained 1 epochs has accuracy 44.35% and saved.\n",
      "Model 127 trained 1 epochs has accuracy 43.25% and saved.\n",
      "Model 128 trained 1 epochs has accuracy 43.07% and saved.\n",
      "Model 129 trained 1 epochs has accuracy 45.78% and saved.\n",
      "Model 130 trained 1 epochs has accuracy 43.49% and saved.\n",
      "Model 131 trained 1 epochs has accuracy 44.91% and saved.\n",
      "Model 132 trained 1 epochs has accuracy 40.95% and saved.\n",
      "Model 133 trained 1 epochs has accuracy 47.11% and saved.\n",
      "Model 134 trained 1 epochs has accuracy 45.90% and saved.\n",
      "Model 135 trained 1 epochs has accuracy 44.33% and saved.\n",
      "Model 136 trained 1 epochs has accuracy 42.61% and saved.\n",
      "Model 137 trained 1 epochs has accuracy 43.48% and saved.\n",
      "Model 138 trained 1 epochs has accuracy 45.86% and saved.\n",
      "Model 139 trained 1 epochs has accuracy 44.43% and saved.\n",
      "Model 140 trained 1 epochs has accuracy 45.12% and saved.\n",
      "Model 141 trained 1 epochs has accuracy 43.01% and saved.\n",
      "Model 142 trained 1 epochs has accuracy 41.85% and saved.\n",
      "Model 143 trained 1 epochs has accuracy 42.37% and saved.\n",
      "Model 144 trained 1 epochs has accuracy 45.84% and saved.\n",
      "Model 145 trained 1 epochs has accuracy 43.64% and saved.\n",
      "Model 146 trained 1 epochs has accuracy 42.97% and saved.\n",
      "Model 147 trained 1 epochs has accuracy 40.61% and saved.\n",
      "Model 148 trained 1 epochs has accuracy 41.47% and saved.\n",
      "Model 149 trained 1 epochs has accuracy 41.58% and saved.\n",
      "Model 150 trained 1 epochs has accuracy 45.35% and saved.\n",
      "Model 151 trained 1 epochs has accuracy 45.94% and saved.\n",
      "Model 152 trained 1 epochs has accuracy 41.96% and saved.\n",
      "Model 153 trained 1 epochs has accuracy 41.83% and saved.\n",
      "Model 154 trained 1 epochs has accuracy 42.71% and saved.\n",
      "Model 155 trained 1 epochs has accuracy 43.97% and saved.\n",
      "Model 156 trained 1 epochs has accuracy 43.11% and saved.\n",
      "Model 157 trained 1 epochs has accuracy 43.46% and saved.\n",
      "Model 158 trained 1 epochs has accuracy 44.41% and saved.\n",
      "Model 159 trained 1 epochs has accuracy 42.43% and saved.\n",
      "Model 160 trained 1 epochs has accuracy 44.79% and saved.\n",
      "Model 161 trained 1 epochs has accuracy 44.15% and saved.\n",
      "Model 162 trained 1 epochs has accuracy 44.21% and saved.\n",
      "Model 163 trained 1 epochs has accuracy 41.87% and saved.\n",
      "Model 164 trained 1 epochs has accuracy 45.54% and saved.\n",
      "Model 165 trained 1 epochs has accuracy 42.24% and saved.\n",
      "Model 166 trained 1 epochs has accuracy 45.54% and saved.\n",
      "Model 167 trained 1 epochs has accuracy 43.32% and saved.\n",
      "Model 168 trained 1 epochs has accuracy 43.56% and saved.\n",
      "Model 169 trained 1 epochs has accuracy 44.23% and saved.\n",
      "Model 170 trained 1 epochs has accuracy 43.83% and saved.\n",
      "Model 171 trained 1 epochs has accuracy 40.59% and saved.\n",
      "Model 172 trained 1 epochs has accuracy 39.89% and saved.\n",
      "Model 173 trained 1 epochs has accuracy 44.12% and saved.\n",
      "Model 174 trained 1 epochs has accuracy 42.81% and saved.\n",
      "Model 175 trained 1 epochs has accuracy 43.13% and saved.\n",
      "Model 176 trained 1 epochs has accuracy 43.48% and saved.\n",
      "Model 177 trained 1 epochs has accuracy 44.28% and saved.\n",
      "Model 178 trained 1 epochs has accuracy 43.07% and saved.\n",
      "Model 179 trained 1 epochs has accuracy 44.99% and saved.\n",
      "Model 180 trained 1 epochs has accuracy 43.06% and saved.\n",
      "Model 181 trained 1 epochs has accuracy 43.75% and saved.\n",
      "Model 182 trained 1 epochs has accuracy 42.25% and saved.\n",
      "Model 183 trained 1 epochs has accuracy 44.39% and saved.\n",
      "Model 184 trained 1 epochs has accuracy 44.25% and saved.\n",
      "Model 185 trained 1 epochs has accuracy 45.21% and saved.\n",
      "Model 186 trained 1 epochs has accuracy 40.45% and saved.\n",
      "Model 187 trained 1 epochs has accuracy 42.39% and saved.\n",
      "Model 188 trained 1 epochs has accuracy 45.04% and saved.\n",
      "Model 189 trained 1 epochs has accuracy 43.31% and saved.\n",
      "Model 190 trained 1 epochs has accuracy 42.53% and saved.\n",
      "Model 191 trained 1 epochs has accuracy 42.89% and saved.\n",
      "Model 192 trained 1 epochs has accuracy 43.37% and saved.\n",
      "Model 193 trained 1 epochs has accuracy 45.69% and saved.\n",
      "Model 194 trained 1 epochs has accuracy 46.24% and saved.\n",
      "Model 195 trained 1 epochs has accuracy 42.35% and saved.\n",
      "Model 196 trained 1 epochs has accuracy 42.32% and saved.\n",
      "Model 197 trained 1 epochs has accuracy 40.32% and saved.\n",
      "Model 198 trained 1 epochs has accuracy 44.54% and saved.\n",
      "Model 199 trained 1 epochs has accuracy 40.57% and saved.\n",
      "Model 200 trained 1 epochs has accuracy 44.26% and saved.\n",
      "Model 201 trained 1 epochs has accuracy 43.33% and saved.\n",
      "Model 202 trained 1 epochs has accuracy 45.48% and saved.\n",
      "Model 203 trained 1 epochs has accuracy 44.65% and saved.\n",
      "Model 204 trained 1 epochs has accuracy 45.36% and saved.\n",
      "Model 205 trained 1 epochs has accuracy 45.20% and saved.\n",
      "Model 206 trained 1 epochs has accuracy 44.48% and saved.\n",
      "Model 207 trained 1 epochs has accuracy 42.98% and saved.\n",
      "Model 208 trained 1 epochs has accuracy 45.75% and saved.\n",
      "Model 209 trained 1 epochs has accuracy 44.70% and saved.\n",
      "Model 210 trained 1 epochs has accuracy 43.93% and saved.\n",
      "Model 211 trained 1 epochs has accuracy 42.84% and saved.\n",
      "Model 212 trained 1 epochs has accuracy 40.23% and saved.\n",
      "Model 213 trained 1 epochs has accuracy 44.39% and saved.\n",
      "Model 214 trained 1 epochs has accuracy 44.31% and saved.\n",
      "Model 215 trained 1 epochs has accuracy 44.85% and saved.\n",
      "Model 216 trained 1 epochs has accuracy 42.24% and saved.\n",
      "Model 217 trained 1 epochs has accuracy 43.92% and saved.\n",
      "Model 218 trained 1 epochs has accuracy 41.20% and saved.\n",
      "Model 219 trained 1 epochs has accuracy 45.54% and saved.\n",
      "Model 220 trained 1 epochs has accuracy 43.59% and saved.\n",
      "Model 221 trained 1 epochs has accuracy 44.21% and saved.\n",
      "Model 222 trained 1 epochs has accuracy 44.34% and saved.\n",
      "Model 223 trained 1 epochs has accuracy 42.57% and saved.\n",
      "Model 224 trained 1 epochs has accuracy 44.27% and saved.\n",
      "Model 225 trained 1 epochs has accuracy 45.19% and saved.\n",
      "Model 226 trained 1 epochs has accuracy 43.25% and saved.\n",
      "Model 227 trained 1 epochs has accuracy 44.78% and saved.\n",
      "Model 228 trained 1 epochs has accuracy 40.31% and saved.\n",
      "Model 229 trained 1 epochs has accuracy 45.63% and saved.\n",
      "Model 230 trained 1 epochs has accuracy 42.47% and saved.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdummy_imagenet_resnet_models/resnet_weights_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet20()\n\u001b[0;32m---> 10\u001b[0m train(model, train_loader, epochs \u001b[38;5;241m=\u001b[39m epochs)\n\u001b[1;32m     11\u001b[0m acc \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n",
      "Cell \u001b[0;32mIn[2], line 85\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, epochs, device)\u001b[0m\n\u001b[1;32m     83\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     84\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m---> 85\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = get_train_loader()\n",
    "test_loader = get_test_loader()\n",
    "epochs = 1\n",
    "\n",
    "for i in range(230, 500): # and leave it overnight - currently 230 \n",
    "    \n",
    "    os.makedirs('dummy_imagenet_resnet_models', exist_ok=True)\n",
    "    save_path = f\"dummy_imagenet_resnet_models/resnet_weights_{i}.pt\"\n",
    "    model = ResNet20()\n",
    "    train(model, train_loader, epochs = epochs)\n",
    "    acc = evaluate(model, test_loader)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f\"Model {i} trained {epochs} epochs has accuracy {acc:.2f}% and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623ac9d-3fa9-4ade-898a-90299c6399cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_and_generate_resnet_weights(\n",
    "        num_models=75,\n",
    "        n_samples=5,\n",
    "        n_iters = 100,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f3bc0-4bc7-4cb5-b92a-78ab982dc479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the results for ADAM built-in stepping are better than the cosine lr scheduler results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472bea86-89e4-4c3f-84a4-8d967836ea2a",
   "metadata": {},
   "source": [
    "# Just generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b90e462-439b-4daf-9373-38f20fd69cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_resnet_weights(\n",
    "    num_models,\n",
    "    batch_size=8,\n",
    "    n_samples=25,\n",
    "    load_it = 25, \n",
    "    n_steps = 100,\n",
    "    device=None\n",
    "):\n",
    "\n",
    "\n",
    "    # All weight shapes\n",
    "    layer_shapes = [\n",
    "        (16, 3, 3, 3),  # conv1\n",
    "    \n",
    "        # layer1 (3 BasicBlocks)\n",
    "        (16, 16, 3, 3), (16, 16, 3, 3),  # block 0\n",
    "        (16, 16, 3, 3), (16, 16, 3, 3),  # block 1\n",
    "        (16, 16, 3, 3), (16, 16, 3, 3),  # block 2\n",
    "    \n",
    "        # layer2 (3 BasicBlocks)\n",
    "        (32, 16, 1, 1),                  # shortcut for layer2[0]\n",
    "        (32, 16, 3, 3), (32, 32, 3, 3),  # block 0\n",
    "        (32, 32, 3, 3), (32, 32, 3, 3),  # block 1\n",
    "        (32, 32, 3, 3), (32, 32, 3, 3),  # block 2\n",
    "    \n",
    "        # layer3 (3 BasicBlocks)\n",
    "        (64, 32, 1, 1),                  # shortcut for layer3[0]\n",
    "        (64, 32, 3, 3), (64, 64, 3, 3),  # block 0\n",
    "        (64, 64, 3, 3), (64, 64, 3, 3),  # block 1\n",
    "        (64, 64, 3, 3), (64, 64, 3, 3),  # block 2\n",
    "    \n",
    "        (10, 64)  # final linear\n",
    "    ]\n",
    "\n",
    "    bias_shapes = [\n",
    "        (10,)\n",
    "    ]\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    logging.info(\"Getting permuted models...\")\n",
    "    ref_model, permuted_models = get_permuted_models_data(num_models=num_models, device=device)\n",
    "\n",
    "\n",
    "    logging.info(\"Converting models to WeightSpaceObjects...\")\n",
    "    weights_list = []\n",
    "    for model in tqdm(permuted_models):\n",
    "        weights = (\n",
    "            model.conv1.weight.data.clone(),\n",
    "            model.layer1[0].conv1.weight.data.clone(),\n",
    "            model.layer1[0].conv2.weight.data.clone(),\n",
    "            model.layer1[1].conv1.weight.data.clone(),\n",
    "            model.layer1[1].conv2.weight.data.clone(),\n",
    "            model.layer1[2].conv1.weight.data.clone(),\n",
    "            model.layer1[2].conv2.weight.data.clone(),\n",
    "            model.layer2[0].shortcut[0].weight.data.clone(),\n",
    "            model.layer2[0].conv1.weight.data.clone(),\n",
    "            model.layer2[0].conv2.weight.data.clone(),\n",
    "            model.layer2[1].conv1.weight.data.clone(),\n",
    "            model.layer2[1].conv2.weight.data.clone(),\n",
    "            model.layer2[2].conv1.weight.data.clone(),\n",
    "            model.layer2[2].conv2.weight.data.clone(),\n",
    "            model.layer3[0].shortcut[0].weight.data.clone(),\n",
    "            model.layer3[0].conv1.weight.data.clone(),\n",
    "            model.layer3[0].conv2.weight.data.clone(),\n",
    "            model.layer3[1].conv1.weight.data.clone(),\n",
    "            model.layer3[1].conv2.weight.data.clone(),\n",
    "            model.layer3[2].conv1.weight.data.clone(),\n",
    "            model.layer3[2].conv2.weight.data.clone(),\n",
    "            model.linear.weight.data.clone()\n",
    "        )\n",
    "        \n",
    "        biases = (\n",
    "            model.linear.bias.data.clone(),\n",
    "        )\n",
    "        \n",
    "        wso = WeightSpaceObject(weights, biases)\n",
    "        weights_list.append(wso)\n",
    "    \n",
    "   \n",
    "    flat_target_weights = torch.stack([wso.flatten(device) for wso in weights_list])\n",
    "    flat_dim = flat_target_weights.shape[1]\n",
    "\n",
    "    logging.info(f\"Created {len(weights_list)} target weight configurations\")\n",
    "\n",
    "    # for experimenting by generating source wsos from guassian noise\n",
    "    source_std = 0.001 #for noise we had 0.001\n",
    "\n",
    "    # Uncomment this part to get source from random noise\n",
    "    # flat_source_weights = torch.randn(len(weights_list), flat_dim, device=device) * source_std\n",
    "\n",
    "    mean_zero_wso = zero_like_wso(weights_list[0])\n",
    "    source_wsos = sample_gaussian_wsos(mean=mean_zero_wso, std=source_std, n=len(weights_list))\n",
    "    flat_source_weights = torch.stack([wso.flatten(device) for wso in source_wsos])\n",
    "\n",
    "    # Uncomment for experimenting by generating source wsos from kaimings Initialization\n",
    "    # source_wsos = get_random_pytorch_initialized_wsos(n=len(weights_list), device = device)\n",
    "    # flat_source_weights = torch.stack([wso.flatten(device) for wso in source_wsos])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    source_dataset = TensorDataset(flat_source_weights)\n",
    "    target_dataset = TensorDataset(flat_target_weights)\n",
    "\n",
    "    sourceloader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True, drop_last=False) # drop_last=True\n",
    "    targetloader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True, drop_last=False) # drop_last=True\n",
    "\n",
    "    \n",
    "    # Flow model\n",
    "    flow_model = TimeConditionedMLP(\n",
    "        input_dim=flat_dim,\n",
    "        hidden_dims=[2048, 2048, 2048, 2048],\n",
    "        output_dim=flat_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    logging.info(f\"ResNet20 params: {sum(p.numel() for p in ResNet20().parameters()):,}\")\n",
    "    logging.info(f\"Flow model params: {sum(p.numel() for p in flow_model.parameters()):,}\")\n",
    "\n",
    "    logging.info(\"Loading pretrained flow model...\")\n",
    "    flow_model.load_state_dict(torch.load(f\"checkpoints/CIFAR10_linear_cfm_{load_it}.pth\", weights_only=True)) # yucky that this is hardcoded\n",
    "\n",
    "    # optimizer = torch.optim.AdamW(flow_model.parameters(), lr=0.0001, weight_decay = 1e-6)\n",
    "\n",
    "    # Flow Matching\n",
    "    cfm =  SimpleCFM(\n",
    "        sourceloader=sourceloader,\n",
    "        targetloader=targetloader,\n",
    "        model=flow_model,\n",
    "        fm_type=\"ot\",\n",
    "        mode=\"ot\",\n",
    "        t_dist=\"beta\",\n",
    "        device=device,\n",
    "        normalize_pred=True,\n",
    "        geometric=True,\n",
    "    )\n",
    "    \n",
    "\n",
    "    # cfm = torch.load(f\"checkpoints/CIFAR10_linear_cfm_{load_it}.pth\", weights_only=True) # yucky that this is hardcoded\n",
    "\n",
    "    \n",
    "    # Sampling\n",
    "    logging.info(\"Generating new ResNet weights...\")\n",
    "    source_wsos = get_random_pytorch_initialized_wsos(n_samples, device=device)\n",
    "    random_flat = torch.stack([\n",
    "        wso.flatten(device) + torch.randn_like(wso.flatten()) * source_std\n",
    "        for wso in source_wsos\n",
    "    ])\n",
    "    \n",
    "    random_flat = torch.randn(n_samples, flat_dim, device=device) * source_std\n",
    "    new_weights_flat = cfm.map(\n",
    "        random_flat, \n",
    "        n_steps=n_steps, # tune this?\n",
    "        noise_scale=0.0005\n",
    "    )\n",
    "\n",
    "    test_loader = get_test_loader(batch_size=128)\n",
    "    accs = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        new_wso = WeightSpaceObject.from_flat(\n",
    "            new_weights_flat[i], \n",
    "            layer_shapes=layer_shapes, \n",
    "            bias_shapes = bias_shapes,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        model = ResNet20()\n",
    "\n",
    "        idx = 0\n",
    "        # conv1\n",
    "        model.conv1.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "        \n",
    "        # layer1 (3 BasicBlocks)\n",
    "        for block in range(3):\n",
    "            model.layer1[block].conv1.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "            model.layer1[block].conv2.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "        \n",
    "        # layer2 (3 BasicBlocks)\n",
    "        for block in range(3):\n",
    "            if block == 0:\n",
    "                # Assign shortcut (downsample) conv for first block of layer2\n",
    "                model.layer2[block].shortcut[0].weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "            model.layer2[block].conv1.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "            model.layer2[block].conv2.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "        \n",
    "        # layer3 (3 BasicBlocks)\n",
    "        for block in range(3):\n",
    "            if block == 0:\n",
    "                # Assign shortcut (downsample) conv for first block of layer3\n",
    "                model.layer3[block].shortcut[0].weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "            model.layer3[block].conv1.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "            model.layer3[block].conv2.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "        \n",
    "        # Final linear layer\n",
    "        model.linear.weight.data = new_wso.weights[idx].clone(); idx += 1\n",
    "        model.linear.bias.data = new_wso.biases[0].clone()\n",
    "            \n",
    "        model = model.to(device)\n",
    "\n",
    "        try:\n",
    "            acc = evaluate(model, test_loader, device)\n",
    "            accs.append(acc)\n",
    "            logging.info(f\"Generated model {i} accuracy: {acc:.8f}%\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to evaluate model {i}: {e}\")\n",
    "        \n",
    "    plt.hist(accs)\n",
    "\n",
    "    logging.info(\"Generation complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778cc87-9b1a-419e-83b2-4c7a17ca1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_resnet_weights(\n",
    "    num_models = 50,\n",
    "    batch_size=8,\n",
    "    n_samples=50,\n",
    "    load_it = 25,\n",
    "    n_steps = 100, # Not sure what the best value is \n",
    "    device=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9165e35-cc93-4cda-b94d-55f52661ab98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab3214-d5c3-4edd-9c5b-9713d2777bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
