{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16410c95-33d0-4c8e-8b36-9b132c8997ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from typing import Optional, Tuple, List, Dict, Union\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import copy\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Set up device and logging\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "label = \"flow_matching2\"\n",
    "logging.basicConfig(filename=f'Outputs/{label}.log',\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    filemode='a')\n",
    "\n",
    "class LoggerWriter(object):\n",
    "    def __init__(self, level):\n",
    "        self.level = level\n",
    "\n",
    "    def write(self, message):\n",
    "        if message.strip() != \"\":\n",
    "            logging.log(self.level, message.strip())\n",
    "\n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "sys.stdout = LoggerWriter(logging.INFO)\n",
    "sys.stderr = LoggerWriter(logging.INFO)\n",
    "\n",
    "# ======================== UTILITY CLASSES ========================\n",
    "class Bunch:\n",
    "    \"\"\"Simple Bunch class for storing data\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# ======================== WEIGHT SPACE OBJECTS ========================\n",
    "class WeightSpaceObject:\n",
    "    \"\"\"Base class for weight space objects (MLPs)\"\"\"\n",
    "    def __init__(self, weights, biases):\n",
    "        self.weights = weights if isinstance(weights, tuple) else tuple(weights)\n",
    "        self.biases = biases if isinstance(biases, tuple) else tuple(biases)\n",
    "        \n",
    "    def flatten(self, device=None):\n",
    "        \"\"\"Flatten weights and biases into a single vector\"\"\"\n",
    "        flat = torch.cat([w.flatten() for w in self.weights] + \n",
    "                          [b.flatten() for b in self.biases])\n",
    "        if device:\n",
    "            flat = flat.to(device)\n",
    "        return flat\n",
    "    \n",
    "    @classmethod\n",
    "    def from_flat(cls, flat, layers, device):\n",
    "        \"\"\"Create WeightSpaceObject from flattened vector\"\"\"\n",
    "        sizes = []\n",
    "        # Calculate sizes for weight matrices\n",
    "        for i in range(len(layers) - 1):\n",
    "            sizes.append(layers[i] * layers[i+1])  # Weight matrix\n",
    "        # Calculate sizes for bias vectors\n",
    "        for i in range(1, len(layers)):\n",
    "            sizes.append(layers[i])  # Bias vector\n",
    "            \n",
    "        # Split flat tensor into parts\n",
    "        parts = []\n",
    "        start = 0\n",
    "        for size in sizes:\n",
    "            parts.append(flat[start:start+size])\n",
    "            start += size\n",
    "            \n",
    "        # Reshape into weight matrices and bias vectors\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            weights.append(parts[i].reshape(layers[i+1], layers[i]))\n",
    "            biases.append(parts[i + len(layers) - 1])\n",
    "            \n",
    "        return cls(weights, biases).to(device)\n",
    "    \n",
    "    def to(self, device):\n",
    "        \"\"\"Move weights and biases to specified device\"\"\"\n",
    "        weights = tuple(w.to(device) for w in self.weights)\n",
    "        biases = tuple(b.to(device) for b in self.biases)\n",
    "        return WeightSpaceObject(weights, biases)\n",
    "\n",
    "@dataclass\n",
    "class AttentionWeights:\n",
    "    \"\"\"Container for multi-head attention weights\"\"\"\n",
    "    qkv_weight: torch.Tensor  # Combined QKV projection\n",
    "    qkv_bias: Optional[torch.Tensor]\n",
    "    proj_weight: torch.Tensor  # Output projection\n",
    "    proj_bias: Optional[torch.Tensor]\n",
    "    num_heads: int\n",
    "    \n",
    "    def split_heads(self):\n",
    "        \"\"\"Split QKV weights by heads\"\"\"\n",
    "        d_model = self.qkv_weight.shape[1]\n",
    "        head_dim = d_model // self.num_heads\n",
    "        \n",
    "        # Reshape QKV: [3*d_model, d_model] -> [3, num_heads, head_dim, d_model]\n",
    "        qkv = self.qkv_weight.reshape(3, self.num_heads, head_dim, d_model)\n",
    "        q_weights = qkv[0]  # [num_heads, head_dim, d_model]\n",
    "        k_weights = qkv[1]\n",
    "        v_weights = qkv[2]\n",
    "        \n",
    "        return q_weights, k_weights, v_weights\n",
    "\n",
    "@dataclass\n",
    "class TransformerBlockWeights:\n",
    "    \"\"\"Container for transformer block weights\"\"\"\n",
    "    attention: AttentionWeights\n",
    "    norm1_weight: torch.Tensor\n",
    "    norm1_bias: torch.Tensor\n",
    "    mlp_weights: Tuple[torch.Tensor, ...]  # MLP layer weights\n",
    "    mlp_biases: Tuple[torch.Tensor, ...]\n",
    "    norm2_weight: torch.Tensor\n",
    "    norm2_bias: torch.Tensor\n",
    "\n",
    "class VisionTransformerWeightSpace:\n",
    "    \"\"\"Weight space object for Vision Transformers\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 patch_embed_weight: torch.Tensor,\n",
    "                 patch_embed_bias: Optional[torch.Tensor],\n",
    "                 cls_token: torch.Tensor,\n",
    "                 pos_embed: torch.Tensor,\n",
    "                 blocks: List[TransformerBlockWeights],\n",
    "                 norm_weight: torch.Tensor,\n",
    "                 norm_bias: torch.Tensor,\n",
    "                 head_weight: torch.Tensor,\n",
    "                 head_bias: torch.Tensor):\n",
    "        \n",
    "        self.patch_embed_weight = patch_embed_weight\n",
    "        self.patch_embed_bias = patch_embed_bias\n",
    "        self.cls_token = cls_token\n",
    "        self.pos_embed = pos_embed\n",
    "        self.blocks = blocks\n",
    "        self.norm_weight = norm_weight\n",
    "        self.norm_bias = norm_bias\n",
    "        self.head_weight = head_weight\n",
    "        self.head_bias = head_bias\n",
    "        \n",
    "    @classmethod\n",
    "    def from_vit_model(cls, model: nn.Module):\n",
    "        \"\"\"Extract weights from a ViT model\"\"\"\n",
    "        blocks = []\n",
    "        \n",
    "        # Extract transformer blocks\n",
    "        for block in model.blocks:\n",
    "            # Multi-head attention weights\n",
    "            attn = block.attn\n",
    "            attention_weights = AttentionWeights(\n",
    "                qkv_weight=attn.qkv.weight.data.clone(),\n",
    "                qkv_bias=attn.qkv.bias.data.clone() if attn.qkv.bias is not None else None,\n",
    "                proj_weight=attn.proj.weight.data.clone(),\n",
    "                proj_bias=attn.proj.bias.data.clone() if attn.proj.bias is not None else None,\n",
    "                num_heads=attn.num_heads\n",
    "            )\n",
    "            \n",
    "            # MLP weights - iterate through MLP's children modules\n",
    "            mlp_weights = []\n",
    "            mlp_biases = []\n",
    "            for name, layer in block.mlp.named_children():\n",
    "                if hasattr(layer, 'weight'):\n",
    "                    mlp_weights.append(layer.weight.data.clone())\n",
    "                    if hasattr(layer, 'bias') and layer.bias is not None:\n",
    "                        mlp_biases.append(layer.bias.data.clone())\n",
    "            mlp_weights = tuple(mlp_weights)\n",
    "            mlp_biases = tuple(mlp_biases)\n",
    "            \n",
    "            # Layer norms\n",
    "            block_weights = TransformerBlockWeights(\n",
    "                attention=attention_weights,\n",
    "                norm1_weight=block.norm1.weight.data.clone(),\n",
    "                norm1_bias=block.norm1.bias.data.clone(),\n",
    "                mlp_weights=mlp_weights,\n",
    "                mlp_biases=mlp_biases,\n",
    "                norm2_weight=block.norm2.weight.data.clone(),\n",
    "                norm2_bias=block.norm2.bias.data.clone()\n",
    "            )\n",
    "            blocks.append(block_weights)\n",
    "        \n",
    "        # Create weight space object\n",
    "        return cls(\n",
    "            patch_embed_weight=model.patch_embed.proj.weight.data.clone(),\n",
    "            patch_embed_bias=model.patch_embed.proj.bias.data.clone() \n",
    "                            if model.patch_embed.proj.bias is not None else None,\n",
    "            cls_token=model.cls_token.data.clone(),\n",
    "            pos_embed=model.pos_embed.data.clone(),\n",
    "            blocks=blocks,\n",
    "            norm_weight=model.norm.weight.data.clone(),\n",
    "            norm_bias=model.norm.bias.data.clone(),\n",
    "            head_weight=model.head.weight.data.clone(),\n",
    "            head_bias=model.head.bias.data.clone()\n",
    "        )\n",
    "    \n",
    "    def apply_to_model(self, model: nn.Module):\n",
    "        \"\"\"Apply weights to a ViT model\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Patch embedding\n",
    "            model.patch_embed.proj.weight.data.copy_(self.patch_embed_weight)\n",
    "            if self.patch_embed_bias is not None:\n",
    "                model.patch_embed.proj.bias.data.copy_(self.patch_embed_bias)\n",
    "            \n",
    "            # Tokens and embeddings\n",
    "            model.cls_token.data.copy_(self.cls_token)\n",
    "            model.pos_embed.data.copy_(self.pos_embed)\n",
    "            \n",
    "            # Transformer blocks\n",
    "            for block, block_weights in zip(model.blocks, self.blocks):\n",
    "                # Attention\n",
    "                attn = block.attn\n",
    "                attn.qkv.weight.data.copy_(block_weights.attention.qkv_weight)\n",
    "                if block_weights.attention.qkv_bias is not None:\n",
    "                    attn.qkv.bias.data.copy_(block_weights.attention.qkv_bias)\n",
    "                attn.proj.weight.data.copy_(block_weights.attention.proj_weight)\n",
    "                if block_weights.attention.proj_bias is not None:\n",
    "                    attn.proj.bias.data.copy_(block_weights.attention.proj_bias)\n",
    "                \n",
    "                # Layer norms\n",
    "                block.norm1.weight.data.copy_(block_weights.norm1_weight)\n",
    "                block.norm1.bias.data.copy_(block_weights.norm1_bias)\n",
    "                block.norm2.weight.data.copy_(block_weights.norm2_weight)\n",
    "                block.norm2.bias.data.copy_(block_weights.norm2_bias)\n",
    "                \n",
    "                # MLP - iterate through MLP's children modules\n",
    "                mlp_layers = [layer for name, layer in block.mlp.named_children() \n",
    "                             if hasattr(layer, 'weight')]\n",
    "                for layer, weight in zip(mlp_layers, block_weights.mlp_weights):\n",
    "                    layer.weight.data.copy_(weight)\n",
    "                    \n",
    "                # Handle biases separately since not all layers may have them\n",
    "                mlp_bias_idx = 0\n",
    "                for name, layer in block.mlp.named_children():\n",
    "                    if hasattr(layer, 'bias') and layer.bias is not None:\n",
    "                        if mlp_bias_idx < len(block_weights.mlp_biases):\n",
    "                            layer.bias.data.copy_(block_weights.mlp_biases[mlp_bias_idx])\n",
    "                            mlp_bias_idx += 1\n",
    "            \n",
    "            # Final norm and head\n",
    "            model.norm.weight.data.copy_(self.norm_weight)\n",
    "            model.norm.bias.data.copy_(self.norm_bias)\n",
    "            model.head.weight.data.copy_(self.head_weight)\n",
    "            model.head.bias.data.copy_(self.head_bias)\n",
    "    \n",
    "    def flatten(self, device=None) -> torch.Tensor:\n",
    "        \"\"\"Flatten all weights into a single vector\"\"\"\n",
    "        all_params = []\n",
    "        \n",
    "        # Patch embedding\n",
    "        all_params.append(self.patch_embed_weight.flatten())\n",
    "        if self.patch_embed_bias is not None:\n",
    "            all_params.append(self.patch_embed_bias.flatten())\n",
    "        \n",
    "        # Tokens and embeddings\n",
    "        all_params.append(self.cls_token.flatten())\n",
    "        all_params.append(self.pos_embed.flatten())\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            # Attention\n",
    "            all_params.append(block.attention.qkv_weight.flatten())\n",
    "            if block.attention.qkv_bias is not None:\n",
    "                all_params.append(block.attention.qkv_bias.flatten())\n",
    "            all_params.append(block.attention.proj_weight.flatten())\n",
    "            if block.attention.proj_bias is not None:\n",
    "                all_params.append(block.attention.proj_bias.flatten())\n",
    "            \n",
    "            # Norms\n",
    "            all_params.append(block.norm1_weight.flatten())\n",
    "            all_params.append(block.norm1_bias.flatten())\n",
    "            all_params.append(block.norm2_weight.flatten())\n",
    "            all_params.append(block.norm2_bias.flatten())\n",
    "            \n",
    "            # MLP\n",
    "            for w in block.mlp_weights:\n",
    "                all_params.append(w.flatten())\n",
    "            for b in block.mlp_biases:\n",
    "                all_params.append(b.flatten())\n",
    "        \n",
    "        # Final norm and head\n",
    "        all_params.append(self.norm_weight.flatten())\n",
    "        all_params.append(self.norm_bias.flatten())\n",
    "        all_params.append(self.head_weight.flatten())\n",
    "        all_params.append(self.head_bias.flatten())\n",
    "        \n",
    "        flat = torch.cat(all_params)\n",
    "        if device:\n",
    "            flat = flat.to(device)\n",
    "        return flat\n",
    "    \n",
    "    @classmethod\n",
    "    def from_flat(cls, flat_tensor, reference_ws, device=None):\n",
    "        \"\"\"Reconstruct VisionTransformerWeightSpace from flattened weights\"\"\"\n",
    "        if device is None:\n",
    "            device = flat_tensor.device\n",
    "            \n",
    "        # Get all parameter shapes from reference\n",
    "        param_shapes = []\n",
    "        param_types = []\n",
    "        \n",
    "        # Patch embedding\n",
    "        param_shapes.append(reference_ws.patch_embed_weight.shape)\n",
    "        param_types.append('patch_embed_weight')\n",
    "        \n",
    "        if reference_ws.patch_embed_bias is not None:\n",
    "            param_shapes.append(reference_ws.patch_embed_bias.shape)\n",
    "            param_types.append('patch_embed_bias')\n",
    "        \n",
    "        # CLS token and pos embed\n",
    "        param_shapes.append(reference_ws.cls_token.shape)\n",
    "        param_types.append('cls_token')\n",
    "        \n",
    "        param_shapes.append(reference_ws.pos_embed.shape)\n",
    "        param_types.append('pos_embed')\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block_idx, block in enumerate(reference_ws.blocks):\n",
    "            # Attention weights\n",
    "            param_shapes.append(block.attention.qkv_weight.shape)\n",
    "            param_types.append(f'block_{block_idx}_attn_qkv_weight')\n",
    "            \n",
    "            if block.attention.qkv_bias is not None:\n",
    "                param_shapes.append(block.attention.qkv_bias.shape)\n",
    "                param_types.append(f'block_{block_idx}_attn_qkv_bias')\n",
    "            \n",
    "            param_shapes.append(block.attention.proj_weight.shape)\n",
    "            param_types.append(f'block_{block_idx}_attn_proj_weight')\n",
    "            \n",
    "            if block.attention.proj_bias is not None:\n",
    "                param_shapes.append(block.attention.proj_bias.shape)\n",
    "                param_types.append(f'block_{block_idx}_attn_proj_bias')\n",
    "            \n",
    "            # Layer norms\n",
    "            param_shapes.append(block.norm1_weight.shape)\n",
    "            param_types.append(f'block_{block_idx}_norm1_weight')\n",
    "            \n",
    "            param_shapes.append(block.norm1_bias.shape)\n",
    "            param_types.append(f'block_{block_idx}_norm1_bias')\n",
    "            \n",
    "            param_shapes.append(block.norm2_weight.shape)\n",
    "            param_types.append(f'block_{block_idx}_norm2_weight')\n",
    "            \n",
    "            param_shapes.append(block.norm2_bias.shape)\n",
    "            param_types.append(f'block_{block_idx}_norm2_bias')\n",
    "            \n",
    "            # MLP weights\n",
    "            for mlp_idx, mlp_weight in enumerate(block.mlp_weights):\n",
    "                param_shapes.append(mlp_weight.shape)\n",
    "                param_types.append(f'block_{block_idx}_mlp_weight_{mlp_idx}')\n",
    "            \n",
    "            # MLP biases\n",
    "            for mlp_idx, mlp_bias in enumerate(block.mlp_biases):\n",
    "                param_shapes.append(mlp_bias.shape)\n",
    "                param_types.append(f'block_{block_idx}_mlp_bias_{mlp_idx}')\n",
    "        \n",
    "        # Final norm and head\n",
    "        param_shapes.append(reference_ws.norm_weight.shape)\n",
    "        param_types.append('norm_weight')\n",
    "        \n",
    "        param_shapes.append(reference_ws.norm_bias.shape)\n",
    "        param_types.append('norm_bias')\n",
    "        \n",
    "        param_shapes.append(reference_ws.head_weight.shape)\n",
    "        param_types.append('head_weight')\n",
    "        \n",
    "        param_shapes.append(reference_ws.head_bias.shape)\n",
    "        param_types.append('head_bias')\n",
    "        \n",
    "        # Split flat tensor according to shapes\n",
    "        sizes = [np.prod(shape) for shape in param_shapes]\n",
    "        parts = []\n",
    "        start = 0\n",
    "        \n",
    "        for size in sizes:\n",
    "            parts.append(flat_tensor[start:start+size])\n",
    "            start += size\n",
    "        \n",
    "        # Reconstruct parameters\n",
    "        reconstructed_params = {}\n",
    "        for i, (shape, param_type) in enumerate(zip(param_shapes, param_types)):\n",
    "            reconstructed_params[param_type] = parts[i].reshape(shape).to(device)\n",
    "        \n",
    "        # Build the blocks\n",
    "        reconstructed_blocks = []\n",
    "        num_blocks = len(reference_ws.blocks)\n",
    "        \n",
    "        for block_idx in range(num_blocks):\n",
    "            # Reconstruct attention weights\n",
    "            qkv_weight = reconstructed_params[f'block_{block_idx}_attn_qkv_weight']\n",
    "            qkv_bias = reconstructed_params.get(f'block_{block_idx}_attn_qkv_bias', None)\n",
    "            proj_weight = reconstructed_params[f'block_{block_idx}_attn_proj_weight']  \n",
    "            proj_bias = reconstructed_params.get(f'block_{block_idx}_attn_proj_bias', None)\n",
    "            \n",
    "            attention = AttentionWeights(\n",
    "                qkv_weight=qkv_weight,\n",
    "                qkv_bias=qkv_bias,\n",
    "                proj_weight=proj_weight,\n",
    "                proj_bias=proj_bias,\n",
    "                num_heads=reference_ws.blocks[block_idx].attention.num_heads\n",
    "            )\n",
    "            \n",
    "            # Reconstruct MLP weights\n",
    "            mlp_weights = []\n",
    "            mlp_biases = []\n",
    "            \n",
    "            mlp_weight_idx = 0\n",
    "            while f'block_{block_idx}_mlp_weight_{mlp_weight_idx}' in reconstructed_params:\n",
    "                mlp_weights.append(reconstructed_params[f'block_{block_idx}_mlp_weight_{mlp_weight_idx}'])\n",
    "                mlp_weight_idx += 1\n",
    "            \n",
    "            mlp_bias_idx = 0\n",
    "            while f'block_{block_idx}_mlp_bias_{mlp_bias_idx}' in reconstructed_params:\n",
    "                mlp_biases.append(reconstructed_params[f'block_{block_idx}_mlp_bias_{mlp_bias_idx}'])\n",
    "                mlp_bias_idx += 1\n",
    "            \n",
    "            # Create block\n",
    "            block = TransformerBlockWeights(\n",
    "                attention=attention,\n",
    "                norm1_weight=reconstructed_params[f'block_{block_idx}_norm1_weight'],\n",
    "                norm1_bias=reconstructed_params[f'block_{block_idx}_norm1_bias'],\n",
    "                mlp_weights=tuple(mlp_weights),\n",
    "                mlp_biases=tuple(mlp_biases),\n",
    "                norm2_weight=reconstructed_params[f'block_{block_idx}_norm2_weight'],\n",
    "                norm2_bias=reconstructed_params[f'block_{block_idx}_norm2_bias']\n",
    "            )\n",
    "            \n",
    "            reconstructed_blocks.append(block)\n",
    "        \n",
    "        # Create the full weight space object\n",
    "        return cls(\n",
    "            patch_embed_weight=reconstructed_params['patch_embed_weight'],\n",
    "            patch_embed_bias=reconstructed_params.get('patch_embed_bias', None),\n",
    "            cls_token=reconstructed_params['cls_token'],\n",
    "            pos_embed=reconstructed_params['pos_embed'],\n",
    "            blocks=reconstructed_blocks,\n",
    "            norm_weight=reconstructed_params['norm_weight'], \n",
    "            norm_bias=reconstructed_params['norm_bias'],\n",
    "            head_weight=reconstructed_params['head_weight'],\n",
    "            head_bias=reconstructed_params['head_bias']\n",
    "        )\n",
    "\n",
    "# ======================== REBASIN / TRANSFUSION ========================\n",
    "class PermutationSpec:\n",
    "    \"\"\"Specification for permutations applied throughout the network\"\"\"\n",
    "    \n",
    "    def __init__(self, num_blocks: int):\n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_perms = []\n",
    "        for _ in range(num_blocks):\n",
    "            self.block_perms.append({\n",
    "                'attention_in': None,\n",
    "                'attention_out': None,\n",
    "                'mlp1': None,\n",
    "                'mlp2': None,\n",
    "            })\n",
    "        \n",
    "    def set_block_perm(self, block_idx: int, perm_type: str, perm: torch.Tensor):\n",
    "        \"\"\"Set a specific permutation for a block\"\"\"\n",
    "        if block_idx < len(self.block_perms):\n",
    "            self.block_perms[block_idx][perm_type] = perm\n",
    "\n",
    "class TransFusionMatcher:\n",
    "    \"\"\"Weight matching using TransFusion approach\"\"\"\n",
    "    \n",
    "    def __init__(self, num_iterations: int = 3, epsilon: float = 1e-8):\n",
    "        self.num_iterations = num_iterations\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def compute_spectral_distance(self, weight1: torch.Tensor, weight2: torch.Tensor) -> float:\n",
    "        \"\"\"Compute permutation-invariant distance using singular values\"\"\"\n",
    "        try:\n",
    "            _, s1, _ = torch.svd(weight1.float())\n",
    "            _, s2, _ = torch.svd(weight2.float())\n",
    "        except:\n",
    "            try:\n",
    "                _, s1, _ = np.linalg.svd(weight1.cpu().numpy())\n",
    "                _, s2, _ = np.linalg.svd(weight2.cpu().numpy())\n",
    "                s1 = torch.tensor(s1, device=weight1.device)\n",
    "                s2 = torch.tensor(s2, device=weight2.device)\n",
    "            except:\n",
    "                # Fallback to Frobenius norm\n",
    "                return torch.norm(weight1 - weight2).item()\n",
    "        \n",
    "        # Pad to same length if necessary\n",
    "        max_len = max(len(s1), len(s2))\n",
    "        if len(s1) < max_len:\n",
    "            s1 = torch.cat([s1, torch.zeros(max_len - len(s1), device=s1.device)])\n",
    "        if len(s2) < max_len:\n",
    "            s2 = torch.cat([s2, torch.zeros(max_len - len(s2), device=s2.device)])\n",
    "        \n",
    "        return torch.norm(s1 - s2).item()\n",
    "    \n",
    "    def match_attention_heads(self, attn1: AttentionWeights, attn2: AttentionWeights):\n",
    "        \"\"\"Match attention heads between two attention layers\"\"\"\n",
    "        try:\n",
    "            q1, k1, v1 = attn1.split_heads()\n",
    "            q2, k2, v2 = attn2.split_heads()\n",
    "            \n",
    "            num_heads = attn1.num_heads\n",
    "            d_model = attn1.qkv_weight.shape[1]\n",
    "            head_dim = d_model // num_heads\n",
    "            \n",
    "            # Inter-head alignment using spectral distance\n",
    "            distance_matrix = torch.zeros(num_heads, num_heads)\n",
    "            \n",
    "            for i in range(num_heads):\n",
    "                for j in range(num_heads):\n",
    "                    dist_q = self.compute_spectral_distance(q1[i], q2[j])\n",
    "                    dist_k = self.compute_spectral_distance(k1[i], k2[j])\n",
    "                    dist_v = self.compute_spectral_distance(v1[i], v2[j])\n",
    "                    distance_matrix[i, j] = dist_q + dist_k + dist_v\n",
    "            \n",
    "            # Solve assignment problem\n",
    "            row_ind, col_ind = linear_sum_assignment(distance_matrix.cpu().numpy())\n",
    "            \n",
    "            # Create permutation matrix\n",
    "            perm = torch.eye(d_model, device=attn1.qkv_weight.device)\n",
    "            \n",
    "            # Apply head-wise permutation (simplified)\n",
    "            for i, j in zip(row_ind, col_ind):\n",
    "                if i != j:\n",
    "                    # Swap head positions\n",
    "                    start_i, end_i = i * head_dim, (i + 1) * head_dim\n",
    "                    start_j, end_j = j * head_dim, (j + 1) * head_dim\n",
    "                    # This is a simplified permutation - proper implementation would be more complex\n",
    "            \n",
    "            return None, None, perm\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error in attention matching: {e}\")\n",
    "            d_model = attn1.qkv_weight.shape[1]\n",
    "            return None, None, torch.eye(d_model, device=attn1.qkv_weight.device)\n",
    "    \n",
    "    def match_mlp_layer(self, weight1: torch.Tensor, weight2: torch.Tensor, \n",
    "                       prev_perm: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"Match MLP layers using Hungarian algorithm\"\"\"\n",
    "        try:\n",
    "            # Apply previous permutation if exists\n",
    "            if prev_perm is not None and prev_perm.shape[0] == weight1.shape[1]:\n",
    "                weight1_permuted = torch.mm(weight1, prev_perm.t())\n",
    "            else:\n",
    "                weight1_permuted = weight1\n",
    "            \n",
    "            # Compute cost matrix\n",
    "            cost_matrix = -torch.mm(weight2, weight1_permuted.t())\n",
    "            \n",
    "            # Solve assignment problem\n",
    "            row_ind, col_ind = linear_sum_assignment(cost_matrix.cpu().numpy())\n",
    "            \n",
    "            # Create permutation matrix\n",
    "            n = weight1.shape[0]\n",
    "            perm = torch.zeros(n, n, device=weight1.device)\n",
    "            perm[row_ind, col_ind] = 1.0\n",
    "            \n",
    "            return perm\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error in MLP matching: {e}\")\n",
    "            return torch.eye(weight1.shape[0], device=weight1.device)\n",
    "    \n",
    "    def canonicalize_model(self, models: List[VisionTransformerWeightSpace], \n",
    "                          reference_idx: int = 0) -> List[VisionTransformerWeightSpace]:\n",
    "        \"\"\"Canonicalize multiple models using one as reference\"\"\"\n",
    "        reference = models[reference_idx]\n",
    "        canonicalized = []\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            if i == reference_idx:\n",
    "                canonicalized.append(reference)\n",
    "            else:                \n",
    "                # Simple canonicalization - in practice would be more sophisticated\n",
    "                try:\n",
    "                    current_model = copy.deepcopy(model)\n",
    "                    \n",
    "                    # Apply simple permutation matching for each block\n",
    "                    for block_idx in range(len(current_model.blocks)):\n",
    "                        current_block = current_model.blocks[block_idx]\n",
    "                        reference_block = reference.blocks[block_idx]\n",
    "                        \n",
    "                        # Match attention (simplified)\n",
    "                        _, _, attn_perm = self.match_attention_heads(\n",
    "                            current_block.attention, reference_block.attention\n",
    "                        )\n",
    "                        \n",
    "                        # Match MLP layers\n",
    "                        if len(current_block.mlp_weights) >= 1:\n",
    "                            mlp_perm = self.match_mlp_layer(\n",
    "                                current_block.mlp_weights[0],\n",
    "                                reference_block.mlp_weights[0]\n",
    "                            )\n",
    "                    \n",
    "                    canonicalized.append(current_model)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    canonicalized.append(model)  # Use original if canonicalization fails\n",
    "        \n",
    "        return canonicalized\n",
    "\n",
    "# ======================== VIT MODEL DEFINITION ========================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention module\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP module\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, hidden_features: Optional[int] = None, \n",
    "                 out_features: Optional[int] = None, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features * 4\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with attention and MLP\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, num_heads: int = 8, mlp_ratio: float = 4.0, \n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttention(dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, int(dim * mlp_ratio), dropout=dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Image to patch embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size: int = 32, patch_size: int = 4, \n",
    "                 in_chans: int = 3, embed_dim: int = 192):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, \n",
    "                             kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simple Vision Transformer for CIFAR-10\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 img_size: int = 32,\n",
    "                 patch_size: int = 4,\n",
    "                 in_chans: int = 3,\n",
    "                 num_classes: int = 10,\n",
    "                 embed_dim: int = 512,\n",
    "                 depth: int = 8,\n",
    "                 num_heads: int = 8,\n",
    "                 mlp_ratio: float = 4.0,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "                \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def create_vit_small(num_classes: int = 10, **kwargs) -> VisionTransformer:\n",
    "    \"\"\"Create a small ViT suitable for CIFAR-10\"\"\"\n",
    "    # Set default values, but allow overrides from kwargs\n",
    "    defaults = {\n",
    "        'img_size': 32,\n",
    "        'patch_size': 4,\n",
    "        'embed_dim': 256,\n",
    "        'depth': 4,\n",
    "        'num_heads': 4,\n",
    "        'mlp_ratio': 4.0,\n",
    "        'num_classes': num_classes\n",
    "    }\n",
    "    \n",
    "    # Update defaults with any provided kwargs\n",
    "    defaults.update(kwargs)\n",
    "    \n",
    "    return VisionTransformer(**defaults)\n",
    "\n",
    "# ======================== DATA LOADING ========================\n",
    "def load_cifar10(batch_size=128):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "class SimpleCFM:\n",
    "    \"\"\"Base Conditional Flow Matching class\"\"\"\n",
    "    \n",
    "    def __init__(self, sourceloader, targetloader, model, mode=\"velocity\", \n",
    "                 t_dist=\"uniform\", device=None):\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.sourceloader = sourceloader\n",
    "        self.targetloader = targetloader\n",
    "        self.model = model\n",
    "        self.mode = mode\n",
    "        self.t_dist = t_dist\n",
    "        self.sigma = 0.001\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        self.input_dim = getattr(model, 'input_dim', model.net[0].in_features)\n",
    "\n",
    "    def sample_from_loader(self, loader):\n",
    "        \"\"\"Sample from a dataloader\"\"\"\n",
    "        try:\n",
    "            if not hasattr(loader, '_iterator') or loader._iterator is None:\n",
    "                loader._iterator = iter(loader)\n",
    "            try:\n",
    "                batch = next(loader._iterator)\n",
    "            except StopIteration:\n",
    "                loader._iterator = iter(loader)\n",
    "                batch = next(loader._iterator)\n",
    "            return batch[0]\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error sampling from loader: {e}\")\n",
    "            # Return zero tensor as fallback\n",
    "            return torch.zeros(1, self.input_dim, device=self.device)\n",
    "    \n",
    "    def sample_time_and_flow(self):\n",
    "        \"\"\"Sample time, start and end points, and intermediate x_t\"\"\"\n",
    "        x0 = self.sample_from_loader(self.sourceloader)\n",
    "        x1 = self.sample_from_loader(self.targetloader)\n",
    "        \n",
    "        batch_size = min(x0.size(0), x1.size(0))\n",
    "        x0 = x0[:batch_size].to(self.device)\n",
    "        x1 = x1[:batch_size].to(self.device)\n",
    "        \n",
    "        if self.t_dist == \"uniform\":\n",
    "            t = torch.rand(batch_size, device=self.device)\n",
    "        elif self.t_dist == \"beta\":\n",
    "            t = torch.distributions.Beta(2.0, 5.0).sample((batch_size,)).to(self.device)\n",
    "        \n",
    "        t_pad = t.reshape(-1, *([1] * (x0.dim() - 1)))\n",
    "        \n",
    "        mu_t = (1 - t_pad) * x0 + t_pad * x1\n",
    "        sigma_pad = torch.tensor(self.sigma, device=self.device)\n",
    "        xt = mu_t + sigma_pad * torch.randn_like(x0)\n",
    "        ut = x1 - x0\n",
    "        \n",
    "        return Bunch(t=t.unsqueeze(-1), x0=x0, xt=xt, x1=x1, ut=ut, batch_size=batch_size)\n",
    "\n",
    "    def forward(self, flow):\n",
    "        \"\"\"Forward pass through the model\"\"\"\n",
    "        try:\n",
    "            flow_pred = self.model(flow.xt, flow.t)\n",
    "            return None, flow_pred\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error in forward pass: {e}\")\n",
    "            return None, torch.zeros_like(flow.ut)\n",
    "        \n",
    "    def loss_fn(self, flow_pred, flow):\n",
    "        \"\"\"Compute loss between predicted and true flows\"\"\"\n",
    "        if self.mode == \"velocity\":\n",
    "            l_flow = torch.mean((flow_pred.squeeze() - flow.ut) ** 2)\n",
    "        else:\n",
    "            l_flow = torch.mean((flow_pred.squeeze() - flow.x1) ** 2)\n",
    "        return None, l_flow\n",
    "\n",
    "    def map(self, x0, n_steps=100, return_traj=False, method=\"rk4\", adaptive=False):\n",
    "        \"\"\"Enhanced mapping function optimized for weight spaces\"\"\"\n",
    "        \n",
    "        # Use best model state if available\n",
    "        if self.best_model_state is not None:\n",
    "            current_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "\n",
    "        self.model.eval()\n",
    "        batch_size, flat_dim = x0.size()\n",
    "        \n",
    "        # Initialize trajectory tracking\n",
    "        traj = [x0.detach().clone()] if return_traj else None\n",
    "        xt = x0.clone()\n",
    "\n",
    "        if method == \"euler\":\n",
    "            # Your current Euler method (works fine for weight spaces)\n",
    "            times = torch.linspace(0, 1, n_steps).to(self.device)\n",
    "            dt = times[1] - times[0]\n",
    "\n",
    "            for i, t in enumerate(times[:-1]):\n",
    "                with torch.no_grad():\n",
    "                    t_tensor = torch.ones(batch_size, 1).to(self.device) * t\n",
    "                    \n",
    "                    try:\n",
    "                        pred = self.model(xt, t_tensor)\n",
    "                        if pred.dim() > 2:\n",
    "                            pred = pred.squeeze(-1)\n",
    "\n",
    "                        # Get velocity\n",
    "                        if self.mode == \"velocity\":\n",
    "                            vt = pred\n",
    "                        else:\n",
    "                            vt = pred - xt\n",
    "\n",
    "                        # Euler step\n",
    "                        xt = xt + vt * dt\n",
    "                        \n",
    "                        # For weight spaces, remove the late-stage noise addition\n",
    "                        # Weight distributions are stable enough without it\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Error at step {i}: {e}\")\n",
    "                        break\n",
    "                \n",
    "                if return_traj:\n",
    "                    traj.append(xt.detach().clone())\n",
    "                    \n",
    "        elif method == \"rk4\":\n",
    "            # Runge-Kutta 4th order (more accurate for smooth weight spaces)\n",
    "            times = torch.linspace(0, 1, n_steps).to(self.device)\n",
    "            dt = times[1] - times[0]\n",
    "            \n",
    "            for i, t in enumerate(times[:-1]):\n",
    "                with torch.no_grad():\n",
    "                    # RK4 integration\n",
    "                    t_tensor = torch.ones(batch_size, 1).to(self.device) * t\n",
    "                    \n",
    "                    try:\n",
    "                        # k1\n",
    "                        pred1 = self.model(xt, t_tensor)\n",
    "                        if pred1.dim() > 2:\n",
    "                            pred1 = pred1.squeeze(-1)\n",
    "                        k1 = pred1 if self.mode == \"velocity\" else pred1 - xt\n",
    "                        \n",
    "                        # k2  \n",
    "                        xt_k2 = xt + 0.5 * dt * k1\n",
    "                        t_k2 = t_tensor + 0.5 * dt\n",
    "                        pred2 = self.model(xt_k2, t_k2)\n",
    "                        if pred2.dim() > 2:\n",
    "                            pred2 = pred2.squeeze(-1)\n",
    "                        k2 = pred2 if self.mode == \"velocity\" else pred2 - xt_k2\n",
    "                        \n",
    "                        # k3\n",
    "                        xt_k3 = xt + 0.5 * dt * k2  \n",
    "                        pred3 = self.model(xt_k3, t_k2)\n",
    "                        if pred3.dim() > 2:\n",
    "                            pred3 = pred3.squeeze(-1)\n",
    "                        k3 = pred3 if self.mode == \"velocity\" else pred3 - xt_k3\n",
    "                        \n",
    "                        # k4\n",
    "                        xt_k4 = xt + dt * k3\n",
    "                        t_k4 = t_tensor + dt\n",
    "                        pred4 = self.model(xt_k4, t_k4)\n",
    "                        if pred4.dim() > 2:\n",
    "                            pred4 = pred4.squeeze(-1)\n",
    "                        k4 = pred4 if self.mode == \"velocity\" else pred4 - xt_k4\n",
    "                        \n",
    "                        # Final RK4 step\n",
    "                        xt = xt + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"RK4 error at step {i}: {e}\")\n",
    "                        # Fallback to Euler step\n",
    "                        pred = self.model(xt, t_tensor)\n",
    "                        if pred.dim() > 2:\n",
    "                            pred = pred.squeeze(-1)\n",
    "                        vt = pred if self.mode == \"velocity\" else pred - xt\n",
    "                        xt = xt + vt * dt\n",
    "                \n",
    "                if return_traj:\n",
    "                    traj.append(xt.detach().clone())\n",
    "                    \n",
    "        elif method == \"adaptive\":\n",
    "            # Adaptive step size (overkill for most weight spaces, but available)\n",
    "            from scipy.integrate import solve_ivp\n",
    "            \n",
    "            def vector_field_numpy(t, x_flat):\n",
    "                x_tensor = torch.from_numpy(x_flat.reshape(batch_size, -1)).float().to(self.device)\n",
    "                t_tensor = torch.ones(batch_size, 1).to(self.device) * t\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    pred = self.model(x_tensor, t_tensor)\n",
    "                    if pred.dim() > 2:\n",
    "                        pred = pred.squeeze(-1)\n",
    "                    vt = pred if self.mode == \"velocity\" else pred - x_tensor\n",
    "                \n",
    "                return vt.cpu().numpy().flatten()\n",
    "            \n",
    "            # Solve ODE with adaptive step size\n",
    "            sol = solve_ivp(\n",
    "                vector_field_numpy, \n",
    "                [0, 1], \n",
    "                xt.cpu().numpy().flatten(),\n",
    "                dense_output=True,\n",
    "                rtol=1e-6\n",
    "            )\n",
    "            \n",
    "            if return_traj:\n",
    "                # Evaluate at regular intervals for trajectory\n",
    "                t_eval = np.linspace(0, 1, n_steps)\n",
    "                traj_points = sol.sol(t_eval)\n",
    "                traj = [torch.from_numpy(tp.reshape(batch_size, -1)).to(self.device) \n",
    "                       for tp in traj_points.T]\n",
    "            \n",
    "            xt = torch.from_numpy(sol.y[:, -1].reshape(batch_size, -1)).to(self.device)\n",
    "\n",
    "        # Restore model state and mode\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(current_state)\n",
    "        self.model.train()\n",
    "\n",
    "        return traj if return_traj else xt\n",
    "    \n",
    "    # def map(self, x0, n_steps=50, method=\"euler\"):\n",
    "    #     \"\"\"Map points using the flow model\"\"\"\n",
    "    #     if self.best_model_state is not None:\n",
    "    #         current_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "    #         self.model.load_state_dict(self.best_model_state)\n",
    "\n",
    "    #     self.model.eval()\n",
    "    #     batch_size, flat_dim = x0.size()\n",
    "        \n",
    "    #     times = torch.linspace(0, 1, n_steps, device=self.device)\n",
    "    #     dt = times[1] - times[0]\n",
    "    #     xt = x0.clone()\n",
    "\n",
    "    #     for t in times[:-1]:\n",
    "    #         with torch.no_grad():\n",
    "    #             t_tensor = torch.ones(batch_size, 1, device=self.device) * t\n",
    "                \n",
    "    #             try:\n",
    "    #                 pred = self.model(xt, t_tensor)\n",
    "    #                 if pred.dim() > 2:\n",
    "    #                     pred = pred.squeeze(-1)\n",
    "\n",
    "    #                 if self.mode == \"velocity\":\n",
    "    #                     vt = pred\n",
    "    #                 else:\n",
    "    #                     vt = pred - xt\n",
    "\n",
    "    #                 xt = xt + vt * dt\n",
    "                        \n",
    "    #             except Exception as e:\n",
    "    #                 logging.warning(f\"Error during mapping at t={t}: {e}\")\n",
    "    #                 break\n",
    "\n",
    "    #     if self.best_model_state is not None:\n",
    "    #         self.model.load_state_dict(current_state)\n",
    "    #     self.model.train()\n",
    "\n",
    "    #     return xt\n",
    "    \n",
    "    def train(self, n_iters=10000, optimizer=None, sigma=0.001, log_freq=100):\n",
    "        \"\"\"Train the flow model\"\"\"\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        pbar = tqdm(range(n_iters), desc=\"Training CFM\")\n",
    "        for i in pbar:\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                flow = self.sample_time_and_flow()\n",
    "                _, flow_pred = self.forward(flow)\n",
    "                _, loss = self.loss_fn(flow_pred, flow)\n",
    "                \n",
    "                if torch.isfinite(loss):\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                    if loss.item() < self.best_loss:\n",
    "                        self.best_loss = loss.item()\n",
    "                        self.best_model_state = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "                else:\n",
    "                    logging.warning(f\"Invalid loss at step {i}: {loss.item()}\")\n",
    "                    continue\n",
    "                \n",
    "                if i % log_freq == 0:\n",
    "                    pbar.set_description(f\"CFM Training [loss: {loss.item():.6f}]\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during training iteration {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "class EnhancedWeightSpaceCFM(SimpleCFM):\n",
    "    \"\"\"Enhanced CFM for weight spaces with better stability\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sigma = 0.001\n",
    "\n",
    "class VisionTransformerFlowModel(nn.Module):\n",
    "    \"\"\"Flow model for ViT weight spaces\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, time_embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        \n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, time_embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Main network with skip connections\n",
    "        # Smaller network since your weight space seems well-behaved\n",
    "        hidden_dim = min(256, input_dim // 4)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + time_embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.LayerNorm(hidden_dim//2), \n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(hidden_dim//2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize output to zero\n",
    "        nn.init.zeros_(self.net[-1].weight)\n",
    "        nn.init.zeros_(self.net[-1].bias)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t_embed = self.time_embed(t)\n",
    "        combined = torch.cat([x, t_embed], dim=-1)\n",
    "        return self.net(combined)\n",
    "\n",
    "def get_permuted_models_data(ref_point=0, model_dir=\"imagenet_vit_models\", \n",
    "                           num_models=50, device=None):\n",
    "    \"\"\"Load and align ViT models using rebasin\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    ref_model = create_vit_small()\n",
    "    ref_model_path = f\"{model_dir}/vit_weights_{ref_point}.pt\"\n",
    "    \n",
    "    try:\n",
    "        ref_model.load_state_dict(torch.load(ref_model_path, map_location=device))\n",
    "        ref_model = ref_model.to(device)\n",
    "        logging.info(f\"Loaded reference model from {ref_model_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference model: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    ref_ws = VisionTransformerWeightSpace.from_vit_model(ref_model)\n",
    "    weight_space_objects = [ref_ws]\n",
    "    \n",
    "    matcher = TransFusionMatcher(num_iterations=2)\n",
    "    \n",
    "    for i in tqdm(range(num_models), desc=\"Processing ViT models\"):\n",
    "        if i == ref_point:\n",
    "            continue\n",
    "        \n",
    "        model_path = f\"{model_dir}/vit_weights_{i}.pt\"\n",
    "        if not os.path.exists(model_path):\n",
    "            logging.warning(f\"Skipping model {i} - file not found\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            model = create_vit_small()\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            model = model.to(device)\n",
    "            \n",
    "            ws = VisionTransformerWeightSpace.from_vit_model(model)\n",
    "            \n",
    "            canonicalized_list = matcher.canonicalize_model([ref_ws, ws], reference_idx=0)\n",
    "            aligned_ws = canonicalized_list[1]\n",
    "            \n",
    "            weight_space_objects.append(aligned_ws)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing model {i}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    logging.info(f\"Successfully processed {len(weight_space_objects)} ViT models\")\n",
    "    return ref_model, weight_space_objects\n",
    "\n",
    "def train_vit_flow_matching(vit_config=None, model_dir=\"imagenet_vit_models\", \n",
    "                           num_models=50):\n",
    "    \"\"\"Complete pipeline for training flow matching on ViT weights\"\"\"\n",
    "    if vit_config is None:\n",
    "        vit_config = {\n",
    "            'num_classes': 10,\n",
    "            'embed_dim': 256,\n",
    "            'depth': 4,\n",
    "            'num_heads': 4,\n",
    "            'mlp_ratio': 4.0,\n",
    "            'dropout': 0.1\n",
    "        }\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    ref_model, weight_space_objects = get_permuted_models_data(\n",
    "        model_dir=model_dir,\n",
    "        num_models=num_models,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Converting ViT weights to flat tensors...\")\n",
    "    flat_weights = []\n",
    "    for ws in tqdm(weight_space_objects):\n",
    "        flat = ws.flatten(device=device)\n",
    "        flat_weights.append(flat)\n",
    "    \n",
    "    flat_target_weights = torch.stack(flat_weights)\n",
    "    flat_dim = flat_target_weights.shape[1]\n",
    "    \n",
    "    logging.info(f\"ViT weight space dimension: {flat_dim:,}\")\n",
    "\n",
    "    target_std = torch.std(flat_target_weights).item()\n",
    "    logging.info(f\"Target std: {target_std:.6f}\")\n",
    "    num_samples = len(flat_target_weights)\n",
    "    flat_source_weights = torch.randn(num_samples, flat_dim, device=device) * target_std\n",
    "    \n",
    "    # # Create source distribution\n",
    "    # flat_source_weights = torch.randn(num_samples, flat_dim, device=device) * 0.1\n",
    "    \n",
    "    # Create dataloaders\n",
    "    source_dataset = TensorDataset(flat_source_weights)\n",
    "    target_dataset = TensorDataset(flat_target_weights)\n",
    "    \n",
    "    sourceloader = DataLoader(source_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "    targetloader = DataLoader(target_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
    "\n",
    "    \n",
    "    # Create and train flow model\n",
    "    flow_model = VisionTransformerFlowModel(flat_dim).to(device)\n",
    "    logging.info(f\"Flow model parameters: {count_parameters(flow_model):,}\")\n",
    "    \n",
    "    cfm = EnhancedWeightSpaceCFM(\n",
    "        sourceloader=sourceloader,\n",
    "        targetloader=targetloader,\n",
    "        model=flow_model,\n",
    "        mode=\"velocity\",\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        flow_model.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-5,\n",
    "        betas=(0.9, 0.95)\n",
    "    )\n",
    "    \n",
    "    cfm.train(\n",
    "        n_iters=30000,\n",
    "        optimizer=optimizer,\n",
    "        sigma=0.001,\n",
    "        log_freq=50\n",
    "    )\n",
    "    \n",
    "    return cfm, weight_space_objects[0], vit_config, target_std\n",
    "\n",
    "def generate_new_vit_models(cfm, reference_ws, vit_config, target_std, n_samples=5):\n",
    "    \"\"\"Generate new ViT models using trained flow matching\"\"\"\n",
    "    device = cfm.device\n",
    "    flat_dim = cfm.input_dim\n",
    "    \n",
    "    logging.info(f\"Generating {n_samples} new ViT models...\")\n",
    "    # source_samples = torch.randn(n_samples, flat_dim, device=device) * target_std\n",
    "    source_samples = torch.randn(n_samples, flat_dim, device=device) * 0.01\n",
    "    \n",
    "    generated_flat = cfm.map(source_samples, n_steps=100)\n",
    "    \n",
    "    generated_models = []\n",
    "    test_loader = load_cifar10(batch_size=128)[1]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        try:\n",
    "            generated_ws = VisionTransformerWeightSpace.from_flat(\n",
    "                generated_flat[i], reference_ws, device\n",
    "            )\n",
    "            \n",
    "            new_model = create_vit_small(**vit_config).to(device)\n",
    "            generated_ws.apply_to_model(new_model)\n",
    "            \n",
    "            accuracy = evaluate(new_model, test_loader, device)\n",
    "            logging.info(f\"Generated ViT model {i}: {accuracy*100:.2f}% accuracy\")\n",
    "            \n",
    "            generated_models.append(new_model)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating model {i}: {e}\")\n",
    "            logging.error(f\"vit_config contents: {vit_config}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    return generated_models\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function demonstrating ViT flow matching\"\"\"\n",
    "    logging.info(\"Starting ViT Flow Matching Pipeline...\")\n",
    "    \n",
    "    # Configuration\n",
    "    vit_config = {\n",
    "        'num_classes': 10,\n",
    "        'embed_dim': 256,\n",
    "        'depth': 4,\n",
    "        'num_heads': 4,\n",
    "        'mlp_ratio': 4.0,\n",
    "        'dropout': 0.1\n",
    "    }\n",
    "    \n",
    "    logging.info(f\"Using ViT config: {vit_config}\")\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Testing model creation...\")\n",
    "        test_model = create_vit_small(**vit_config)\n",
    "        \n",
    "        cfm, reference_ws, config, target_std = train_vit_flow_matching(\n",
    "            vit_config=vit_config,\n",
    "            model_dir=\"/scratch/sgupta/data/imagenet_vit_models\",\n",
    "            num_models=100\n",
    "        )\n",
    "        \n",
    "        generated_models = generate_new_vit_models(\n",
    "            cfm, reference_ws, config, target_std, n_samples=100\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Successfully generated {len(generated_models)} new ViT models!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main pipeline: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (splicing)",
   "language": "python",
   "name": "splicing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
