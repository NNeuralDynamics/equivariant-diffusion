{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e00fe6c8-6444-4a77-ada2-472cd056562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "from experiments.data import INRDataset\n",
    "from experiments.utils import (\n",
    "    common_parser,\n",
    "    count_parameters,\n",
    "    get_device,\n",
    "    set_logger,\n",
    "    set_seed,\n",
    "    str2bool,\n",
    ")\n",
    "from nn.models import DWSModelForClassification, MLPModelForClassification\n",
    "\n",
    "from experiments.mnist.generate_data_splits import generate_splits\n",
    "from experiments.mnist.compute_statistics import compute_stats\n",
    "\n",
    "set_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52984b0b-0751-43bb-8fb5-4f95cf3ee5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Tesla P100-PCIE-12GB\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())  # The ID of the current GPU\n",
    "print(torch.cuda.get_device_name(0))  # The name of the specified GPU\n",
    "print(torch.cuda.device_count())  # The amount of GPUs that are accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4acf94d0-9e9d-4cd9-9c5d-4a558590a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77cfddde-c01f-40f6-8ac4-27a7abee2159",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea4da9-7279-4d80-be09-6b73ec3334e2",
   "metadata": {
    "id": "cb3a1c45",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## INR Dataset\n",
    "\n",
    "We create INR Datasets and Dataloaders, and visualize some INRs (by reconstruction the images).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cbcf4cd-b6f2-48ff-a234-a2369ed9c41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/talisman/sgupta/DWSNets/equivariant-diffusion\n"
     ]
    }
   ],
   "source": [
    "#Loading inr data we created while mnist training\n",
    "import os\n",
    "current_working_directory = os.getcwd()\n",
    "print(current_working_directory)\n",
    "path = current_working_directory + \"/notebooks/dataset/mnist_splits.json\"\n",
    "statistics_path = current_working_directory + \"/notebooks/dataset/statistics.pth\"\n",
    "normalize = True\n",
    "augmentation = True\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ea15596-50a1-42aa-9433-408bf0af368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/talisman/sgupta/DWSNets/equivariant-diffusion\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd40c5f4-1892-4c10-8ca1-a70f9ddb8f0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/talisman/sgupta/DWSNets/notebooks/dataset/mnist-inrs/mnist_png_training_5_635/checkpoints/model_final.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-82c8634ba4ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mloader_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mbatch_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_aug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/talisman/sgupta/DWSNets/equivariant-diffusion/experiments/data.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"weight\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/talisman/sgupta/DWSNets/notebooks/dataset/mnist-inrs/mnist_png_training_5_635/checkpoints/model_final.pth'"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image, make_grid\n",
    "import torch\n",
    "\n",
    "from experiments.data import INRImageDataset\n",
    "from experiments.utils import set_seed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = INRImageDataset(\n",
    "    path=path,  # path to splits json file\n",
    "    augmentation=False,\n",
    "    split=\"train\",\n",
    ")\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "dataset_aug = INRImageDataset(\n",
    "    path=path,  # path to splits json file\n",
    "    augmentation=True,\n",
    "    split=\"train\",\n",
    ")\n",
    "loader_aug = torch.utils.data.DataLoader(dataset_aug, batch_size=64, shuffle=False)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "batch_aug = next(iter(loader_aug))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,20)) \n",
    "\n",
    "axs[0].imshow(make_grid(batch.image.squeeze(-1)).permute(1, 2, 0).clip(0, 1))\n",
    "axs[0].set_title('Recunstracted Images from INRs')\n",
    "\n",
    "axs[1].imshow(make_grid(batch_aug.image.squeeze(-1)).permute(1, 2, 0).clip(0, 1))\n",
    "axs[1].set_title('Recunstracted Images from Augmented INRs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e4161e-7af3-4641-bae2-5491126553a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 15:54:59,492 - root - INFO - train size 55000, val size 5000, test size 10000\n"
     ]
    }
   ],
   "source": [
    "train_set = INRDataset(\n",
    "        path=path,\n",
    "        split=\"train\",\n",
    "        normalize=normalize,\n",
    "        augmentation=augmentation,\n",
    "        statistics_path=statistics_path,\n",
    "    )\n",
    "\n",
    "val_set = INRDataset(\n",
    "    path=path,\n",
    "    split=\"val\",\n",
    "    normalize=normalize,\n",
    "    statistics_path=statistics_path,\n",
    ")\n",
    "\n",
    "test_set = INRDataset(\n",
    "    path=path,\n",
    "    split=\"test\",\n",
    "    normalize=normalize,\n",
    "    statistics_path=statistics_path,\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "logging.info(\n",
    "    f\"train size {len(train_set)}, \"\n",
    "    f\"val size {len(val_set)}, \"\n",
    "    f\"test size {len(test_set)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9ea36efa-b9c9-498d-812b-3bfe7ce094d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Tuple\n",
    "# from nn.layers import BN, DownSampleDWSLayer, Dropout, DWSLayer, InvariantLayer, ReLU\n",
    "\n",
    "# #Our DWSModel equivalent to convolutional block of AutoEncoder which is used to build encoder/decoder layers\n",
    "# class DWSModel(nn.Module):\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             weight_shapes,\n",
    "#             bias_shapes,\n",
    "#             input_features,\n",
    "#             hidden_dims,\n",
    "#             n_hidden=2,\n",
    "#             output_features=None,\n",
    "#             reduction=\"max\",\n",
    "#             bias=True,\n",
    "#             n_fc_layers=1,\n",
    "#             num_heads=8,\n",
    "#             set_layer=\"sab\",\n",
    "#             input_dim_downsample=None,\n",
    "#             dropout_rate=0.0,\n",
    "#             add_skip=False,\n",
    "#             add_layer_skip=False,\n",
    "#             init_scale=1e-4,\n",
    "#             init_off_diag_scale_penalty=1.,\n",
    "#             bn=False,\n",
    "#             diagonal=False,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         assert len(weight_shapes) > 2, \"the current implementation only support input networks with M>2 layers.\"\n",
    "#         assert len(hidden_dims) == n_hidden ,\"hidden dims length must be equal to the number of hidden dimensions\"\n",
    "#         self.input_features = input_features\n",
    "#         self.input_dim_downsample = input_dim_downsample\n",
    "#         if output_features is None:\n",
    "#             output_features = hidden_dims[-1]\n",
    "\n",
    "#         self.add_skip = add_skip\n",
    "#         if self.add_skip:\n",
    "#             self.skip = nn.Linear(\n",
    "#                 input_features,\n",
    "#                 output_features,\n",
    "#                 bias=bias\n",
    "#             )\n",
    "#             with torch.no_grad():\n",
    "#                 torch.nn.init.constant_(self.skip.weight, 1. / self.skip.weight.numel())\n",
    "#                 torch.nn.init.constant_(self.skip.bias, 0.)\n",
    "\n",
    "#         if input_dim_downsample is None:\n",
    "#             layers = [\n",
    "#                 DWSLayer(\n",
    "#                     weight_shapes=weight_shapes,\n",
    "#                     bias_shapes=bias_shapes,\n",
    "#                     in_features=input_features,\n",
    "#                     out_features=hidden_dims[0],\n",
    "#                     reduction=reduction,\n",
    "#                     bias=bias,\n",
    "#                     n_fc_layers=n_fc_layers,\n",
    "#                     num_heads=num_heads,\n",
    "#                     set_layer=set_layer,\n",
    "#                     add_skip=add_layer_skip,\n",
    "#                     init_scale=init_scale,\n",
    "#                     init_off_diag_scale_penalty=init_off_diag_scale_penalty,\n",
    "#                     diagonal=diagonal,\n",
    "#                 ),\n",
    "#             ]\n",
    "#             for i in range(n_hidden):\n",
    "#                 if bn:\n",
    "#                     layers.append(BN(hidden_dims[i], len(weight_shapes), len(bias_shapes)))\n",
    "\n",
    "#                 layers.extend(\n",
    "#                     [\n",
    "\n",
    "#                         ReLU(),\n",
    "#                         Dropout(dropout_rate),\n",
    "#                         DWSLayer(\n",
    "#                             weight_shapes=weight_shapes,\n",
    "#                             bias_shapes=bias_shapes,\n",
    "#                             in_features=hidden_dims[i],\n",
    "#                             out_features=hidden_dims[i+1] if i != (n_hidden - 1) else output_features,\n",
    "#                             reduction=reduction,\n",
    "#                             bias=bias,\n",
    "#                             n_fc_layers=n_fc_layers,\n",
    "#                             num_heads=num_heads if i != (n_hidden - 1) else 1,\n",
    "#                             set_layer=set_layer,\n",
    "#                             add_skip=add_layer_skip,\n",
    "#                             init_scale=init_scale,\n",
    "#                             init_off_diag_scale_penalty=init_off_diag_scale_penalty,\n",
    "#                             diagonal=diagonal,\n",
    "#                         ),\n",
    "#                     ]\n",
    "#                 )\n",
    "#         else:\n",
    "#             layers = [\n",
    "#                 DownSampleDWSLayer(\n",
    "#                     weight_shapes=weight_shapes,\n",
    "#                     bias_shapes=bias_shapes,\n",
    "#                     in_features=input_features,\n",
    "#                     out_features=hidden_dims[0],\n",
    "#                     reduction=reduction,\n",
    "#                     bias=bias,\n",
    "#                     n_fc_layers=n_fc_layers,\n",
    "#                     num_heads=num_heads,\n",
    "#                     set_layer=set_layer,\n",
    "#                     downsample_dim=input_dim_downsample,\n",
    "#                     add_skip=add_layer_skip,\n",
    "#                     init_scale=init_scale,\n",
    "#                     init_off_diag_scale_penalty=init_off_diag_scale_penalty,\n",
    "#                     diagonal=diagonal,\n",
    "#                 ),\n",
    "#             ]\n",
    "#             for i in range(n_hidden):\n",
    "#                 if bn:\n",
    "#                     layers.append(BN(hidden_dims[i], len(weight_shapes), len(bias_shapes)))\n",
    "\n",
    "#                 layers.extend(\n",
    "#                     [\n",
    "#                         ReLU(),\n",
    "#                         Dropout(dropout_rate),\n",
    "#                         DownSampleDWSLayer(\n",
    "#                             weight_shapes=weight_shapes,\n",
    "#                             bias_shapes=bias_shapes,\n",
    "#                             in_features=hidden_dims[i],\n",
    "#                             out_features=hidden_dims[i+1] if i != (n_hidden - 1) else output_features,\n",
    "#                             reduction=reduction,\n",
    "#                             bias=bias,\n",
    "#                             n_fc_layers=n_fc_layers,\n",
    "#                             num_heads=num_heads if i != (n_hidden - 1) else 1,\n",
    "#                             set_layer=set_layer,\n",
    "#                             downsample_dim=input_dim_downsample,\n",
    "#                             add_skip=add_layer_skip,\n",
    "#                             init_scale=init_scale,\n",
    "#                             init_off_diag_scale_penalty=init_off_diag_scale_penalty,\n",
    "#                             diagonal=diagonal,\n",
    "#                         ),\n",
    "#                     ]\n",
    "#                 )\n",
    "#         self.layers = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x: Tuple[Tuple[torch.tensor], Tuple[torch.tensor]]):\n",
    "#         out = self.layers(x)\n",
    "#         if self.add_skip:\n",
    "#             skip_out = tuple(self.skip(w) for w in x[0]), tuple(\n",
    "#                 self.skip(b) for b in x[1]\n",
    "#             )\n",
    "#             weight_out = tuple(ws + w for w, ws in zip(out[0], skip_out[0]))\n",
    "#             bias_out = tuple(bs + b for b, bs in zip(out[1], skip_out[1]))\n",
    "#             out = (weight_out, bias_out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea273fcd-5612-4b2c-8917-f139580fbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from nn.layers import BN, DownSampleDWSLayer, Dropout, DWSLayer, InvariantLayer, ReLU\n",
    "\n",
    "class DWSEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            weight_shapes,\n",
    "            bias_shapes,\n",
    "            input_features,\n",
    "            hidden_dims,\n",
    "            n_hidden=2,\n",
    "            output_features=None,\n",
    "            reduction=\"max\",\n",
    "            bias=True,\n",
    "            n_fc_layers=1,\n",
    "            num_heads=8,\n",
    "            set_layer=\"sab\",\n",
    "            input_dim_downsample=None,\n",
    "            dropout_rate=0.0,\n",
    "            init_scale=1e-4,\n",
    "            init_off_diag_scale_penalty=1.,\n",
    "            bn=False,\n",
    "            diagonal=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(weight_shapes) > 2, \"the current implementation only support input networks with M>2 layers.\"\n",
    "        assert len(hidden_dims) == n_hidden ,\"hidden dims length must be equal to the number of hidden dimensions\"\n",
    "        assert len(hidden_dims) == 2 , \"One hidden dimension needed for DWSLayer and another of Invariant Layer\"\n",
    "        if output_features is None:\n",
    "            output_features = hidden_dims[-1]\n",
    "\n",
    "        if input_dim_downsample is None:\n",
    "            layers = [\n",
    "                DWSLayer(\n",
    "                    weight_shapes=weight_shapes,\n",
    "                    bias_shapes=bias_shapes,\n",
    "                    in_features=input_features,\n",
    "                    out_features=hidden_dims[0],\n",
    "                    reduction=reduction,\n",
    "                    bias=bias,\n",
    "                    n_fc_layers=n_fc_layers,\n",
    "                    num_heads=num_heads,\n",
    "                    set_layer=set_layer,\n",
    "                    add_skip=add_layer_skip,\n",
    "                    init_scale=init_scale,\n",
    "                    init_off_diag_scale_penalty=init_off_diag_scale_penalty,\n",
    "                    diagonal=diagonal,\n",
    "                ),\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            layers = [\n",
    "                DownSampleDWSLayer(\n",
    "                    weight_shapes=weight_shapes,\n",
    "                    bias_shapes=bias_shapes,\n",
    "                    in_features=input_features,\n",
    "                    out_features=hidden_dims[0],\n",
    "                    reduction=reduction,\n",
    "                    bias=bias,\n",
    "                    n_fc_layers=n_fc_layers,\n",
    "                    num_heads=num_heads,\n",
    "                    set_layer=set_layer,\n",
    "                    downsample_dim=input_dim_downsample,\n",
    "                    add_skip=add_layer_skip,\n",
    "                    init_scale=init_scale,\n",
    "                    init_off_diag_scale_penalty=init_off_diag_scale_penalty,\n",
    "                    diagonal=diagonal,\n",
    "                ),\n",
    "            ]\n",
    "        \n",
    "        if bn:\n",
    "            layers.append(BN(hidden_dims[0], len(weight_shapes), len(bias_shapes)))\n",
    "        \n",
    "        layers.extend(\n",
    "            [\n",
    "                ReLU(),\n",
    "                Dropout(dropout_rate),\n",
    "                InvariantLayer(\n",
    "                    weight_shapes=weight_shapes,\n",
    "                    bias_shapes=bias_shapes,\n",
    "                    in_features= hidden_dims[0],\n",
    "                    out_features=hidden_dims[1],\n",
    "                    reduction=reduction,\n",
    "                    n_fc_layers=3,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tuple[Tuple[torch.tensor], Tuple[torch.tensor]]):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d2e1c-d89c-41d9-a382-5109e78f013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn.layers.base import BaseLayer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ReverseInvariantLayer(BaseLayer):\n",
    "    def __init__(self,\n",
    "                 weight_shapes,\n",
    "                 bias_shapes,\n",
    "                 batch_size,\n",
    "                 input_features,\n",
    "                 out_features):\n",
    "        super().__init__()\n",
    "        self.weight_shapes = weight_shapes\n",
    "        self.bias_shapes = bias_shapes\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_layers = len(weight_shapes) + len(bias_shapes)\n",
    "        \n",
    "        # Calculate the total output size required for the MLP\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_sizes = [batch_size * w[0] * w[1] * out_features for w in weight_shapes]\n",
    "        self.bias_sizes = [batch_size * b[0] * out_features for b in bias_shapes]\n",
    "        self.out_features = out_features # hidden_dims[0]\n",
    "        self.input_features = input_features # hidden_dims[1]\n",
    "        total_output_size = sum(weight_sizes) + sum(bias_sizes)\n",
    "        # Initialize the MLP layer\n",
    "        self.layer = self._get_mlp(\n",
    "            in_features= input_features,\n",
    "            out_features= total_output_size,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, output: torch.Tensor):\n",
    "        # First, apply the MLP to generate a tensor of shape (bs, total_output_size)\n",
    "        intermediate = self.layer(output)\n",
    "        # Start index for slicing\n",
    "        start_idx = 0\n",
    "        \n",
    "        # Extract weights\n",
    "        weights = []\n",
    "        for i, size in enumerate(self.weight_sizes):\n",
    "            end_idx = start_idx + size\n",
    "            weight_tensor = intermediate[:, start_idx:end_idx].view(self.batch_size, *self.weight_shapes[i], self.out_features)\n",
    "            weights.append(weight_tensor)\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        # Extract biases\n",
    "        biases = []\n",
    "        for i, size in enumerate(self.bias_sizes):\n",
    "            end_idx = start_idx + size\n",
    "            bias_tensor = intermediate[:, start_idx:end_idx].view(self.batch_size, *self.bias_shapes[i], self.out_features)\n",
    "            biases.append(bias_tensor)\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        return (tuple(weights), tuple(biases))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4d9ccda8-3c41-44a6-b75d-fb09d1ae5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from nn.layers import BN, DownSampleDWSLayer, Dropout, DWSLayer, InvariantLayer, ReLU\n",
    "\n",
    "class DWSDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            weight_shapes,\n",
    "            bias_shapes,\n",
    "            batch_size,\n",
    "            hidden_dims,\n",
    "            n_hidden=2,\n",
    "            output_features=None,\n",
    "            reduction=\"max\",\n",
    "            bias=True,\n",
    "            n_fc_layers=1,\n",
    "            num_heads=8,\n",
    "            set_layer=\"sab\",\n",
    "            input_dim_downsample=None,\n",
    "            dropout_rate=0.0,\n",
    "            add_layer_skip=False,\n",
    "            init_scale=1e-4,\n",
    "            init_off_diag_scale_penalty=1.,\n",
    "            bn=False,\n",
    "            diagonal=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(weight_shapes) > 2, \"the current implementation only support input networks with M>2 layers.\"\n",
    "        assert len(hidden_dims) == n_hidden ,\"hidden dims length must be equal to the number of hidden dimensions\"\n",
    "        assert len(hidden_dims) == 2 , \"One hidden dimension needed for DWSLayer and another of Invariant Layer\"\n",
    "        \n",
    "        layers = [\n",
    "            ReverseInvariantLayer(\n",
    "              weight_shapes=weight_shapes,\n",
    "              bias_shapes=bias_shapes,\n",
    "              batch_size = batch_size,\n",
    "              input_features = hidden_dims[1],\n",
    "              out_features = hidden_dims[0]\n",
    "            )\n",
    "        ]\n",
    "                \n",
    "        if bn:\n",
    "            layers.append(BN(hidden_dims[0], len(weight_shapes), len(bias_shapes)))\n",
    "        \n",
    "        if input_dim_downsample is None:\n",
    "            layers.extend(\n",
    "            [\n",
    "                ReLU(),\n",
    "                Dropout(dropout_rate),\n",
    "                DWSLayer(\n",
    "                    weight_shapes=weight_shapes,\n",
    "                    bias_shapes=bias_shapes,\n",
    "                    in_features=hidden_dims[0],\n",
    "                    out_features=out_features,\n",
    "                    reduction=reduction,\n",
    "                    bias=bias,\n",
    "                    n_fc_layers=n_fc_layers,\n",
    "                    num_heads=num_heads,\n",
    "                    set_layer=set_layer,\n",
    "                    add_skip=add_layer_skip,\n",
    "                    init_scale=init_scale,\n",
    "                    init_off_diag_scale_penalty=init_off_diag_scale_penalty,\n",
    "                    diagonal=diagonal,\n",
    "                ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            layers.extend(\n",
    "            [\n",
    "                ReLU(),\n",
    "                Dropout(dropout_rate),\n",
    "                DownSampleDWSLayer(\n",
    "                    weight_shapes=weight_shapes,\n",
    "                    bias_shapes=bias_shapes,\n",
    "                    in_features=hidden_dims[0],\n",
    "                    out_features=out_features,\n",
    "                    reduction=reduction,\n",
    "                    bias=bias,\n",
    "                    n_fc_layers=n_fc_layers,\n",
    "                    num_heads=num_heads,\n",
    "                    set_layer=set_layer,\n",
    "                    add_skip=add_layer_skip,\n",
    "                    init_scale=init_scale,\n",
    "                    init_off_diag_scale_penalty=init_off_diag_scale_penalty,\n",
    "                    diagonal=diagonal,\n",
    "                ),\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tuple[Tuple[torch.tensor], Tuple[torch.tensor]]):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7f09531-4f84-437d-9915-495c4163546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([2, 32]), torch.Size([32, 32]), torch.Size([32, 1])) (torch.Size([32]), torch.Size([32]), torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "point = train_set.__getitem__(0)\n",
    "weight_shapes = tuple(w.shape[:2] for w in point.weights)\n",
    "bias_shapes = tuple(b.shape[:1] for b in point.biases)\n",
    "print(weight_shapes,bias_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "03cc6e4a-fb7b-4036-8521-7e17a5a0d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our AutoEncoder using DWSModel\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_features,\n",
    "            weight_shapes,\n",
    "            bias_shapes,\n",
    "            hidden_dims,\n",
    "            n_hidden=2,\n",
    "            output_features=None,\n",
    "            input_dim_downsample=None,\n",
    "            add_skip=False,\n",
    "            bn = False,\n",
    "            batch_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = DWSEncoder(weight_shapes=weight_shapes,\n",
    "                                bias_shapes=bias_shapes,\n",
    "                                input_features=input_features,\n",
    "                                hidden_dims=hidden_dims,\n",
    "                                n_hidden=n_hidden,\n",
    "                                bn=bn).to(device)\n",
    "        self.decoder = DWSDecoder(weight_shapes=weight_shapes,\n",
    "                                bias_shapes=bias_shapes,\n",
    "                                batch_size= batch_size,\n",
    "                                hidden_dims=hidden_dims,\n",
    "                                output_features = input_features,\n",
    "                                n_hidden=n_hidden,\n",
    "                                bn=bn).to(device)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "#         weights1, biases1 = inputs\n",
    "#         weights1_tensors, biases1_tensors = [[torch.tensor(w, requires_grad=True) for w in x] for x in [weights1, biases1]]\n",
    "#         # Print the shapes of weights\n",
    "#         print(\"Shapes of weights1_tensors:\")\n",
    "#         for i, weight in enumerate(weights1_tensors):\n",
    "#             print(f\"Weight {i+1} shape: {weight.shape}\")\n",
    "\n",
    "#         # Print the shapes of biases\n",
    "#         print(\"Shapes of biases1_tensors:\")\n",
    "#         for i, bias in enumerate(biases1_tensors):\n",
    "#             print(f\"Bias {i+1} shape: {bias.shape}\")\n",
    "        encoded_data = self.encoder(inputs)   \n",
    "        output = self.decoder(encoded_data)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6eb3634-b1d5-4d1e-8edc-8f21aadc1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function for tuples of tensors because our AutoEncoder gets tensors as output (not sure if this is the correct approach)\n",
    "class TupleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TupleLoss, self).__init__()\n",
    "        self.base_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        weights1, biases1 = output\n",
    "        weights2, biases2 = target\n",
    "        weights1_tensors, weights2_tensors, biases1_tensors, biases2_tensors = [[torch.tensor(w, requires_grad=True) for w in x] for x in [weights1, weights2, biases1, biases2]]\n",
    "        total_loss = sum(self.base_loss(w1, w2) for w1, w2 in zip(weights1_tensors, weights2_tensors)) + \\\n",
    "             sum(self.base_loss(b1, b2) for b1, b2 in zip(biases1_tensors, biases2_tensors))\n",
    "\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "483e4810-56f0-4f47-a342-f2c7040787bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    total = 0.0\n",
    "    criterion =  TupleLoss()\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        inputs = (batch.weights, batch.biases)\n",
    "        out = model(inputs)\n",
    "        loss += criterion(out, inputs)\n",
    "        total += 1\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = loss / total\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "879569f2-c501-4d09-ac86-1d32358289c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def train_model(model):\n",
    "    learning_rate = 1e-3\n",
    "    num_epochs = 20\n",
    "    criterion =  TupleLoss()\n",
    "    epoch_iter = trange(num_epochs)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    test_loss = -1\n",
    "    for epoch in epoch_iter:\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            inputs = (batch.weights, batch.biases)\n",
    "            out = model(inputs)\n",
    "            loss = criterion(out, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_iter.set_description(\n",
    "                f\"[{epoch} {i+1}], train loss: {loss.item():.3f}, test loss: {test_loss:.3f}\"\n",
    "            )\n",
    "            test_loss =  evaluate(model, test_loader)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c4653c28-78ec-4248-a24f-04d6d5671421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]<ipython-input-113-965c2f2ae388>:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weights1_tensors, biases1_tensors = [[torch.tensor(w, requires_grad=True) for w in x] for x in [weights1, biases1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-e29e65f1d556>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weights1_tensors, weights2_tensors, biases1_tensors, biases2_tensors = [[torch.tensor(w, requires_grad=True) for w in x] for x in [weights1, weights2, biases1, biases2]]\n",
      "[0 1], train loss: 6.711, test loss: -1.000:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n",
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0 1], train loss: 6.711, test loss: -1.000:   0%|          | 0/20 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of weights1_tensors:\n",
      "Weight 1 shape: torch.Size([32, 2, 32, 1])\n",
      "Weight 2 shape: torch.Size([32, 32, 32, 1])\n",
      "Weight 3 shape: torch.Size([32, 32, 1, 1])\n",
      "Shapes of biases1_tensors:\n",
      "Bias 1 shape: torch.Size([32, 32, 1])\n",
      "Bias 2 shape: torch.Size([32, 32, 1])\n",
      "Bias 3 shape: torch.Size([32, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/shared/centos7/anaconda3/2021.05/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-0cba46cebcbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m ).to(device)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Outputs/model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e926278b0db7>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;34mf\"[{epoch} {i+1}], train loss: {loss.item():.3f}, test loss: {test_loss:.3f}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             )\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-f130ae65b65d>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-965c2f2ae388>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbiases1_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Bias {i+1} shape: {bias.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mencoded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-ba2bc7843e39>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_skip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             skip_out = tuple(self.skip(w) for w in x[0]), tuple(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/talisman/sgupta/DWSNets/nn/layers/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# add and normalize by the number of matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         new_weights = tuple(\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mw0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_weights_from_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_weights_from_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/talisman/sgupta/DWSNets/nn/layers/layers.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# add and normalize by the number of matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         new_weights = tuple(\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mw0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_weights_from_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_weights_from_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Model with DWS Layers and No Batch Normalization\n",
    "model = AutoEncoder(\n",
    "    input_features=1, \n",
    "    weights_size = weight_shapes, \n",
    "    bias_size = bias_shapes,\n",
    "    batch_size =32,\n",
    "    hidden_dims=[8,256],\n",
    "    n_hidden=2,\n",
    "    bn=False,\n",
    ").to(device)\n",
    "train_model(model)\n",
    "torch.save(model.state_dict(), \"Outputs/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff974152-65f1-447c-90db-3a794c464a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Model with DWS DownSample Layers\n",
    "# model2 = AutoEncoder(\n",
    "#     input_features=1,\n",
    "#     hidden_dims=[64,32],\n",
    "#     input_dim_downsample = 64,\n",
    "#     n_hidden=3,\n",
    "#     bn=False,\n",
    "# ).to(device)\n",
    "# train_model(model2)\n",
    "# torch.save(model.state_dict(), \"Outputs/model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081654b-66d3-4ff4-a438-c140f107a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Model with DWSLayers and Batch Normalization\n",
    "# model3 = AutoEncoder(\n",
    "#     input_features=1,\n",
    "#     hidden_dims=[128,64,32],\n",
    "#     n_hidden=3,\n",
    "#     bn=True,\n",
    "# ).to(device)\n",
    "# train_model(model3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
