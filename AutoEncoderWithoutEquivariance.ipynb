{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a4c62c-dd84-4529-b51d-fe4d10e2783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "from experiments.data import INRDataset\n",
    "from experiments.utils import (\n",
    "    common_parser,\n",
    "    count_parameters,\n",
    "    get_device,\n",
    "    set_logger,\n",
    "    set_seed,\n",
    "    str2bool,\n",
    ")\n",
    "from nn.models import DWSModelForClassification, MLPModelForClassification\n",
    "\n",
    "from experiments.mnist.generate_data_splits import generate_splits\n",
    "from experiments.mnist.compute_statistics import compute_stats\n",
    "\n",
    "set_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748aa66c-d884-4e12-9b60-08963b725b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Tesla P100-PCIE-12GB\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())  # The ID of the current GPU\n",
    "print(torch.cuda.get_device_name(0))  # The name of the specified GPU\n",
    "print(torch.cuda.device_count())  # The amount of GPUs that are accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae804791-6a1b-44ea-8613-320eb4b3c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af17711-6813-44bc-9255-fe3227516f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d648dfe5-e217-4c78-89dc-8a89b7e6867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/talisman/sgupta/DWSNets/equivariant-diffusion\n"
     ]
    }
   ],
   "source": [
    "#Loading inr data we created while mnist training\n",
    "import os\n",
    "current_working_directory = os.getcwd()\n",
    "print(current_working_directory)\n",
    "path = current_working_directory + \"/notebooks/dataset/mnist_splits.json\"\n",
    "statistics_path = current_working_directory + \"/notebooks/dataset/statistics.pth\"\n",
    "normalize = True\n",
    "augmentation = True\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c4cd5a5-40c4-4714-bdce-67c6cf64570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 13:00:25,460 - root - INFO - train size 55000, val size 5000, test size 10000\n"
     ]
    }
   ],
   "source": [
    "train_set = INRDataset(\n",
    "        path=path,\n",
    "        split=\"train\",\n",
    "        normalize=normalize,\n",
    "        augmentation=augmentation,\n",
    "        statistics_path=statistics_path,\n",
    "    )\n",
    "\n",
    "val_set = INRDataset(\n",
    "    path=path,\n",
    "    split=\"val\",\n",
    "    normalize=normalize,\n",
    "    statistics_path=statistics_path,\n",
    ")\n",
    "\n",
    "test_set = INRDataset(\n",
    "    path=path,\n",
    "    split=\"test\",\n",
    "    normalize=normalize,\n",
    "    statistics_path=statistics_path,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "logging.info(\n",
    "    f\"train size {len(train_set)}, \"\n",
    "    f\"val size {len(val_set)}, \"\n",
    "    f\"test size {len(test_set)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0271aad3-306a-4e58-9c5e-08ff2652bf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3eab7efc-56d3-4be4-b128-9230e5f6c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Latent_AE_cnn(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim,\n",
    "            time_step=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.enc1 = nn.Sequential(nn.Conv1d(1, 10, 3, stride=1),nn.LeakyReLU(),nn.Conv1d(1, 10, 3, stride=1),)\n",
    "        self.in_dim = in_dim\n",
    "        self.fold_rate = 5\n",
    "        self.kernal_size = 3\n",
    "        self.channel_list = [4, 4, 4, 4]\n",
    "        self.channel_list_dec = [8, 256, 256, 4]\n",
    "        print(self.fold_rate)\n",
    "        print(self.kernal_size)\n",
    "        print(self.channel_list)\n",
    "        print(self.channel_list_dec)\n",
    "        self.real_input_dim = (\n",
    "                int(in_dim / self.fold_rate ** 4 + 1) * self.fold_rate ** 4\n",
    "        )\n",
    "\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.InstanceNorm1d(self.real_input_dim),\n",
    "            nn.Conv1d(1, self.channel_list[0], self.kernal_size, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim),\n",
    "            nn.Conv1d(self.channel_list[0], self.channel_list[0], self.kernal_size, stride=self.fold_rate, padding=0),\n",
    "            # nn.MaxPool1d(2),\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate),\n",
    "            nn.Conv1d(self.channel_list[0], self.channel_list[0], self.kernal_size, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate),\n",
    "            nn.Conv1d(self.channel_list[0], self.channel_list[1], self.kernal_size, stride=self.fold_rate, padding=0),\n",
    "            # nn.MaxPool1d(2),\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate ** 2),\n",
    "            nn.Conv1d(self.channel_list[1], self.channel_list[1], self.kernal_size, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate ** 2),\n",
    "            nn.Conv1d(self.channel_list[1], self.channel_list[2], self.kernal_size, stride=self.fold_rate, padding=0),\n",
    "            # nn.MaxPool1d(2),\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate ** 3),\n",
    "            nn.Conv1d(self.channel_list[2], self.channel_list[2], self.kernal_size, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate ** 3),\n",
    "            nn.Conv1d(self.channel_list[2], self.channel_list[3], self.kernal_size, stride=self.fold_rate, padding=0),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate ** 4),\n",
    "            nn.ConvTranspose1d(\n",
    "                self.channel_list_dec[3], self.channel_list_dec[3], self.kernal_size, stride=self.fold_rate, padding=0\n",
    "            ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate ** 4),\n",
    "            nn.Conv1d(self.channel_list_dec[3], self.channel_list_dec[2], self.kernal_size, stride=1,\n",
    "                      padding=self.fold_rate - 1),\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate ** 3),\n",
    "            nn.ConvTranspose1d(\n",
    "                self.channel_list_dec[2], self.channel_list_dec[2], self.kernal_size, stride=self.fold_rate, padding=0\n",
    "            ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate ** 3),\n",
    "            nn.Conv1d(self.channel_list_dec[2], self.channel_list_dec[1], self.kernal_size, stride=1,\n",
    "                      padding=self.fold_rate - 1),\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate ** 2),\n",
    "            nn.ConvTranspose1d(\n",
    "                self.channel_list_dec[1], self.channel_list_dec[1], self.kernal_size, stride=self.fold_rate, padding=0\n",
    "            ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate ** 2),\n",
    "            nn.Conv1d(self.channel_list_dec[1], self.channel_list_dec[0], self.kernal_size, stride=1,\n",
    "                      padding=self.fold_rate - 1),\n",
    "        )\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate),\n",
    "            nn.ConvTranspose1d(\n",
    "                self.channel_list_dec[0], self.channel_list_dec[0], self.kernal_size, stride=self.fold_rate, padding=0\n",
    "            ),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.InstanceNorm1d(self.real_input_dim // self.fold_rate),\n",
    "            nn.Conv1d(self.channel_list_dec[0], 1, self.kernal_size, stride=1, padding=self.fold_rate),\n",
    "        )\n",
    "\n",
    "        # self.time_encode = nn.Embedding(time_step, self.real_input_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_shape = input.shape\n",
    "        if len(input.size()) == 2:\n",
    "            input = input.view(input.size(0), 1, -1)\n",
    "\n",
    "        input = torch.cat(\n",
    "            [\n",
    "                input,\n",
    "                torch.zeros(input.shape[0], 1, (self.real_input_dim - self.in_dim)).to(\n",
    "                    input.device\n",
    "                ),\n",
    "            ],\n",
    "            dim=2,\n",
    "        )\n",
    "        emb_enc1 = self.enc1(input)\n",
    "        emb_enc2 = self.enc2(emb_enc1)\n",
    "        emb_enc3 = self.enc3(emb_enc2)\n",
    "        emb_enc4 = self.enc4(emb_enc3)\n",
    "\n",
    "        emb_enc4 = emb_enc4 + torch.randn_like(emb_enc4) * 0.1\n",
    "\n",
    "        emb_dec1 = self.dec1(emb_enc4)\n",
    "        emb_dec2 = self.dec2(emb_dec1)\n",
    "        emb_dec3 = self.dec3(emb_dec2)\n",
    "        emb_dec4 = self.dec4(emb_dec3)[:, :, :input_shape[-1]]\n",
    "\n",
    "        return emb_dec4.reshape(input_shape)\n",
    "\n",
    "    def Enc(self, input):\n",
    "        if len(input.size()) == 2:\n",
    "            input = input.view(input.size(0), 1, -1)\n",
    "\n",
    "        input = torch.cat(\n",
    "            [\n",
    "                input,\n",
    "                torch.zeros(input.shape[0], 1, (self.real_input_dim - self.in_dim)).to(input.device),\n",
    "            ],\n",
    "            dim=2,\n",
    "        )\n",
    "        emb_enc1 = self.enc1(input)\n",
    "        emb_enc2 = self.enc2(emb_enc1)\n",
    "        emb_enc3 = self.enc3(emb_enc2)\n",
    "        emb_enc4 = self.enc4(emb_enc3)\n",
    "\n",
    "        return emb_enc4\n",
    "\n",
    "    def Dec(self, emb_enc4):\n",
    "        emb_dec1 = self.dec1(emb_enc4)\n",
    "        emb_dec2 = self.dec2(emb_dec1)\n",
    "        emb_dec3 = self.dec3(emb_dec2)\n",
    "        emb_dec4 = self.dec4(emb_dec3)[:, :, :self.in_dim]\n",
    "\n",
    "        return emb_dec4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a809cafe-c54e-4330-865b-cf95eb923e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_input(batch):\n",
    "\n",
    "    weights, biases = batch.weights, batch.biases\n",
    "\n",
    "    #  Flatten weights and biases\n",
    "    weights_flat = [w.view(w.size(0), -1) for w in weights]\n",
    "    biases_flat = [b.view(b.size(0), -1) for b in biases]\n",
    "\n",
    "    concatenated_layers = []\n",
    "\n",
    "    for w, b in zip(weights_flat, biases_flat):\n",
    "        concatenated_layers.append(w)\n",
    "        concatenated_layers.append(b)\n",
    "\n",
    "    # Concatenate all layers along the feature dimension\n",
    "    final_concatenated = torch.cat(concatenated_layers, dim=1)\n",
    "    return final_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2cd170ce-8e27-4de3-a5a5-ebcdbabf8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_model(model):\n",
    "    learning_rate = 1e-3\n",
    "    num_epochs = 100\n",
    "    criterion =  nn.MSELoss()\n",
    "    epoch_iter = trange(num_epochs)\n",
    "    epoch_loss = -1\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in epoch_iter:\n",
    "        total_loss = 0\n",
    "        counter = 0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = batch.to(device)\n",
    "            data = reshape_input(batch)\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_iter.set_description(\n",
    "                f\"[{epoch} {i+1}], train loss: {loss.item():.3f}, epoch loss: {epoch_loss:.3f}\"\n",
    "            )\n",
    "            \n",
    "            total_loss = total_loss + loss.item()\n",
    "            counter +=1\n",
    "        epoch_loss = total_loss/counter\n",
    "        if (epoch+1)%25 == 0:\n",
    "            torch.save(model.state_dict(), f\"Outputs/model4_epoch{epoch}_loss{epoch_loss}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5e4cc-b3e3-4276-85a9-8109c0e141fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n",
      "[4, 4, 4, 4]\n",
      "[8, 256, 256, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11 1487], train loss: 0.996, epoch loss: 1.025:  11%|â–ˆ         | 11/100 [54:23<5:41:34, 230.27s/it]"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = Latent_AE_cnn(\n",
    "   in_dim = 1185\n",
    ").to(device)\n",
    "train_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
