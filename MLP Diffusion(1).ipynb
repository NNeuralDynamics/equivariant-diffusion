{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001e0c61-f759-4615-9daa-20d0ecdc036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Downsampled MNIST tensor shape: torch.Size([14, 14])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time \n",
    "from collections import defaultdict\n",
    "import numpy.ma as ma\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch_geometric.data import Data, DataLoader  # PyG Data and loader\n",
    "from torch.optim import Adam, SGD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Define the transform to downsample the images to 14x14 pixels\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((14, 14)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset with the downsampling transform\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# # Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_features, test_labels = next(iter(test_loader))\n",
    "true = test_features[0].squeeze()\n",
    "plt.imshow(true, cmap=\"gray\")\n",
    "print(f\"Downsampled MNIST tensor shape: {true.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b62ddfa-7074-451e-8e9f-76308185c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MNIST classifier MLP class for dataset\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, init_type='xavier', seed=None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(196, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)  # Set a unique seed for reproducibility\n",
    "\n",
    "        self.init_weights(init_type)\n",
    "\n",
    "    def init_weights(self, init_type):\n",
    "        if init_type == 'xavier':\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "            nn.init.xavier_uniform_(self.fc2.weight)\n",
    "            nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        elif init_type == 'he':\n",
    "            nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "        else:\n",
    "            nn.init.normal_(self.fc1.weight)\n",
    "            nn.init.normal_(self.fc2.weight)\n",
    "            nn.init.normal_(self.fc3.weight)\n",
    "        \n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 196)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def test_mlp(model, test_loader, device = device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to compute gradients for evaluation\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train_mlp(model, epochs=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e7c1d3-dfe4-440c-8511-eba6ce65ea8b",
   "metadata": {},
   "source": [
    "# Git Rebasin Permutation Canonicalization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76367922-c918-4e25-87c9-a2811dc21342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for permutations and weight matching\n",
    "def permute_layer_weights(layer, perm):\n",
    "    \"\"\"Permutes weights and biases of a layer based on the given permutation matrix.\"\"\"\n",
    "    weight = layer.weight.data.clone()\n",
    "    bias = layer.bias.data.clone()\n",
    "\n",
    "    # Permute rows of weights (output dimension)\n",
    "    layer.weight.data = weight[perm, :]\n",
    "\n",
    "    # Permute bias (output dimension)\n",
    "    layer.bias.data = bias[perm]\n",
    "    return layer\n",
    "\n",
    "def permute_model(model, permutations):\n",
    "    \"\"\"Applies a list of permutations to a model's layers.\"\"\"\n",
    "    permute_layer_weights(model.fc1, permutations[0])\n",
    "    permute_layer_weights(model.fc2, permutations[1])\n",
    "\n",
    "    # Adjust input weights of the second hidden layer\n",
    "    model.fc2.weight.data = model.fc2.weight.data[:, permutations[0]]\n",
    "    # Adjust input weights of the output layer\n",
    "    model.fc3.weight.data = model.fc3.weight.data[:, permutations[1]]\n",
    "    return model\n",
    "\n",
    "# Functions to permute weights and adjust input weights\n",
    "def apply_permutation(layer, perm):\n",
    "    \"\"\"\n",
    "    Applies the given permutation to the weights and biases of a layer.\n",
    "    Args:\n",
    "        layer: The layer to permute.\n",
    "        perm: The permutation array.\n",
    "    \"\"\"\n",
    "    # Permute rows (output dimension) of weights\n",
    "    layer.weight.data = layer.weight.data[perm, :]\n",
    "    # Permute biases\n",
    "    layer.bias.data = layer.bias.data[perm]\n",
    "\n",
    "def adjust_input_weights(layer, perm):\n",
    "    \"\"\"\n",
    "    Adjusts the input wei\n",
    "    ghts of a layer according to the permutation of the previous layer.\n",
    "    Args:\n",
    "        layer: The layer to adjust.\n",
    "        perm: The permutation array of the previous layer.\n",
    "    \"\"\"\n",
    "    layer.weight.data = layer.weight.data[:, perm]\n",
    "    \n",
    "def generate_permutation_matrix(hidden_dim):\n",
    "    \"\"\"Generates a random permutation matrix.\"\"\"\n",
    "    perm = np.random.permutation(hidden_dim)\n",
    "    return torch.tensor(perm, dtype=torch.long)\n",
    "\n",
    "def compute_similarity_matrix(weights_a, weights_b):\n",
    "    \"\"\"\n",
    "    Computes the similarity matrix for two layers' weights.\n",
    "    \"\"\"\n",
    "    weights_a = weights_a.view(weights_a.size(0), -1)\n",
    "    weights_b = weights_b.view(weights_b.size(0), -1)\n",
    "    return torch.matmul(weights_a, weights_b.T)\n",
    "\n",
    "def get_permuted_weights(weights, perm):\n",
    "    \"\"\"\n",
    "    Permutes the rows of weights based on the given permutation.\n",
    "    \"\"\"\n",
    "    return weights[perm, :]\n",
    "\n",
    "\n",
    "def weight_matching(rng, model_a, model_b, max_iter=100, init_perm=None):\n",
    "    \"\"\"\n",
    "    Optimizes permutations to match Model B to Model A.\n",
    "    \"\"\"\n",
    "    layers = ['fc1', 'fc2', 'fc3']\n",
    "    perm_sizes = {layer: getattr(model_a, layer).weight.size(0) for layer in layers[:-1]}\n",
    "    perm = {layer: np.arange(size) for layer, size in perm_sizes.items()} if init_perm is None else init_perm\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        progress = False\n",
    "        for layer in layers[:-1]:  # Exclude the output layer (fc3)\n",
    "            n = perm_sizes[layer]\n",
    "            similarity_matrix = compute_similarity_matrix(\n",
    "                getattr(model_a, layer).weight.data,\n",
    "                get_permuted_weights(getattr(model_b, layer).weight.data, perm[layer])\n",
    "            )\n",
    "            row_ind, col_ind = linear_sum_assignment(similarity_matrix.numpy(), maximize=True)\n",
    "\n",
    "            old_score = np.sum(similarity_matrix.numpy()[np.arange(n), perm[layer]])\n",
    "            new_score = np.sum(similarity_matrix.numpy()[np.arange(n), col_ind])\n",
    "\n",
    "            if new_score > old_score:\n",
    "                perm[layer] = col_ind\n",
    "                progress = True\n",
    "\n",
    "        if not progress:\n",
    "            break\n",
    "\n",
    "    return perm, new_score\n",
    "\n",
    "def rebase_model_b_to_a(rng, model_a, model_b, max_iter=100):\n",
    "    \"\"\"\n",
    "    Rebases Model B to match Model A using weight matching.\n",
    "    \"\"\"\n",
    "    # Generate initial permutations\n",
    "    init_perm = {layer: generate_permutation_matrix(getattr(model_a, layer).weight.size(0)) for layer in ['fc1', 'fc2']}\n",
    "    \n",
    "    # Perform weight matching\n",
    "    permutations, score = weight_matching(rng, model_a, model_b, max_iter=max_iter, init_perm=init_perm)\n",
    "    \n",
    "    # Apply the permutations to Model B\n",
    "    model_b = permute_model(model_b, [permutations['fc1'], permutations['fc2']])\n",
    "    \n",
    "    return model_b, permutations, score\n",
    "\n",
    "def load_model_weights(model, weight_path):\n",
    "    # Load weights and biases\n",
    "    weights, biases = torch.load(weight_path)\n",
    "    model.fc1.weight.data = weights[0]\n",
    "    model.fc2.weight.data = weights[1]\n",
    "    model.fc3.weight.data = weights[2]\n",
    "    model.fc1.bias.data = biases[0]\n",
    "    model.fc2.bias.data = biases[1]\n",
    "    model.fc3.bias.data = biases[2]\n",
    "    return model\n",
    "\n",
    "def reconstruct_model(model, final_permutations):\n",
    "    \"\"\"\n",
    "    Reconstruct model_b by applying the final permutations to its layers.\n",
    "    Args:\n",
    "        model_b (MLP): The model to be reconstructed.\n",
    "        final_permutations (dict): Dictionary containing layer-wise permutations.\n",
    "    Returns:\n",
    "        MLP: The reconstructed model.\n",
    "    \"\"\"\n",
    "    # Apply permutations to fc1 layer\n",
    "    apply_permutation(model.fc1, final_permutations[1]['fc1'])\n",
    "    adjust_input_weights(model.fc2, final_permutations[1]['fc1'])\n",
    "    \n",
    "    # Apply permutations to fc2 layer\n",
    "    apply_permutation(model.fc2, final_permutations[1]['fc2'])\n",
    "    adjust_input_weights(model.fc3, final_permutations[1]['fc2'])\n",
    "    \n",
    "    # Return the reconstructed model\n",
    "    return model\n",
    "\n",
    "def extract_weights_and_biases(model):\n",
    "    \"\"\"Flatten the weights and biases of each layer in the model into a single vector.\"\"\"\n",
    "    all_weights_biases = []\n",
    "    \n",
    "    # Flatten weights and biases for each layer\n",
    "    for layer_name in ['fc1', 'fc2', 'fc3']:\n",
    "        weights = getattr(model, layer_name).weight.data.numpy().flatten()  # Flatten the weights\n",
    "        biases = getattr(model, layer_name).bias.data.numpy().flatten()    # Flatten the biases\n",
    "        all_weights_biases.extend(weights)\n",
    "        all_weights_biases.extend(biases)\n",
    "    \n",
    "    return np.array(all_weights_biases)\n",
    "\n",
    "def get_permuted_models_data(ref_point=0, N = 10, path = f\"models/reduced_mlp_weights_{0}.pt\"):\n",
    "    ref_model = MLP()\n",
    "    ref_model_path = path #f\"models/reduced_mlp_weights_{ref_point}.pt\"\n",
    "#     ref_model_path =   f\"models/dropout_mlp_weights_{ref_point}.pt\"\n",
    "\n",
    "    ref_model = load_model_weights(ref_model, ref_model_path)\n",
    "    \n",
    "    org_models=[]\n",
    "    permuted_models = []\n",
    "    full_times = []\n",
    "    rebasin_times = []\n",
    "    scores = []\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    for i in range(0,N):\n",
    "        if i == ref_point:\n",
    "            continue\n",
    "        all_start = time.time() # start timing\n",
    "        model_path = f\"models/reduced_mlp_weights_{i}.pt\"\n",
    "#         model_path = f\"models/dropout_mlp_weights_{i}.pt\"\n",
    "        \n",
    "        model = MLP()\n",
    "        model = load_model_weights(model, model_path)\n",
    "        org_models.append(model)\n",
    "        loaded_model = time.time() # end timing for loading the model from .pth file\n",
    "        \n",
    "        reconstructed_model, final_permutations, score = rebase_model_b_to_a(rng, ref_model, model, max_iter=100)\n",
    "        scores.append(score)\n",
    "        permuted_models.append(reconstructed_model)\n",
    "        all_end = time.time() # end timing for the entire loop\n",
    "        \n",
    "        full_times.append(all_end - all_start)\n",
    "        rebasin_times.append(all_end - loaded_model)\n",
    "        \n",
    "#         print(f\"Org model accuracy:{test_mlp(model, test_loader)} Reconstructed model accuracy:{test_mlp(reconstructed_model, test_loader)}\")\n",
    "    return org_models, permuted_models, scores, rebasin_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d87e403-3c19-4c5d-b022-d550f81e80ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/697gbr4j3lzdh9ccy7qxb5ph0000gn/T/ipykernel_63343/800785483.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights, biases = torch.load(weight_path)\n"
     ]
    }
   ],
   "source": [
    "org_models, permuted_models, scores, rebasin_times = get_permuted_models_data(ref_point=0, N = 45) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4595db-4434-414e-9422-a2b05f6743cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd598785-9307-409a-8b3d-ab93500a96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "###############################\n",
    "# 1. Dataset: Neuron-level Graph Representation\n",
    "###############################\n",
    "# class MLPGraphDatasetNeurons(torch.utils.data.Dataset):\n",
    "#     def __init__(self, model_folder):\n",
    "#         # Collect all .pt files in the folder.\n",
    "#         self.model_paths = [os.path.join(model_folder, fname) \n",
    "#                             for fname in os.listdir(model_folder) if fname.endswith('.pt')]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.model_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Each file stores (weights, biases) using weights_only=True.\n",
    "#         weights, biases = torch.load(self.model_paths[idx], weights_only=True)\n",
    "#         # --- Build Node Features ---\n",
    "#         # Input layer: use the input dimension from the first weight matrix.\n",
    "#         input_dim = weights[0].shape[1]\n",
    "#         input_feats = torch.zeros(input_dim, 1)\n",
    "#         node_features = [input_feats]\n",
    "#         for b in biases:\n",
    "#             # Each bias is a 1D tensor; reshape it as a column vector.\n",
    "#             node_feats = b.view(-1, 1)\n",
    "#             node_features.append(node_feats)\n",
    "#         x = torch.cat(node_features, dim=0)\n",
    "\n",
    "#         # --- Build Edges and Edge Attributes ---\n",
    "#         edge_index_list = []\n",
    "#         edge_attr_list = []\n",
    "#         offset = 0  # starting index for current layer's nodes\n",
    "#         for w in weights:\n",
    "#             in_dim = w.shape[1]    # nodes in source layer\n",
    "#             out_dim = w.shape[0]   # nodes in destination layer\n",
    "#             src_offset = offset\n",
    "#             dst_offset = offset + in_dim\n",
    "#             for i_out in range(out_dim):\n",
    "#                 for j_in in range(in_dim):\n",
    "#                     src = src_offset + j_in\n",
    "#                     dst = dst_offset + i_out\n",
    "#                     edge_index_list.append([src, dst])\n",
    "#                     edge_attr_list.append([w[i_out, j_in].item()])\n",
    "#             offset += in_dim\n",
    "#         # Convert lists into tensors.\n",
    "#         edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "#         edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "#         return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "class MLPGraphDatasetNeuronsFromList(torch.utils.data.Dataset):\n",
    "    def __init__(self, mlp_list):\n",
    "        \"\"\"\n",
    "        mlp_list: a list of MLP objects.\n",
    "        \"\"\"\n",
    "        self.models = mlp_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.models)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the MLP instance from the list.\n",
    "        mlp = self.models[idx]\n",
    "        # Extract weights and biases directly from the model's layers.\n",
    "        # We assume the model has attributes fc1, fc2, fc3.\n",
    "        weights = [mlp.fc1.weight, mlp.fc2.weight, mlp.fc3.weight]\n",
    "        biases = [mlp.fc1.bias, mlp.fc2.bias, mlp.fc3.bias]\n",
    "\n",
    "        # --- Build Node Features ---\n",
    "        # Input layer: create nodes based on the input dimension of fc1.\n",
    "        input_dim = weights[0].shape[1]\n",
    "        input_feats = torch.zeros(input_dim, 1)\n",
    "        # For subsequent layers, use the bias of each layer as the node feature.\n",
    "        node_features = [input_feats]\n",
    "        for b in biases:\n",
    "            node_feats = b.view(-1, 1)\n",
    "            node_features.append(node_feats)\n",
    "        # Concatenate features from all layers; total nodes = input_dim + sum(layer output sizes)\n",
    "        x = torch.cat(node_features, dim=0)\n",
    "\n",
    "        # --- Build Edges and Edge Attributes ---\n",
    "        edge_index_list = []\n",
    "        edge_attr_list = []\n",
    "        offset = 0  # starting index for current layer's nodes\n",
    "        # Iterate over each weight matrix corresponding to a layer.\n",
    "        for w in weights:\n",
    "            in_dim = w.shape[1]    # number of nodes in source layer\n",
    "            out_dim = w.shape[0]   # number of nodes in destination layer\n",
    "            src_offset = offset\n",
    "            dst_offset = offset + in_dim\n",
    "            for i_out in range(out_dim):\n",
    "                for j_in in range(in_dim):\n",
    "                    src = src_offset + j_in\n",
    "                    dst = dst_offset + i_out\n",
    "                    edge_index_list.append([src, dst])\n",
    "                    edge_attr_list.append([w[i_out, j_in].item()])\n",
    "            offset += in_dim  # move the offset for the next layer\n",
    "\n",
    "        # Convert lists to torch tensors.\n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "def vgae_to_mlp(generated_data):\n",
    "    \"\"\"\n",
    "    generated_data: a Data object that has the reconstructed node features (x)\n",
    "    and edge attributes (edge_attr).  \n",
    "    The original graph was built as:\n",
    "      - First 196 nodes: input layer,\n",
    "      - Next 32 nodes: fc1 neurons,\n",
    "      - Next 32 nodes: fc2 neurons,\n",
    "      - Next 10 nodes: fc3 neurons.\n",
    "    Edge ordering is:\n",
    "      - fc1: edges from input (196 nodes) to fc1 (32 nodes) in a nested loop:\n",
    "             for i in range(32): for j in range(196)\n",
    "      - fc2: edges from fc1 (32 nodes) to fc2 (32 nodes)\n",
    "      - fc3: edges from fc2 (32 nodes) to fc3 (10 nodes)\n",
    "    This function instantiates a new MLP and sets its weights and biases from the generated outputs.\n",
    "    \"\"\"\n",
    "    x_rec = generated_data.x.squeeze()  # shape: (196+32+32+10,)\n",
    "    edge_attr_rec = generated_data.edge_attr  # shape: (32*196 + 32*32 + 10*32,)\n",
    "\n",
    "    # Extract biases.\n",
    "    fc1_bias = x_rec[196:196+32]\n",
    "    fc2_bias = x_rec[196+32:196+32+32]\n",
    "    fc3_bias = x_rec[196+32+32:196+32+32+10]\n",
    "\n",
    "    # Extract edge weights.\n",
    "    # For fc1: first 32*196 values.\n",
    "    fc1_weight = edge_attr_rec[:32*196].view(32, 196)\n",
    "    # For fc2: next 32*32 values.\n",
    "    start_fc2 = 32*196\n",
    "    fc2_weight = edge_attr_rec[start_fc2:start_fc2+32*32].view(32, 32)\n",
    "    # For fc3: remaining 10*32 values.\n",
    "    start_fc3 = start_fc2 + 32*32\n",
    "    fc3_weight = edge_attr_rec[start_fc3:start_fc3+10*32].view(10, 32)\n",
    "\n",
    "    # Create a new instance of the MLP.\n",
    "    new_mlp = MLP()\n",
    "    # Assign weights and biases.\n",
    "    with torch.no_grad():\n",
    "        new_mlp.fc1.weight.copy_(fc1_weight)\n",
    "        new_mlp.fc1.bias.copy_(fc1_bias)\n",
    "        new_mlp.fc2.weight.copy_(fc2_weight)\n",
    "        new_mlp.fc2.bias.copy_(fc2_bias)\n",
    "        new_mlp.fc3.weight.copy_(fc3_weight)\n",
    "        new_mlp.fc3.bias.copy_(fc3_bias)\n",
    "    return new_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6707172-5af3-4f2a-9255-d3174101e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import NNConv, GraphNorm\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Encoder: edge‐conditioned graph conv with residuals & normalization\n",
    "# -----------------------------------------------------------------------------\n",
    "class EdgeConditionedEncoder(nn.Module):\n",
    "    def __init__(self, in_ch, hidden_ch, latent_dim):\n",
    "        super().__init__()\n",
    "        # MLP to produce per-edge weight kernels\n",
    "        self.edge_mlp1 = nn.Sequential(\n",
    "            nn.Linear(1, in_ch * hidden_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_ch * hidden_ch, in_ch * hidden_ch)\n",
    "        )\n",
    "        self.conv1 = NNConv(in_ch, hidden_ch, self.edge_mlp1, aggr='mean')\n",
    "        self.norm1 = GraphNorm(hidden_ch)\n",
    "\n",
    "        self.edge_mlp2 = nn.Sequential(\n",
    "            nn.Linear(1, hidden_ch * hidden_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_ch * hidden_ch, hidden_ch * hidden_ch)\n",
    "        )\n",
    "        self.conv2 = NNConv(hidden_ch, hidden_ch, self.edge_mlp2, aggr='mean')\n",
    "        self.norm2 = GraphNorm(hidden_ch)\n",
    "\n",
    "        # final mapping to latent space\n",
    "        self.lin = nn.Linear(hidden_ch, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        ew = edge_weight.view(-1,1)  # [E,1]\n",
    "        h0 = x\n",
    "        h  = F.relu(self.norm1(self.conv1(x, edge_index, ew)))\n",
    "        h  = h + h0                        # residual\n",
    "        h1 = h\n",
    "        h  = F.relu(self.norm2(self.conv2(h, edge_index, ew)))\n",
    "        h  = h + h1                        # residual\n",
    "        z  = self.lin(h)                   # [N, latent_dim]\n",
    "        return z\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Joint Decoder: reconstruct both edge weights & node features\n",
    "# -----------------------------------------------------------------------------\n",
    "class JointDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # project node embeddings\n",
    "        self.node_proj   = nn.Linear(latent_dim, hidden_dim)\n",
    "        # project raw edge weights\n",
    "        self.edge_proj   = nn.Linear(1, hidden_dim)\n",
    "        # MLP heads\n",
    "        self.edge_h1     = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.edge_out    = nn.Linear(hidden_dim, 1)\n",
    "        self.node_h1     = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.node_out    = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, z, edge_index, edge_weight):\n",
    "        # z: [N, latent_dim]\n",
    "        # edge_index: [2, E], edge_weight: [E,1] or [E]\n",
    "        if edge_weight.dim() == 1:\n",
    "            ew = edge_weight.unsqueeze(1)\n",
    "        else:\n",
    "            ew = edge_weight  # [E,1]\n",
    "\n",
    "        # project node embeddings\n",
    "        h_n = F.relu(self.node_proj(z))           # [N, H]\n",
    "\n",
    "        # build edge-specific features\n",
    "        src, dst = edge_index\n",
    "        h_e = h_n[src] + h_n[dst]                  # [E, H]\n",
    "        h_ew = F.relu(self.edge_proj(ew))          # [E, H]\n",
    "        h_comb = F.relu(h_e + h_ew)                # [E, H]\n",
    "\n",
    "        # edge reconstruction\n",
    "        he = F.relu(self.edge_h1(h_comb))\n",
    "        edge_pred = self.edge_out(he).squeeze()    # [E]\n",
    "\n",
    "        # node reconstruction\n",
    "        hn = F.relu(self.node_h1(h_n))\n",
    "        node_pred = self.node_out(hn).squeeze()    # [N]\n",
    "\n",
    "        return edge_pred, node_pred\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Graph Autoencoder wrapper\n",
    "# -----------------------------------------------------------------------------\n",
    "class GraphAutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, data):\n",
    "        z = self.encoder(data.x, data.edge_index, data.edge_attr.view(-1))\n",
    "        return z\n",
    "\n",
    "    def compute_loss(self, data):\n",
    "        z = self.encoder(data.x, data.edge_index, data.edge_attr.view(-1))\n",
    "        ep, npred = self.decoder(z, data.edge_index, data.edge_attr.view(-1))\n",
    "        loss_e = F.mse_loss(ep, data.edge_attr.view(-1))\n",
    "        loss_n = F.mse_loss(npred, data.x.view(-1))\n",
    "        return loss_e + loss_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c79eeb3-20ed-4d6e-8333-e323951c9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MLPGraphDatasetNeurons(\"models\")\n",
    "# loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "dataset = MLPGraphDatasetNeuronsFromList(permuted_models)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d0aac52-ed19-43b8-88d2-c7ebec8a1949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Recon Loss: 0.0726\n",
      "Epoch 002, Recon Loss: 0.0602\n",
      "Epoch 003, Recon Loss: 0.0294\n",
      "Epoch 004, Recon Loss: 0.0274\n",
      "Epoch 005, Recon Loss: 0.0274\n",
      "Epoch 006, Recon Loss: 0.0177\n",
      "Epoch 007, Recon Loss: 0.0157\n",
      "Epoch 008, Recon Loss: 0.0163\n",
      "Epoch 009, Recon Loss: 0.0112\n"
     ]
    }
   ],
   "source": [
    "# Assume you have a Python list of MLPs called `mlp_list`\n",
    "# dataset = MLPGraphDatasetNeuronsFromList(mlp_list)\n",
    "# loader  = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "in_ch = 1\n",
    "hidden_ch = 16\n",
    "latent_dim = 64\n",
    "hidden_dim = 128\n",
    "\n",
    "encoder = EdgeConditionedEncoder(in_ch=in_ch,   hidden_ch=hidden_ch, latent_dim=latent_dim)\n",
    "decoder = JointDecoder(latent_dim=latent_dim,        hidden_dim=hidden_dim)\n",
    "gae     = GraphAutoEncoder(encoder, decoder)\n",
    "\n",
    "optimizer = torch.optim.Adam(gae.parameters(), lr=1e-3)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(1, 10):\n",
    "    gae.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = gae.compute_loss(data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch:03d}, Recon Loss: {total_loss/len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10245dcf-db50-4b48-8545-57dd71850fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoising GAE has 109954 parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Function: Count the learnable params in a portion of the model\n",
    "    Args: model segment (ie, encoder, decoder, entire model)\n",
    "    Returns: the number of learnable params in that model/segment\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "print(f\"Denoising GAE has {count_parameters(gae)} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db1fff-28e7-4978-a673-ef6d5aa03c07",
   "metadata": {},
   "source": [
    "# Test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fd09e73-51d4-4c66-b45a-2b7c70046d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def reconstruct_nth_mlp(gae, dataset, n = 0, device = device):\n",
    "    \"\"\"\n",
    "    Reconstructs the first MLP in `dataset` via your deterministic GAE.\n",
    "    \n",
    "    Args:\n",
    "        gae      : the trained GraphAutoEncoder (with .encoder and .decoder).\n",
    "        dataset  : your MLPGraphDatasetNeuronsFromList (or equivalent).\n",
    "        device   : torch.device ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "        new_mlp  : an MLP instance whose weights & biases were produced\n",
    "                   by the GAE reconstructions.\n",
    "    \"\"\"\n",
    "    # 1) Grab the first graph\n",
    "    data = dataset[n]\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # 2) Run through encoder + decoder\n",
    "    gae.eval()\n",
    "    with torch.no_grad():\n",
    "        # encode\n",
    "        z = gae.encoder(data.x, data.edge_index, data.edge_attr.view(-1))\n",
    "        # decode edges & nodes\n",
    "        if hasattr(gae.decoder, 'forward'):\n",
    "            # JointDecoder interface: returns (edge_pred, node_pred)\n",
    "            edge_pred, node_pred = gae.decoder(z, data.edge_index, data.edge_attr.view(-1))\n",
    "        else:\n",
    "            # If you have separate decoders:\n",
    "            edge_pred = gae.edge_decoder(z, data.edge_index, data.edge_attr.view(-1))\n",
    "            node_pred = gae.node_decoder(z, data.edge_index, data.edge_attr.view(-1))\n",
    "    \n",
    "    # 3) Build a new Data object with the reconstructions\n",
    "    rec_data = Data(\n",
    "        x         = node_pred.unsqueeze(1),      # [N,1]\n",
    "        edge_index= data.edge_index,             # same topology\n",
    "        edge_attr = edge_pred.unsqueeze(1)       # [E,1]\n",
    "    )\n",
    "    \n",
    "    # 4) Convert graph back to an MLP\n",
    "    # This is the same vgae_to_mlp function from before:\n",
    "    new_mlp = vgae_to_mlp(rec_data)\n",
    "    return new_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6af031e3-0322-472f-a0a6-09d44cfd23e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed MLP test accuracy: 65.81%\n"
     ]
    }
   ],
   "source": [
    "# assuming you have:\n",
    "#   gae         : your trained GraphAutoEncoder\n",
    "#   dataset     : your MLPGraphDatasetNeuronsFromList instance\n",
    "#   device      : torch.device('cuda' or 'cpu')\n",
    "#   test_loader : your MNIST DataLoader\n",
    "#   test_mlp    : function(mlp, device, test_loader) -> accuracy/loss\n",
    "\n",
    "# reconstruct one MLP and immediately test it\n",
    "generated_mlp = reconstruct_nth_mlp(gae, dataset)\n",
    "accuracy = test_mlp(generated_mlp, test_loader)\n",
    "print(f\"Reconstructed MLP test accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "715e6351-e439-42de-b326-59dc9bb38efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed MLP test accuracy: 65.81%\n",
      "Reconstructed MLP test accuracy: 68.05%\n",
      "Reconstructed MLP test accuracy: 53.97%\n",
      "Reconstructed MLP test accuracy: 61.73%\n",
      "Reconstructed MLP test accuracy: 43.83%\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "accuracies = []\n",
    "for i in range(n): \n",
    "    generated_mlp = reconstruct_nth_mlp(gae, dataset, i)\n",
    "    accuracy = test_mlp(generated_mlp, test_loader)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Reconstructed MLP test accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54d8e6ae-08b0-4534-af7c-c66cc56ceeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[270, 1], edge_index=[2, 7616], edge_attr=[7616, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test = dataset[0]\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e154b060-a775-4c1a-abc6-027f512fd738",
   "metadata": {},
   "source": [
    "# Diffusion: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39801bf4-3287-4ce9-9c72-d3df1350044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1) Sinusoidal Time Embeddings\n",
    "# -------------------------------------------------------\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, t):\n",
    "        half = self.dim // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(half, device=t.device) / (half - 1))\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        return torch.cat([args.sin(), args.cos()], dim=-1)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) Residual 2D Block with Time Conditioning\n",
    "# -------------------------------------------------------\n",
    "class ResidualBlock2D(nn.Module):\n",
    "    def __init__(self, channels, time_emb_dim):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(8, channels)\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, channels)\n",
    "    def forward(self, x, t_emb):\n",
    "        # x: [B, C, H, W], t_emb: [B, time_emb_dim]\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "        t = self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = h + t\n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv2(h)\n",
    "        return x + h\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) Simplified 2D U-Net for Latent Matrices\n",
    "# -------------------------------------------------------\n",
    "class UNet2D(nn.Module):\n",
    "    def __init__(self, latent_dim, base_ch=32, time_emb_dim=16):\n",
    "        super().__init__()\n",
    "        # time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim * 4, time_emb_dim)\n",
    "        )\n",
    "        # input projection (1 channel to base_ch)\n",
    "        self.init_conv = nn.Conv2d(1, base_ch, kernel_size=3, padding=1)\n",
    "        # down block\n",
    "        self.down_block = ResidualBlock2D(base_ch, time_emb_dim)\n",
    "        self.down_sample = nn.Conv2d(base_ch, base_ch * 2, kernel_size=4, stride=2, padding=1)\n",
    "        # bottleneck\n",
    "        self.mid_block = ResidualBlock2D(base_ch * 2, time_emb_dim)\n",
    "        # up block\n",
    "        self.up_sample = nn.ConvTranspose2d(base_ch * 2, base_ch, kernel_size=4, stride=2, padding=1)\n",
    "        self.up_block = ResidualBlock2D(base_ch * 2, time_emb_dim)\n",
    "        # final conv back to 1 channel\n",
    "        self.final_conv = nn.Conv2d(base_ch * 2, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # x: [B,1,N_nodes,latent_dim], t: [B]\n",
    "        t_emb = self.time_mlp(t)\n",
    "        # encode\n",
    "        h0 = self.init_conv(x)                    # [B, base_ch, H, W]\n",
    "        h1 = self.down_block(h0, t_emb)           # [B, base_ch, H, W]\n",
    "        h2 = self.down_sample(h1)                 # [B, base_ch*2, H/2, W/2]\n",
    "        # bottleneck\n",
    "        h3 = self.mid_block(h2, t_emb)            # [B, base_ch*2, H/2, W/2]\n",
    "        # decode\n",
    "        h4 = self.up_sample(h3)                   # [B, base_ch, H, W]\n",
    "        h4 = torch.cat([h4, h1], dim=1)           # [B, base_ch*2, H, W]\n",
    "        h5 = self.up_block(h4, t_emb)             # [B, base_ch*2, H, W]\n",
    "        return self.final_conv(h5)                # [B,1,H,W]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4) Diffusion Scheduler\n",
    "# -------------------------------------------------------\n",
    "class DiffusionScheduler:\n",
    "    def __init__(self, timesteps=100, beta_start=1e-4, beta_end=0.02):\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        a = self.alphas_cumprod[t][:, None, None, None]\n",
    "        return a.sqrt() * x_start + (1 - a).sqrt() * noise, noise\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5) Latent Diffusion Model Wrapper for 2D\n",
    "# -------------------------------------------------------\n",
    "class LatentDiffusion2D(nn.Module):\n",
    "    def __init__(self, unet, scheduler):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.scheduler = scheduler\n",
    "    def training_step(self, x0):\n",
    "        # x0: [B,1,H,W]\n",
    "        bsz = x0.size(0)\n",
    "        t = torch.randint(0, self.scheduler.timesteps, (bsz,), device=x0.device)\n",
    "        x_noisy, noise = self.scheduler.q_sample(x0, t)\n",
    "        pred = self.unet(x_noisy, t)\n",
    "        return F.mse_loss(pred, noise)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6) Building a Latent Loader\n",
    "# -------------------------------------------------------\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch.utils.data import Dataset, DataLoader as TorchDataLoader\n",
    "\n",
    "class GraphLatentDataset(Dataset):\n",
    "    def __init__(self, gae, pyg_dataset, device):\n",
    "        \"\"\"\n",
    "        Precomputes per-graph node-wise latents using the graph autoencoder.\n",
    "        Returns tensors of shape [N_nodes, latent_dim] per graph.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.latents = []\n",
    "        gae = gae.to(device)\n",
    "        gae.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in pyg_dataset:\n",
    "                data = data.to(device)\n",
    "                # encode yields [N_nodes, latent_dim]\n",
    "                z = gae.encoder(data.x, data.edge_index, data.edge_attr.view(-1))\n",
    "                self.latents.append(z.cpu())\n",
    "    def __len__(self):\n",
    "        return len(self.latents)\n",
    "    def __getitem__(self, idx):\n",
    "        # returns [N_nodes, latent_dim]\n",
    "        return self.latents[idx]\n",
    "\n",
    "# Create the latent dataset and a regular torch DataLoader:\n",
    "# pyg_dataset is your MLPGraphDatasetNeuronsFromList\n",
    "latent_dataset = GraphLatentDataset(gae, dataset, device)\n",
    "# collate into tensor of shape [B, N_nodes, latent_dim]\n",
    "def collate_latents(batch_list):\n",
    "    return torch.stack(batch_list, dim=0)\n",
    "latent_loader = TorchDataLoader(latent_dataset,\n",
    "                                 batch_size=8,\n",
    "                                 shuffle=True,\n",
    "                                 collate_fn=collate_latents)\n",
    "\n",
    "# Now latent_loader yields batches of shape [B, N_nodes, latent_dim]\n",
    "# To feed into UNet2D, convert to [B,1,N_nodes,latent_dim]:\n",
    "# for batch_z in latent_loader:\n",
    "#     x0 = batch_z.unsqueeze(1)  # shape [B,1,N_nodes,latent_dim]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7) Sampling from Diffusion with Batched Latents\n",
    "# -------------------------------------------------------\n",
    "def sample_latents_2d(model, scheduler, num_graphs, N_nodes, latent_dim, device):\n",
    "    \"\"\"\n",
    "    Reverse diffusion over 2D latents shape [num_graphs, 1, N_nodes, latent_dim]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # start with pure noise of correct shape\n",
    "    x = torch.randn(num_graphs, 1, N_nodes, latent_dim, device=device)\n",
    "    for t in reversed(range(scheduler.timesteps)):\n",
    "        t_batch = torch.full((num_graphs,), t, device=device, dtype=torch.long)\n",
    "        eps_pred = model.unet(x, t_batch)\n",
    "        a = scheduler.alphas[t]\n",
    "        a_cum = scheduler.alphas_cumprod[t]\n",
    "        b = scheduler.betas[t]\n",
    "        # posterior mean\n",
    "        mean = (1 / a.sqrt()) * (x - (b / (1 - a_cum).sqrt()) * eps_pred)\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma = b.sqrt()\n",
    "            x = mean + sigma * noise\n",
    "        else:\n",
    "            x = mean\n",
    "    return x.squeeze(1)  # [num_graphs, N_nodes, latent_dim]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8) Generating New MLPs from Sampled Latents\n",
    "# -------------------------------------------------------\n",
    "# Then loop over each sampled latent matrix to decode and test:\n",
    "# sampled_z = sample_latents_2d(diffusion_model, scheduler, num_samples,\n",
    "#                               N_nodes, latent_dim, device)\n",
    "# for z_graph in sampled_z:\n",
    "#     # z_graph: [N_nodes, latent_dim]\n",
    "#     edge_pred, node_pred = gae.decoder(z_graph.to(device),\n",
    "#                                        template.edge_index,\n",
    "#                                        template.edge_attr.view(-1).to(device))\n",
    "#     rec_data = Data(x=node_pred.unsqueeze(1),\n",
    "#                     edge_index=template.edge_index,\n",
    "#                     edge_attr=edge_pred.unsqueeze(1))\n",
    "#     new_mlp = vgae_to_mlp(rec_data)\n",
    "#     acc = test_mlp(new_mlp, device, test_loader)\n",
    "#     print(f\"Sample accuracy: {acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "403106eb-1d04-417b-b5c3-ca5ac52c3ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/10, Loss: 0.7367\n",
      "Epoch 002/10, Loss: 0.4768\n",
      "Epoch 003/10, Loss: 0.3474\n",
      "Epoch 004/10, Loss: 0.3825\n",
      "Epoch 005/10, Loss: 0.3217\n",
      "Epoch 006/10, Loss: 0.2618\n",
      "Epoch 007/10, Loss: 0.2657\n",
      "Epoch 008/10, Loss: 0.2664\n",
      "Epoch 009/10, Loss: 0.2281\n",
      "Epoch 010/10, Loss: 0.2432\n"
     ]
    }
   ],
   "source": [
    "# Now latent_loader yields batches of shape [B, N_nodes, latent_dim]\n",
    "# To feed into UNet2D, convert to [B,1,N_nodes,latent_dim]:\n",
    "# for batch_z in latent_loader:\n",
    "#     x0 = batch_z.unsqueeze(1)  # shape [B,1,N_nodes,latent_dim]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7) Training the 2D Latent Diffusion Model\n",
    "# -------------------------------------------------------\n",
    "# Instantiate UNet2D and scheduler\n",
    "unet2d = UNet2D(latent_dim = 64, base_ch=16, time_emb_dim=8).to(device)\n",
    "scheduler = DiffusionScheduler()\n",
    "diffusion_model = LatentDiffusion2D(unet2d, scheduler).to(device)\n",
    "optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train for a number of epochs\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    diffusion_model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_z in latent_loader:\n",
    "        # batch_z: [B, N_nodes, latent_dim]\n",
    "        # prepare input: [B,1,N_nodes,latent_dim]\n",
    "        x0 = batch_z.unsqueeze(1).to(device)\n",
    "        loss = diffusion_model.training_step(x0)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(latent_loader)\n",
    "    print(f\"Epoch {epoch:03d}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76c70635-6609-4cb4-a21a-b1b74084a215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet has 60105 parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"UNet has {count_parameters(unet2d)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a3a3400-c36c-4a81-8189-a6006d1c23a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_nodes: 270, latent_dim: 64\n"
     ]
    }
   ],
   "source": [
    "# num_graphs = 32\n",
    "# latent_dim = 64\n",
    "# N_nodes = 270\n",
    "\n",
    "template = dataset[0].to(device)\n",
    "# Determine node count (H) and latent dimensionality (W)\n",
    "N_nodes = template.x.size(0)\n",
    "latent_dim = latent_loader.dataset.latents[0].size(1)\n",
    "print(f\"N_nodes: {N_nodes}, latent_dim: {latent_dim}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8) Sampling from Diffusion with Batched Latents\n",
    "# -------------------------------------------------------\n",
    "def sample_latents_2d(model, scheduler, num_graphs, N_nodes, latent_dim, device):\n",
    "    \"\"\"\n",
    "    Reverse diffusion over 2D latents shape [num_graphs, 1, N_nodes, latent_dim]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # start with pure noise of correct shape\n",
    "    x = torch.randn(num_graphs, 1, N_nodes, latent_dim, device=device)\n",
    "    for t in reversed(range(scheduler.timesteps)):\n",
    "        t_batch = torch.full((num_graphs,), t, device=device, dtype=torch.long)\n",
    "        eps_pred = model.unet(x, t_batch)\n",
    "        a = scheduler.alphas[t]\n",
    "        a_cum = scheduler.alphas_cumprod[t]\n",
    "        b = scheduler.betas[t]\n",
    "        # posterior mean\n",
    "        mean = (1 / a.sqrt()) * (x - (b / (1 - a_cum).sqrt()) * eps_pred)\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma = b.sqrt()\n",
    "            x = mean + sigma * noise\n",
    "        else:\n",
    "            x = mean\n",
    "    return x.squeeze(1)  # [num_graphs, N_nodes, latent_dim]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8) Generating New MLPs from Sampled Latents\n",
    "# -------------------------------------------------------\n",
    "# Then loop over each sampled latent matrix to decode and test:\n",
    "num_samples = 10\n",
    "sampled_z = sample_latents_2d(diffusion_model, scheduler, num_samples,\n",
    "                              N_nodes, latent_dim, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21e74dc6-9572-45e6-b04d-9dec8fbf6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample accuracy: 9.74%\n",
      "Sample accuracy: 11.35%\n",
      "Sample accuracy: 10.28%\n",
      "Sample accuracy: 10.1%\n",
      "Sample accuracy: 9.58%\n",
      "Sample accuracy: 20.71%\n",
      "Sample accuracy: 13.19%\n",
      "Sample accuracy: 10.1%\n",
      "Sample accuracy: 20.18%\n",
      "Sample accuracy: 11.35%\n"
     ]
    }
   ],
   "source": [
    "generated_mlp_acc = []\n",
    "for z_graph in sampled_z:\n",
    "    # z_graph: [N_nodes, latent_dim]\n",
    "    edge_pred, node_pred = gae.decoder(z_graph.to(device),\n",
    "                                       template.edge_index,\n",
    "                                       template.edge_attr.view(-1).to(device))\n",
    "    rec_data = Data(x=node_pred.unsqueeze(1),\n",
    "                    edge_index=template.edge_index,\n",
    "                    edge_attr=edge_pred.unsqueeze(1))\n",
    "    new_mlp = vgae_to_mlp(rec_data)\n",
    "    acc = test_mlp(new_mlp, test_loader)\n",
    "    generated_mlp_acc.append(acc)\n",
    "    print(f\"Sample accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0421a8-259b-4bed-938e-7c546c901f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
