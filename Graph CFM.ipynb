{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32e43cf-5095-486f-8d61-13f4db1e072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./weightflow')  # e.g., './repo_name'\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import NamedTuple\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# from nn.relational_transformer import RelationalTransformer\n",
    "# from nn.graph_constructor import GraphConstructor\n",
    "from flow.flow_matching import CFM\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import logging\n",
    "from utils.data import sample_gaussian_wsos\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import traceback\n",
    "from types import SimpleNamespace\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GATConv, GraphNorm, GCNConv, GraphSAGE\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aec63d3-2fda-4eac-99fe-e08c0f8c4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PermutationSpec class similar to the JAX version but using PyTorch\n",
    "class PermutationSpec(NamedTuple):\n",
    "    perm_to_axes: dict\n",
    "    axes_to_perm: dict\n",
    "\n",
    "def permutation_spec_from_axes_to_perm(axes_to_perm: dict) -> PermutationSpec:\n",
    "    perm_to_axes = defaultdict(list)\n",
    "    for wk, axis_perms in axes_to_perm.items():\n",
    "        for axis, perm in enumerate(axis_perms):\n",
    "            if perm is not None:\n",
    "                perm_to_axes[perm].append((wk, axis))\n",
    "    return PermutationSpec(perm_to_axes=dict(perm_to_axes), axes_to_perm=axes_to_perm)\n",
    "\n",
    "def mlp_permutation_spec_mlp() -> PermutationSpec:\n",
    "    \"\"\"Define permutation spec for MLP architecture\"\"\"\n",
    "    return permutation_spec_from_axes_to_perm({\n",
    "        \"fc1.weight\": (None, \"P_0\"),       # Input (None) to fc1 output (P_0)\n",
    "        \"fc1.bias\": (\"P_0\",),              # Bias for fc1 output (P_0)\n",
    "        \"fc2.weight\": (\"P_0\", \"P_1\"),      # fc1 output (P_0) to fc2 output (P_1)\n",
    "        \"fc2.bias\": (\"P_1\",),              # Bias for fc2 output (P_1)\n",
    "        \"fc3.weight\": (\"P_1\", None),       # fc2 output (P_1) to fc3 output (None)\n",
    "        \"fc3.bias\": (None,),               # Bias for fc3 output (None)\n",
    "    })\n",
    "\n",
    "def get_permuted_param(ps: PermutationSpec, perm, k: str, params, except_axis=None):\n",
    "    \"\"\"Get parameter k from params, with permutations applied.\"\"\"\n",
    "    w = params[k]\n",
    "    for axis, p in enumerate(ps.axes_to_perm[k]):\n",
    "        # Skip the axis we're trying to permute\n",
    "        if axis == except_axis:\n",
    "            continue\n",
    "\n",
    "        # None indicates no permutation for that axis\n",
    "        if p is not None:\n",
    "            w = torch.index_select(w, axis, torch.tensor(perm[p], device=w.device))\n",
    "\n",
    "    return w\n",
    "\n",
    "def apply_permutation(ps: PermutationSpec, perm, params):\n",
    "    \"\"\"Apply permutation to params\"\"\"\n",
    "    return {k: get_permuted_param(ps, perm, k, params) for k in params.keys()}\n",
    "\n",
    "def weight_matching(ps: PermutationSpec, params_a, params_b, max_iter=100, init_perm=None, silent=True, device=None):\n",
    "    \"\"\"Find permutation of params_b to make them match params_a.\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Move all tensors to the correct device\n",
    "    params_a = {k: v.to(device) for k, v in params_a.items()}\n",
    "    params_b = {k: v.to(device) for k, v in params_b.items()}\n",
    "\n",
    "    # Get permutation sizes from the first parameter with each permutation\n",
    "    perm_sizes = {p: params_a[axes[0][0]].shape[axes[0][1]] \n",
    "                  for p, axes in ps.perm_to_axes.items()}\n",
    "    \n",
    "    # Initialize permutations to identity if none provided\n",
    "    if init_perm is None:\n",
    "        perm = {p: torch.arange(n, device=device) for p, n in perm_sizes.items()}\n",
    "    else:\n",
    "        perm = {p: v.to(device) for p, v in init_perm.items()}\n",
    "        \n",
    "    perm_names = list(perm.keys())\n",
    "    \n",
    "    # Use a random number generator with a fixed seed for reproducibility\n",
    "    rng = np.random.RandomState(42)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        progress = False\n",
    "        \n",
    "        # Shuffle the order of permutations to update\n",
    "        for p_ix in rng.permutation(len(perm_names)):\n",
    "            p = perm_names[p_ix]\n",
    "            n = perm_sizes[p]\n",
    "            \n",
    "            # Initialize cost matrix\n",
    "            A = torch.zeros((n, n), device=device)\n",
    "            \n",
    "            # Fill in cost matrix based on all parameters affected by this permutation\n",
    "            for wk, axis in ps.perm_to_axes[p]:\n",
    "                w_a = params_a[wk]\n",
    "                w_b = get_permuted_param(ps, perm, wk, params_b, except_axis=axis)\n",
    "\n",
    "                w_a = w_a.moveaxis(axis, 0).reshape((n, -1))\n",
    "                w_b = w_b.moveaxis(axis, 0).reshape((n, -1))\n",
    "\n",
    "                A += w_a @ w_b.T\n",
    "\n",
    "            # Solve the linear assignment problem\n",
    "            ri, ci = linear_sum_assignment(A.detach().cpu().numpy(), maximize=True)\n",
    "            assert (ri == np.arange(len(ri))).all()\n",
    "\n",
    "            # Calculate improvement\n",
    "            eye_old = torch.eye(n, device=device)[perm[p]]\n",
    "            eye_new = torch.eye(n, device=device)[ci]\n",
    "\n",
    "            oldL = torch.tensordot(A, eye_old, dims=([0, 1], [0, 1]))\n",
    "            newL = torch.tensordot(A, eye_new, dims=([0, 1], [0, 1]))\n",
    "\n",
    "            if not silent and newL > oldL + 1e-12:\n",
    "                logging.info(f\"{iteration}/{p}: {newL.item() - oldL.item()}\")\n",
    "\n",
    "            progress = progress or newL > oldL + 1e-12\n",
    "\n",
    "            perm[p] = torch.tensor(ci, device=device)\n",
    "\n",
    "        if not progress:\n",
    "            break\n",
    "\n",
    "    return perm\n",
    "\n",
    "\n",
    "def update_model_weights(model, aligned_params):\n",
    "    \"\"\"Update model weights with aligned parameters\"\"\"\n",
    "    # Convert numpy arrays to torch tensors if needed\n",
    "    model.fc1.weight.data = aligned_params[\"fc1.weight\"].T\n",
    "    model.fc1.bias.data = aligned_params[\"fc1.bias\"]\n",
    "    model.fc2.weight.data = aligned_params[\"fc2.weight\"].T\n",
    "    model.fc2.bias.data = aligned_params[\"fc2.bias\"]\n",
    "    model.fc3.weight.data = aligned_params[\"fc3.weight\"].T\n",
    "    model.fc3.bias.data = aligned_params[\"fc3.bias\"]\n",
    "    \n",
    "def load_model_weights(model, model_path):\n",
    "    \"\"\"Load model weights from file\"\"\"\n",
    "    weights, biases = torch.load(model_path, map_location=device)\n",
    "    model.fc1.weight.data = weights[0]\n",
    "    model.fc1.bias.data = biases[0]\n",
    "    model.fc2.weight.data = weights[1]\n",
    "    model.fc2.bias.data = biases[1]\n",
    "    model.fc3.weight.data = weights[2]\n",
    "    model.fc3.bias.data = biases[2]\n",
    "    return model.to(device)\n",
    "\n",
    "def get_permuted_models_data(ref_point=0, model_dir=\"models\", num_models=200, model_type = f'MNIST'):\n",
    "    \"\"\"Apply weight matching to align models with a reference model\"\"\"\n",
    "    # Create reference model\n",
    "    ref_model = MLP()  # Assumes MLP class is defined\n",
    "    ref_model_path = f\"{model_dir}/{model_type}_mixed_mlp_weights_{ref_point}.pt\"\n",
    "    ref_model = load_model_weights(ref_model, ref_model_path).to(device)\n",
    "    \n",
    "    ps = mlp_permutation_spec_mlp()\n",
    "    \n",
    "    # Convert reference model weights to dictionary format\n",
    "    params_a = {\n",
    "        \"fc1.weight\": ref_model.fc1.weight.T.to(device),\n",
    "        \"fc1.bias\": ref_model.fc1.bias.to(device),\n",
    "        \"fc2.weight\": ref_model.fc2.weight.T.to(device),\n",
    "        \"fc2.bias\": ref_model.fc2.bias.to(device),\n",
    "        \"fc3.weight\": ref_model.fc3.weight.T.to(device),\n",
    "        \"fc3.bias\": ref_model.fc3.bias.to(device),\n",
    "    }\n",
    "    \n",
    "    org_models = []\n",
    "    permuted_models = []\n",
    "\n",
    "    for i in range(0, num_models):\n",
    "        if i == ref_point:\n",
    "            continue\n",
    "            \n",
    "        model_path = f\"{model_dir}/{model_type}_mixed_mlp_weights_{i}.pt\"\n",
    "\n",
    "        model = MLP()  # Assumes MLP class is defined\n",
    "        model = load_model_weights(model, model_path).to(device)\n",
    "        org_models.append(model)\n",
    "        \n",
    "        # Convert model weights to dictionary format\n",
    "        params_b = {\n",
    "                \"fc1.weight\": model.fc1.weight.T.to(device),\n",
    "                \"fc1.bias\": model.fc1.bias.to(device),\n",
    "                \"fc2.weight\": model.fc2.weight.T.to(device),\n",
    "                \"fc2.bias\": model.fc2.bias.to(device),\n",
    "                \"fc3.weight\": model.fc3.weight.T.to(device),\n",
    "                \"fc3.bias\": model.fc3.bias.to(device),\n",
    "        }\n",
    "\n",
    "        # Find permutation to align with reference model\n",
    "        perm = weight_matching(ps, params_a, params_b)\n",
    "        \n",
    "        # Apply permutation to model_b\n",
    "        aligned_params_b = apply_permutation(ps, perm, params_b)\n",
    "        \n",
    "        # Create a new model with permuted weights\n",
    "        reconstructed_model = copy.deepcopy(model)\n",
    "        update_model_weights(reconstructed_model, aligned_params_b)\n",
    "        \n",
    "        permuted_models.append(reconstructed_model.to(device))\n",
    "\n",
    "            \n",
    "    return ref_model, org_models, permuted_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8430a202-0f99-4fb9-b6dc-27bcc403941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MNIST classifier MLP class for dataset\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected 3 layer neural network for classification tasks where hidden layers have 32 neurons (default) and ReLU activations\n",
    "    input: torch.tensor( [batch_size, 196] )\n",
    "    output: torch.tensor( [batch_size, 10] )\n",
    "    For classifying inputs of 196 into 10 classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim = 32, init_type='xavier', seed=None, type = 'MNIST'):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(196, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 10)\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)  # Set a unique seed for reproducibility\n",
    "\n",
    "        self.init_weights(init_type)\n",
    "        self.type = type\n",
    "\n",
    "    def init_weights(self, init_type):\n",
    "        if init_type == 'xavier':\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "            nn.init.xavier_uniform_(self.fc2.weight)\n",
    "            nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        elif init_type == 'he':\n",
    "            nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "        else:\n",
    "            nn.init.normal_(self.fc1.weight)\n",
    "            nn.init.normal_(self.fc2.weight)\n",
    "            nn.init.normal_(self.fc3.weight)\n",
    "        \n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 196)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def test_mlp(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def zero_like_wso(wso):\n",
    "    zero_weights = tuple(torch.zeros_like(w) for w in wso.weights)\n",
    "    zero_biases = tuple(torch.zeros_like(b) for b in wso.biases)\n",
    "    return WeightSpaceObject(zero_weights, zero_biases)\n",
    "\n",
    "# WeightSpaceObject class for handling MLP weights\n",
    "class WeightSpaceObject:\n",
    "    def __init__(self, weights, biases):\n",
    "        self.weights = weights if isinstance(weights, tuple) else tuple(weights)\n",
    "        self.biases = biases if isinstance(biases, tuple) else tuple(biases)\n",
    "        \n",
    "    def flatten(self, device=None):\n",
    "        \"\"\"Flatten weights and biases into a single vector\"\"\"\n",
    "        flat = torch.cat([w.flatten() for w in self.weights] + \n",
    "                          [b.flatten() for b in self.biases])\n",
    "        if device:\n",
    "            flat = flat.to(device)\n",
    "        return flat\n",
    "    \n",
    "    @classmethod\n",
    "    def from_flat(cls, flat, layers, device):\n",
    "        \"\"\"Create WeightSpaceObject from flattened vector\"\"\"\n",
    "        sizes = []\n",
    "        # Calculate sizes for weight matrices\n",
    "        for i in range(len(layers) - 1):\n",
    "            sizes.append(layers[i] * layers[i+1])  # Weight matrix\n",
    "        # Calculate sizes for bias vectors\n",
    "        for i in range(1, len(layers)):\n",
    "            sizes.append(layers[i])  # Bias vector\n",
    "            \n",
    "        # Split flat tensor into parts\n",
    "        parts = []\n",
    "        start = 0\n",
    "        for size in sizes:\n",
    "            parts.append(flat[start:start+size])\n",
    "            start += size\n",
    "            \n",
    "        # Reshape into weight matrices and bias vectors\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            w_size = layers[i] * layers[i+1]\n",
    "            weights.append(parts[i].reshape(layers[i+1], layers[i]))\n",
    "            biases.append(parts[i + len(layers) - 1])\n",
    "            \n",
    "        return cls(weights, biases).to(device)\n",
    "    \n",
    "    def to(self, device):\n",
    "        \"\"\"Move weights and biases to specified device\"\"\"\n",
    "        weights = tuple(w.to(device) for w in self.weights)\n",
    "        biases = tuple(b.to(device) for b in self.biases)\n",
    "        return WeightSpaceObject(weights, biases)\n",
    "        \n",
    "    def map(self, fn):\n",
    "        new_weights = tuple(fn(w) for w in self.weights)\n",
    "        new_biases = tuple(fn(b) for b in self.biases)\n",
    "        return WeightSpaceObject(new_weights, new_biases)\n",
    "\n",
    "# Simple Bunch class for storing data\n",
    "class Bunch:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "# Safe deflatten function that checks bounds before accessing tensors\n",
    "def safe_deflatten(flat, batch_size, starts, ends):\n",
    "    \"\"\"Safely deflatten a tensor without index errors\"\"\"\n",
    "    parts = []\n",
    "    actual_batch_size = flat.size(0)\n",
    "    \n",
    "    # Ensure we don't exceed the actual batch size\n",
    "    safe_batch_size = min(actual_batch_size, batch_size)\n",
    "    \n",
    "    for i in range(safe_batch_size):\n",
    "        batch_parts = []\n",
    "        for si, ei in zip(starts, ends):\n",
    "            if si < ei:  # Only process valid ranges\n",
    "                batch_parts.append(flat[i][si:ei])\n",
    "        parts.append(batch_parts)\n",
    "    \n",
    "    return parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3925409a-2e2f-4059-be70-8c985da22c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGraphDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Torch Dataset of MLPs converted to graph‐structured Data with:\n",
    "\n",
    "      - Node features: [bias_value, one_hot_node_layer]\n",
    "      - Edge features: [weight_value, one_hot_edge_layer]\n",
    "\n",
    "    Ordering of nodes in each graph: \n",
    "      • nodes 0..(input_dim−1)                    → “input nodes” (layer_id = 0, bias_value = 0)\n",
    "      • next `layer_sizes[1]` nodes                → biases of layer 1 (layer_id = 1)\n",
    "      • next `layer_sizes[2]` nodes                → biases of layer 2 (layer_id = 2)\n",
    "      • next `layer_sizes[3]` nodes                → biases of layer 3 (layer_id = 3)\n",
    "\n",
    "    Ordering of edges in each graph is exactly:\n",
    "      For layer ℓ = 1..3, in order:\n",
    "        for i_out in range(out_dim_ℓ):\n",
    "          for j_in in range(in_dim_ℓ):\n",
    "            add edge (src = node_offset + j_in, dst = node_offset + in_dim_ℓ + i_out)\n",
    "      which matches the weight‐matrix flatten order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mlp_list, layer_sizes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          mlp_list     : list of MLP models, each having attributes `fc1.weight/bias`,\n",
    "                         `fc2.weight/bias`, `fc3.weight/bias`.\n",
    "          layer_sizes  : List[int], e.g. [input_dim, hidden1, hidden2, output_dim].\n",
    "                         The number of weight layers is len(layer_sizes)−1 (i.e. 3).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.models = mlp_list\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes) - 1  # For [784,512,256,10], num_layers = 3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.models)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mlp = self.models[idx]\n",
    "\n",
    "        # Extract weight & bias tensors from fc1, fc2, fc3\n",
    "        weights = [mlp.fc1.weight, mlp.fc2.weight, mlp.fc3.weight]  # shapes: [out_dim, in_dim]\n",
    "        biases  = [mlp.fc1.bias,   mlp.fc2.bias,   mlp.fc3.bias]    # shapes: [out_dim]\n",
    "\n",
    "        input_dim = self.layer_sizes[0]  # e.g. 784\n",
    "        total_nodes = input_dim + sum(self.layer_sizes[1:])  # 784 + 512 + 256 + 10\n",
    "        node_feats_list = []\n",
    "        node_layer_list = []\n",
    "\n",
    "        # === Build node features ===\n",
    "        # 1) Input nodes (layer_id = 0), bias_value = 0\n",
    "        for i in range(input_dim):\n",
    "            node_feats_list.append([0.0])      # bias_value = 0 for input nodes\n",
    "            node_layer_list.append(0)          # layer_id = 0\n",
    "\n",
    "        # 2) Bias nodes for each layer ℓ = 1..num_layers\n",
    "        for layer_id in range(1, self.num_layers + 1):\n",
    "            b = biases[layer_id - 1]           # bias tensor of shape [layer_sizes[layer_id]]\n",
    "            for j in range(self.layer_sizes[layer_id]):\n",
    "                node_feats_list.append([b[j].item()])\n",
    "                node_layer_list.append(layer_id)\n",
    "\n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(node_feats_list, dtype=torch.float)       # [total_nodes, 1]\n",
    "        node_layer_tensor = torch.tensor(node_layer_list, dtype=torch.long)  # [total_nodes]\n",
    "\n",
    "        # One‐hot encode node_layer_tensor (dim = num_layers + 1)\n",
    "        node_layer_one_hot = F.one_hot(node_layer_tensor, num_classes=self.num_layers + 1).float()\n",
    "        # Final node features: [bias_value, one_hot_layer] → dim = 1 + (num_layers + 1)\n",
    "        x = torch.cat([x, node_layer_one_hot], dim=-1)  # [total_nodes, 1 + (num_layers+1)]\n",
    "\n",
    "        # === Build edges and edge attributes ===\n",
    "        edge_index_list = []\n",
    "        edge_attr_list = []\n",
    "        edge_weight_list = []\n",
    "        node_offset = 0\n",
    "\n",
    "        for layer_id, w in enumerate(weights, start=1):\n",
    "            out_dim, in_dim = w.shape\n",
    "            for i_out in range(out_dim):\n",
    "                for j_in in range(in_dim):\n",
    "                    src = node_offset + j_in\n",
    "                    dst = node_offset + in_dim + i_out\n",
    "                    edge_index_list.append([src, dst])\n",
    "\n",
    "                    weight_val = w[i_out, j_in].item()\n",
    "                    one_hot_edge = torch.zeros(self.num_layers)  # length = num_layers\n",
    "                    one_hot_edge[layer_id - 1] = 1.0\n",
    "                    edge_attr_list.append(torch.cat([torch.tensor([weight_val]), one_hot_edge], dim=0))\n",
    "                    edge_weight_list.append(torch.tensor([weight_val]))\n",
    "            node_offset += in_dim\n",
    "\n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()  # [2, E]\n",
    "        edge_attr  = torch.stack(edge_attr_list, dim=0)  # [E, 1 + num_layers]\n",
    "        edge_weight = torch.stack(edge_weight_list, dim = 0)\n",
    "        # For a single‐graph item, batch = all zeros (every node → graph 0)\n",
    "        batch = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "        return Data(x=x, edge_index=edge_index, edge_weight = edge_weight, edge_attr=edge_attr, batch=batch)\n",
    "\n",
    "\n",
    "class GraphFlowNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A GAT‐based conditional flow net that predicts BOTH:\n",
    "      • edge flows (Δweight per edge)\n",
    "      • node flows (Δbias per node)\n",
    "\n",
    "    Conditioning on:\n",
    "      • t ∈ [0,1] (time)\n",
    "      • c ∈ {0,1,…,C-1} (graph type / class index)\n",
    "\n",
    "    Architecture summary:\n",
    "      1) Embed t → t_emb ∈ ℝ^{t_embed_dim}\n",
    "      2) Embed c → c_emb ∈ ℝ^{class_embed_dim}\n",
    "      3) Initial node MLP: [node_feat‖t_emb_node‖c_emb_node] → hidden_dim\n",
    "      4) Initial edge MLP: [edge_attr‖t_emb_edge‖c_emb_edge] → hidden_dim\n",
    "      5) K× GATConv layers (each uses edge‐features for attention)\n",
    "      6) Node‐head: Δbias_i = node_out([h_i^K‖t_emb_node‖c_emb_node])\n",
    "      7) Edge‐head: Δweight_e = edge_out([h_u^K‖h_v^K‖e^0_e‖t_emb_edge‖c_emb_edge])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_channels: int,\n",
    "        edge_in_channels: int,\n",
    "        num_classes: int,\n",
    "        t_embed_dim: int = 16,\n",
    "        class_embed_dim: int = 16,\n",
    "        hidden_dim: int = 128,\n",
    "        num_message_passing_layers: int = 3,\n",
    "        heads: int = 4,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          node_in_channels            : dimension of initial node feat (e.g. 1 + (num_layers+1))\n",
    "          edge_in_channels            : dimension of initial edge feat (e.g. 1 + num_layers)\n",
    "          num_classes                 : total # of graph types / classes (C)\n",
    "          t_embed_dim                 : dim of time embedding\n",
    "          class_embed_dim             : dim of graph‐type embedding\n",
    "          hidden_dim                  : hidden dimension used throughout\n",
    "          num_message_passing_layers  : how many GATConv layers to stack\n",
    "          heads                       : # attention heads per GATConv\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.t_embed_dim      = t_embed_dim\n",
    "        self.class_embed_dim  = class_embed_dim\n",
    "        self.hidden_dim       = hidden_dim\n",
    "        self.num_layers_mp    = num_message_passing_layers\n",
    "        self.num_classes      = num_classes\n",
    "\n",
    "        # 1) Time embedding: ℝ → ℝ^{t_embed_dim}\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, t_embed_dim),\n",
    "        )\n",
    "\n",
    "        # 2) Class embedding: {0,…,C−1} → ℝ^{class_embed_dim}\n",
    "        self.class_emb = nn.Embedding(num_classes, class_embed_dim)\n",
    "\n",
    "        # 3) Initial node projection: [node_feat || t_emb_node || c_emb_node] → hidden_dim\n",
    "        self.node_proj = nn.Linear(node_in_channels + t_embed_dim + class_embed_dim, hidden_dim)\n",
    "\n",
    "        # 4) Initial edge projection: [edge_attr || t_emb_edge || c_emb_edge] → hidden_dim\n",
    "        self.edge_proj = nn.Linear(edge_in_channels + t_embed_dim + class_embed_dim, hidden_dim)\n",
    "\n",
    "\n",
    "        # 5) Norm -> GraphSAGE -> Norm\n",
    "        # self.norm1 = GraphNorm(hidden_dim)\n",
    "        self.norm2 = GraphNorm(hidden_dim)\n",
    "\n",
    "        # Graph SAGE stack with jumping knowledge\n",
    "        self.graph_sage = GraphSAGE(in_channels = hidden_dim, hidden_channels = hidden_dim,\n",
    "                                    num_layers = num_message_passing_layers, out_channels = hidden_dim,\n",
    "                                   jk = \"max\")\n",
    "        \n",
    "        # 5) Stacked GATConvs (each uses edge_dim = hidden_dim to inject edge embeddings)\n",
    "        # self.convs = nn.ModuleList()\n",
    "        # self.norms = nn.ModuleList()\n",
    "        # for _ in range(num_message_passing_layers):\n",
    "        #     # consider another method to avoid exploding neighborhood problem - Zohair. \n",
    "        #     conv = GCNConv(\n",
    "        #         in_channels = hidden_dim, \n",
    "        #         out_channels = hidden_dim, \n",
    "        #         normalize = False, \n",
    "        #         add_self_loops = False,\n",
    "        #     )\n",
    "        #     # conv = GATConv(\n",
    "        #     #     in_channels=hidden_dim,\n",
    "        #     #     out_channels=hidden_dim // heads,\n",
    "        #     #     heads=heads,\n",
    "        #     #     concat=True,\n",
    "        #     #     edge_dim=hidden_dim,\n",
    "        #     #     dropout=0.0,\n",
    "        #     #     add_self_loops=False,\n",
    "        #     # )\n",
    "        #     norm = GraphNorm(hidden_dim)\n",
    "        #     self.convs.append(conv)\n",
    "        #     self.norms.append(norm)\n",
    "\n",
    "        # 6) Node‐flow head: [h_i^K || t_emb_node || c_emb_node] → Δbias_i (scalar)\n",
    "        self.node_out_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + t_embed_dim + class_embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "        # 7) Edge‐flow head: [h_u^K || h_v^K || e⁰_e || t_emb_edge || c_emb_edge] → Δweight_e (scalar)\n",
    "        self.edge_out_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim + hidden_dim + t_embed_dim + class_embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data: Data, t: torch.Tensor, c: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          data : PyG Data (or Batch) with fields:\n",
    "                   • x         [N_total, node_in_channels]\n",
    "                   • edge_index[2, E_total]\n",
    "                   • edge_attr [E_total, edge_in_channels]\n",
    "                   • batch     [N_total]   (maps each node→graph index)\n",
    "          t    : [num_graphs, 1] tensor (each in [0,1])\n",
    "          c    : [num_graphs]     LongTensor (each ∈ {0,1,…,C−1})\n",
    "\n",
    "        Returns:\n",
    "          node_flows : [N_total, 1] predicted Δbias per node\n",
    "          edge_flows : [E_total, 1] predicted Δweight per edge\n",
    "        \"\"\"\n",
    "        device = data.x.device\n",
    "\n",
    "        # 1) Compute time‐embeddings for each graph ⇒ [num_graphs, t_embed_dim]\n",
    "        batch = data.batch                             # [N_total]\n",
    "        num_graphs = int(batch.max().item()) + 1\n",
    "        if t.dim() == 1:\n",
    "            t = t.view(num_graphs, 1)                  # ensure shape = [num_graphs, 1]\n",
    "        t_emb_graph = self.time_embed(t.to(device))    # [num_graphs, t_embed_dim]\n",
    "\n",
    "        # 2) Compute class‐embeddings for each graph ⇒ [num_graphs, class_embed_dim]\n",
    "        c = c.to(device)                               # [num_graphs]\n",
    "        c_emb_graph = self.class_emb(c)                # [num_graphs, class_embed_dim]\n",
    "\n",
    "        # 3) Broadcast t_emb & c_emb to nodes:\n",
    "        t_emb_nodes = t_emb_graph[batch]               # [N_total, t_embed_dim]\n",
    "        c_emb_nodes = c_emb_graph[batch]               # [N_total, class_embed_dim]\n",
    "\n",
    "        # 4) Initial node embedding: concat [x, t_emb_node, c_emb_node] → project → ELU\n",
    "        h = torch.cat([data.x, t_emb_nodes, c_emb_nodes], dim=-1)  # [N_total, node_in_ch + t + c]\n",
    "        h = self.node_proj(h)                                          # [N_total, hidden_dim]\n",
    "        # h = F.relu(h)\n",
    "\n",
    "        # 5) Broadcast t_emb & c_emb to edges via source‐node index:\n",
    "        src_nodes     = data.edge_index[0]                # [E_total]\n",
    "        edge_graph_idx = batch[src_nodes]                 # [E_total]\n",
    "        t_emb_edges   = t_emb_graph[edge_graph_idx]       # [E_total, t_embed_dim]\n",
    "        c_emb_edges   = c_emb_graph[edge_graph_idx]       # [E_total, class_embed_dim]\n",
    "\n",
    "        # 6) Initial edge embedding: [edge_attr, t_emb_edge, c_emb_edge] → project → ELU\n",
    "        e_in = torch.cat([data.edge_attr, t_emb_edges, c_emb_edges], dim=-1)\n",
    "        e   = self.edge_proj(e_in)   # [E_total, hidden_dim]\n",
    "        # e   = F.relu(e)\n",
    "\n",
    "        # 7) K layers of GATConv (edge‐conditioned)\n",
    "        # for conv, norm in zip(self.convs, self.norms):\n",
    "            \n",
    "        #     h1 = conv(h, edge_index = data.edge_index, edge_weight = data.edge_weight)\n",
    "        #     h = F.relu(norm(h1)) + h\n",
    "            \n",
    "        #     # h1, attn_weights = conv(h, data.edge_index, edge_attr=e, return_attention_weights = True)   # [N_total, hidden_dim]\n",
    "        #     # h1 = norm(h1)\n",
    "        #     # e = e + F.relu(attn_weights[1])\n",
    "        #     # h = h + F.relu(h1) \n",
    "\n",
    "        # 7) Norm and K layer Graph SAGE sandiwch\n",
    "        # edge_weight = data.edge_weight,  - bc edge weight and edge_index have different dims. \n",
    "        # num_sampled_nodes_per_hop = 12, num_sampled_edges_per_hop = 256 - bc I can't have both / they break things :/\n",
    "        \n",
    "        h = self.graph_sage(h, edge_index = data.edge_index,\n",
    "                            edge_attr = e, num_sampled_nodes_per_hop = 1\n",
    "                        )\n",
    "        # h = self.norm2(h)\n",
    "        \n",
    "        # 8) Node‐flow head: Δbias_i = node_out_mlp([h_i^K, t_emb_node_i, c_emb_node_i])\n",
    "        node_head_in = torch.cat([h, t_emb_nodes, c_emb_nodes], dim=-1)  # [N_total, hidden_dim + t + c]\n",
    "        node_flows = self.node_out_mlp(node_head_in)                     # [N_total, 1]\n",
    "\n",
    "        # 9) Edge‐flow head: Δweight_e = edge_out_mlp([h_u, h_v, e⁰_e, t_emb_edge, c_emb_edge])\n",
    "        h_u = h[src_nodes]                             # [E_total, hidden_dim]\n",
    "        dst_nodes = data.edge_index[1]                 # [E_total]\n",
    "        h_v = h[dst_nodes]                             # [E_total, hidden_dim]\n",
    "        edge_head_in = torch.cat([h_u, h_v, e, t_emb_edges, c_emb_edges], dim=-1)\n",
    "        edge_flows = self.edge_out_mlp(edge_head_in)   # [E_total, 1]\n",
    "\n",
    "        return node_flows, edge_flows\n",
    "\n",
    "\n",
    "def flatten_batch_params(batch_data: Batch,\n",
    "                         node_flows: torch.Tensor,\n",
    "                         edge_flows: torch.Tensor,\n",
    "                         layer_sizes: list):\n",
    "    \"\"\"\n",
    "    Helper function to convert a batched graph’s predicted node_flows and edge_flows\n",
    "    into a tensor of shape [batch_size, flat_dim], where flat_dim = total # of parameters\n",
    "    in each MLP (i.e. sum over ℓ=1..L of (in_dim_ℓ * out_dim_ℓ + out_dim_ℓ)).\n",
    "\n",
    "    We assume:\n",
    "      • Each graph in batch_data has exactly the same architecture given by layer_sizes.\n",
    "      • batch_data.x is ordered: first input_dim nodes, then bias nodes of layer1, then bias nodes of layer2, ….\n",
    "      • batch_data.edge_index and edge_flows are in the exact “weight‐flatten” order per graph:\n",
    "          – All edges of layer 1 (in_dim1*out_dim1) in row‐major (i_out, j_in) order,\n",
    "          – Then all edges of layer 2, etc.\n",
    "      • edge_flows[e] corresponds to the predicted Δweight for the e-th edge in that global ordering.\n",
    "      • node_flows[n] corresponds to the predicted Δbias for the n-th node in that global ordering.\n",
    "\n",
    "    Args:\n",
    "      batch_data  : a PyG Batch object containing `batch_data.ptr` for node splits.\n",
    "      node_flows  : Tensor [N_total, 1]\n",
    "      edge_flows  : Tensor [E_total, 1]\n",
    "      layer_sizes : e.g. [784, 512, 256, 10]\n",
    "\n",
    "    Returns:\n",
    "      flat_batch : Tensor [batch_size, flat_dim]\n",
    "    \"\"\"\n",
    "    device = node_flows.device\n",
    "    batch_size = batch_data.num_graphs  # number of graphs in the batch\n",
    "    num_layers = len(layer_sizes) - 1\n",
    "\n",
    "    # 1) Use `batch_data.ptr` (length = batch_size+1) to split Node flows:\n",
    "    #    batch_data.ptr[i] .. batch_data.ptr[i+1]-1 are node indices for graph i\n",
    "    node_ptr = batch_data.ptr  # Tensor of shape [batch_size+1]\n",
    "    # node_ptr[i].item() = start index of nodes for graph i, node_ptr[i+1] = end index\n",
    "\n",
    "    # 2) Compute edge_batch_idxs for each edge via source‐node batching:\n",
    "    src_nodes = batch_data.edge_index[0]                 # [E_total]\n",
    "    edge_graph_idx = batch_data.batch[src_nodes]         # [E_total] each ∈ [0..batch_size-1]\n",
    "    edges_per_graph = torch.bincount(edge_graph_idx,\n",
    "                                     minlength=batch_size).to(device)  # [batch_size]\n",
    "    # Build edge pointers: prefix sums\n",
    "    edge_ptr = torch.zeros(batch_size + 1, dtype=torch.long, device=device)\n",
    "    edge_ptr[1:] = torch.cumsum(edges_per_graph, dim=0)  # so edge_ptr[i]..edge_ptr[i+1]-1 are edges of graph i\n",
    "\n",
    "    # 3) For each graph i, slice out its node_flows and edge_flows, then reorder into flat vector\n",
    "    flat_list = []\n",
    "    input_dim = layer_sizes[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Node slice:\n",
    "        n_start = node_ptr[i].item()\n",
    "        n_end   = node_ptr[i + 1].item()\n",
    "        node_seg = node_flows[n_start:n_end].view(-1)  # shape = [num_nodes_in_graph]\n",
    "\n",
    "        # Edge slice:\n",
    "        e_start = edge_ptr[i].item()\n",
    "        e_end   = edge_ptr[i + 1].item()\n",
    "        edge_seg = edge_flows[e_start:e_end].view(-1)  # shape = [num_edges_in_graph]\n",
    "\n",
    "        # Now rebuild the “flat” parameter ordering per layer:\n",
    "        # For ℓ in 1..num_layers:\n",
    "        #   1) Flatten weight matrix of layer ℓ (in_dim * out_dim entries)\n",
    "        #   2) Then flatten bias vector of layer ℓ (out_dim entries)\n",
    "        es = []\n",
    "        ns = []\n",
    "        e_idx = 0\n",
    "        # Node index offset: skip input nodes → first bias layer begins at index = input_dim\n",
    "        n_idx = input_dim\n",
    "\n",
    "        for layer_id in range(1, num_layers + 1):\n",
    "            in_dim  = layer_sizes[layer_id - 1]\n",
    "            out_dim = layer_sizes[layer_id]\n",
    "            n_w = in_dim * out_dim\n",
    "\n",
    "            # 3a) weight entries for this layer ℓ\n",
    "            e_layer = edge_seg[e_idx : e_idx + n_w]  # [n_w]\n",
    "            es.append(e_layer)\n",
    "            e_idx += n_w\n",
    "\n",
    "            # 3b) bias entries for this layer ℓ\n",
    "            b_layer = node_seg[n_idx : n_idx + out_dim]  # [out_dim]\n",
    "            ns.append(b_layer)\n",
    "            n_idx += out_dim\n",
    "\n",
    "        # 4) Concatenate in [w1, b1, w2, b2, ..., wL, bL] order\n",
    "        flat_i = torch.cat([torch.cat([es[j], ns[j]], dim=0) for j in range(num_layers)], dim=0)\n",
    "        flat_list.append(flat_i)\n",
    "\n",
    "    # 5) Stack into a single Tensor [batch_size, flat_dim]\n",
    "    flat_batch = torch.stack(flat_list, dim=0)  # [batch_size, flat_dim]\n",
    "    return flat_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143a92b1-bb36-4f17-aad6-09bf9cb8b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGraphCFM:\n",
    "    \"\"\"\n",
    "    A conditional flow‐matching trainer for graphs, analogous to SimpleCFM,\n",
    "    but using our GraphFlowNet with (t, c) conditioning. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        targetloader: torch.utils.data.DataLoader,  # yields (Batch_of_graphs, graph_type_labels)\n",
    "        model: GraphFlowNet,\n",
    "        layer_sizes: list,     # e.g. [784,512,256,10] (for flattening helper)\n",
    "        device: torch.device = None,\n",
    "        source_std: float = 0.001,\n",
    "    ):\n",
    "        self.targetloader   = targetloader\n",
    "        self.model          = model.to(device)\n",
    "        self.layer_sizes    = layer_sizes\n",
    "        self.device         = device if device is not None else torch.device(\"cpu\")\n",
    "        self.source_std     = source_std\n",
    "        self.metrics        = {\"train_loss\": []}\n",
    "        self.best_loss      = float(\"inf\")\n",
    "        self.best_model_sd  = None\n",
    "\n",
    "        # Iterator over targetloader\n",
    "        self._target_iter = iter(self.targetloader)\n",
    "\n",
    "    def sample_from_loader(self):\n",
    "        \"\"\"\n",
    "        1) Pull one batch: (batch_data1, types1) from targetloader.\n",
    "           - batch_data1 is a PyG Batch of B graphs.\n",
    "           - types1     is [B] long Tensor (graph types).\n",
    "        2) On‐the‐fly: build a “source” batch_data0 with same topology & types:\n",
    "           • Replace each graph’s edge_attr by random N(0, source_std^2)\n",
    "           • Replace each graph’s node‐features (bias entries) by N(0, source_std^2)\n",
    "           • Keep one‐hot‐layers in node_attr & edge_attr unchanged.\n",
    "        3) Sample t ∈ [0,1]^B, then interpolate:\n",
    "           batch_data_t.edge_attr = edge_attr0 + t*(edge_attr1 - edge_attr0)\n",
    "           batch_data_t.x         = node_feat0 + t*(node_feat1 - node_feat0)\n",
    "        4) Construct a `flow` namespace with:\n",
    "           - flow.batch_data_t   (graph at time t)\n",
    "           - flow.t              [B,1]\n",
    "           - flow.true_edge_flow = edge_attr1[:,0] - edge_attr0[:,0]   (just the weight‐component)\n",
    "           - flow.true_node_flow = node_feat1[:,0] - node_feat0[:,0]   (just the bias‐component)\n",
    "           - flow.types = types1  [B]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_data1, types1 = next(self._target_iter)\n",
    "        except StopIteration:\n",
    "            self._target_iter = iter(self.targetloader)\n",
    "            batch_data1, types1 = next(self._target_iter)\n",
    "\n",
    "        batch_data1 = batch_data1.to(self.device)\n",
    "        types1 = types1.to(self.device).long()  # [B]\n",
    "\n",
    "        B = batch_data1.num_graphs\n",
    "        N_total = batch_data1.num_nodes\n",
    "        E_total = batch_data1.num_edges\n",
    "\n",
    "        # ——— 2) Build “source” batch_data0, reusing the same graph structure:\n",
    "        # Copy the original Data object (topology, node & edge one‐hot parts), then overwrite weight/bias dims.\n",
    "        \n",
    "        # batch_data0 = Batch.from_data_list(batch_data1.to_data_list())  # clone\n",
    "        # batch_data0.to(self.device)\n",
    "\n",
    "        batch_data0 = batch_data1\n",
    "        batch_data0.to(self.device)\n",
    "        \n",
    "        # a) For each edge, sample random weight ∼ N(0, source_std^2):\n",
    "        #    data.edge_attr is [E_total, 1 + num_layers]. The first column is the weight; the rest are one‐hot.\n",
    "        edge_attr1 = batch_data1.edge_attr  # [E_total, 1 + num_layers]\n",
    "        edge_one_hot = edge_attr1[:, 1:]    # [E_total, num_layers]\n",
    "        rand_w0 = torch.randn(E_total, 1, device=self.device) * self.source_std\n",
    "        batch_data0.edge_attr = torch.cat([rand_w0, edge_one_hot], dim=-1)  # [E_total, 1+num_layers]\n",
    "\n",
    "        batch_data0.edge_weight = torch.randn(E_total, 1, device=self.device) * self.source_std\n",
    "\n",
    "        # b) For each node, sample random bias ∼ N(0, source_std^2):\n",
    "        node_feat1 = batch_data1.x  # [N_total, 1 + (num_layers+1)] = [bias || one-hot-node-layer]\n",
    "        node_one_hot = node_feat1[:, 1:]  # [N_total, num_layers+1]\n",
    "        rand_b0 = torch.randn(N_total, 1, device=self.device) * self.source_std\n",
    "        batch_data0.x = torch.cat([rand_b0, node_one_hot], dim=-1)  # [N_total, 1 + (num_layers+1)]\n",
    "\n",
    "        # ——— 3) Sample t ∈ [0,1] for each graph in batch\n",
    "        t = torch.rand(B, 1, device=self.device)  # [B,1]\n",
    "\n",
    "        # We need to broadcast t to all nodes/edges according to which graph they belong to:\n",
    "        node_batch = batch_data1.batch           # [N_total], node→graph index\n",
    "        edge_batch = batch_data1.batch[batch_data1.edge_index[0]]  # [E_total]\n",
    "\n",
    "        t_nodes = t[node_batch]   # [N_total, 1]\n",
    "        t_edges = t[edge_batch]   # [E_total, 1]\n",
    "\n",
    "        # a) Interpolate node features:\n",
    "        node_feat0 = batch_data0.x  # [N_total, 1 + (num_layers+1)]\n",
    "        node_feat1 = batch_data1.x\n",
    "        # Only the *bias* entry (column 0) should be interpolated; the one‐hot (columns 1:) stays as in batch_data1.\n",
    "        b0 = node_feat0[:, [0]]  # [N_total,1]\n",
    "        b1 = node_feat1[:, [0]]  # [N_total,1]\n",
    "        node_one_hot = node_feat1[:, 1:]   # [N_total, num_layers+1]\n",
    "        b_t = b0 + t_nodes * (b1 - b0)      # [N_total,1]\n",
    "        \n",
    "        batch_data_t = batch_data1 #Batch.from_data_list(batch_data1.to_data_list())  # clone again to fill in xt\n",
    "        batch_data_t.to(self.device)\n",
    "        batch_data_t.x = torch.cat([b_t, node_one_hot], dim=-1)\n",
    "\n",
    "        # b) Interpolate edge features:\n",
    "        edge_attr0 = batch_data0.edge_attr   # [E_total, 1+num_layers]\n",
    "        edge_attr1 = batch_data1.edge_attr\n",
    "        w0 = edge_attr0[:, [0]]  # [E_total,1]\n",
    "        w1 = edge_attr1[:, [0]]  # [E_total,1]\n",
    "        edge_one_hot = edge_attr1[:, 1:]     # [E_total, num_layers]\n",
    "        w_t = w0 + t_edges * (w1 - w0)       # [E_total,1]\n",
    "        batch_data_t.edge_attr = torch.cat([w_t, edge_one_hot], dim=-1)\n",
    "\n",
    "        w0 = batch_data0.edge_weight\n",
    "        w1 = batch_data1.edge_weight\n",
    "        w_t = w0 + t_edges * (w1-w0)\n",
    "        batch_data_t.edge_weight = w_t \n",
    "        \n",
    "        # c) True flows (for loss):\n",
    "        true_node_flow = (b1 - b0).view(N_total, 1)     # [N_total,1]\n",
    "        true_edge_flow = (w1 - w0).view(E_total, 1)     # [E_total,1]\n",
    "\n",
    "        # d) Build flow namespace and return:\n",
    "        flow = SimpleNamespace()\n",
    "        flow.batch_data_t   = batch_data_t   # Data object at time t\n",
    "        flow.t              = t              # [B,1]\n",
    "        flow.true_node_flow = true_node_flow # [N_total,1]\n",
    "        flow.true_edge_flow = true_edge_flow # [E_total,1]\n",
    "        flow.types          = types1         # [B], graph‐type labels\n",
    "\n",
    "        return flow\n",
    "\n",
    "    def forward(self, flow: SimpleNamespace):\n",
    "        \"\"\"\n",
    "        Given flow.batch_data_t, flow.t, flow.types, call GraphFlowNet → (node_flows, edge_flows)\n",
    "        \"\"\"\n",
    "        batch_data_t = flow.batch_data_t\n",
    "        t           = flow.t\n",
    "        c           = flow.types\n",
    "\n",
    "        node_pred, edge_pred = self.model(batch_data_t, t, c)\n",
    "        return node_pred, edge_pred\n",
    "\n",
    "    def train(self, n_iters: int, optimizer: torch.optim.Optimizer):\n",
    "        \"\"\"\n",
    "        Standard flow‐matching loop:\n",
    "          For it in 1..n_iters:\n",
    "            1) flow = sample_from_loader()\n",
    "            2) (node_pred, edge_pred) = forward(flow)\n",
    "            3) Compute MSE against (flow.true_node_flow, flow.true_edge_flow)\n",
    "            4) Backprop + optimizer.step()\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        start_time = time.time()\n",
    "        for it in range(1, n_iters + 1):\n",
    "            flow          = self.sample_from_loader()\n",
    "            node_pred, edge_pred = self.forward(flow)\n",
    "\n",
    "            # Flatten both predictions & truths into a single loss scalar:\n",
    "            # noise_ = F.normalize(noise.view(noise.size(0), -1), dim=1, p=2).view(noise.size())\n",
    "            # pred_  = F.normalize(pred.view(pred.size(0), -1), dim=1, p=2).view(pred.size())\n",
    "            \n",
    "            loss_nodes = F.mse_loss(node_pred, flow.true_node_flow)\n",
    "            loss_edges = F.mse_loss(edge_pred, flow.true_edge_flow)\n",
    "            loss = loss_nodes + loss_edges\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # logging.info(f\"Iteration: {it} complete. {it/(n_iters+1)*100 :.2f}% complete. \")\n",
    "            \n",
    "            self.metrics[\"train_loss\"].append(loss.item())\n",
    "            if it % 100 == 0 or it == n_iters:\n",
    "                end_time = time.time()\n",
    "                print(f\"[Iter {it:5d}/{n_iters:5d}]  Loss = {loss.item():.6f}, took: {(end_time - start_time):.2f}s\")\n",
    "                \n",
    "                start_time = end_time\n",
    "                \n",
    "                checkpoint_dir = 'checkpoints'\n",
    "                os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                ckpt_path = os.path.join(checkpoint_dir, f'graph_cfm_{it}.pth') # changed!\n",
    "                torch.save(self.model.state_dict(), ckpt_path)\n",
    "\n",
    "            if loss.item() < self.best_loss:\n",
    "                self.best_loss = loss.item()\n",
    "                self.best_model_sd = {\n",
    "                    \"model\":     self.model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"iteration\": it,\n",
    "                }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def map(\n",
    "        self,\n",
    "        random_graph_batch: Data,\n",
    "        types: torch.Tensor,\n",
    "        n_steps: int = 100,\n",
    "        noise_scale: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Perform Euler integration from t=0→1 on a batch of “random_graph_batch”,\n",
    "        conditioned on graph types `types`:\n",
    "         x_{t+dt} = x_t + Δx_t * dt  (where Δx_t = GNN(x_t, t, c))\n",
    "\n",
    "        Args:\n",
    "          random_graph_batch : a PyG Batch with:\n",
    "                                 • x = initial node‐features at t=0 (noise)\n",
    "                                 • edge_attr = initial edge‐attrs at t=0 (noise)\n",
    "                                 • batch, edge_index remain same topology\n",
    "          types              : [B] long Tensor of graph‐type indices ∈ {0,…,C−1}\n",
    "          n_steps            : # Euler steps\n",
    "          noise_scale        : Gaussian noise scale to add at each step\n",
    "        Returns:\n",
    "          final_graph_batch : PyG Batch where x, edge_attr have been updated to t=1\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        G = random_graph_batch.to(self.device)\n",
    "        B = G.num_graphs\n",
    "        N_total = G.num_nodes\n",
    "        E_total = G.num_edges\n",
    "\n",
    "        # On‐the‐fly, extract node_one_hot and edge_one_hot (unchanged):\n",
    "        node_one_hot = G.x[:, 1:]        # [N_total, num_layers+1]\n",
    "        edge_one_hot = G.edge_attr[:, 1:] # [E_total, num_layers]\n",
    "\n",
    "        # Initialize node_bias⁽0⁾ and edge_w⁽0⁾ from G.x[:,0] and G.edge_attr[:,0]\n",
    "        x = G.x[:, [0]].clone()           # [N_total,1]\n",
    "        w = G.edge_attr[:, [0]].clone()   # [E_total,1]\n",
    "\n",
    "        dt = 1.0 / float(n_steps)\n",
    "        for i in range(n_steps):\n",
    "            t_i = torch.full((B, 1), float(i) / float(n_steps), device=self.device)\n",
    "\n",
    "            # Build a new Data object on the fly with current x_t, w_t, plus one‐hots\n",
    "            G_t = Batch.from_data_list(G.to_data_list())  # clone topology & one‐hot's\n",
    "            G_t.to(self.device)\n",
    "            G_t.x = torch.cat([x, node_one_hot], dim=-1)                # [N_total, 1 + (num_layers+1)]\n",
    "            G_t.edge_attr = torch.cat([w, edge_one_hot], dim=-1)       # [E_total, 1 + num_layers]\n",
    "\n",
    "            # Predict flows:\n",
    "            node_flow_pred, edge_flow_pred = self.model(G_t, t_i, types)  # [N_total,1], [E_total,1]\n",
    "\n",
    "            # Euler update on bias & weight:\n",
    "            x = x + node_flow_pred * dt\n",
    "            w = w + edge_flow_pred * dt\n",
    "\n",
    "            if noise_scale > 0.0:\n",
    "                x = x + noise_scale * torch.randn_like(x)\n",
    "                w = w + noise_scale * torch.randn_like(w)\n",
    "\n",
    "        # At the end: build final_graph_batch with x､one_hot & w､edge_one_hot\n",
    "        final_graph_batch = Batch.from_data_list(G.to_data_list())\n",
    "        final_graph_batch.to(self.device)\n",
    "        final_graph_batch.x = torch.cat([x, node_one_hot], dim=-1)\n",
    "        final_graph_batch.edge_attr = torch.cat([w, edge_one_hot], dim=-1)\n",
    "\n",
    "        return final_graph_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54cfaff2-9479-4952-a4ed-d0133bfa8df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledMLPGraphDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wraps MLPGraphDataset so that __getitem__ returns (Data, graph_type).\n",
    "    \"\"\"\n",
    "    def __init__(self, mlp_models, mlp_types, layer_sizes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          mlp_models  : list of MLP instances\n",
    "          mlp_types   : list or 1D tensor of ints, same length as mlp_models,\n",
    "                        each ∈ {0,1,…,C-1}\n",
    "          layer_sizes : e.g. [input_dim, hidden1, hidden2, output_dim]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(mlp_models) == len(mlp_types), \"mlp_models and mlp_types must match length\"\n",
    "        self.base_dataset = MLPGraphDataset(mlp_models, layer_sizes)\n",
    "        self.types        = torch.tensor(mlp_types, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          graph_data : a torch_geometric.data.Data object from MLPGraphDataset\n",
    "          graph_type : a scalar long tensor ∈ {0,…,C-1}\n",
    "        \"\"\"\n",
    "        graph_data = self.base_dataset[idx]  # Data(x, edge_index, edge_attr, batch)\n",
    "        graph_type = self.types[idx]         # Long scalar\n",
    "        return graph_data, graph_type\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "def graph_collate_fn(batch_list):\n",
    "    \"\"\"\n",
    "    batch_list is a Python list of length B, each element = (Data_i, type_i).\n",
    "\n",
    "    We want to return:\n",
    "      • batched_graphs: a PyG Batch\n",
    "      • types_tensor  : a LongTensor of shape [B]\n",
    "    \"\"\"\n",
    "    graphs, types = zip(*batch_list)\n",
    "    batched_graphs = Batch.from_data_list(graphs)\n",
    "    types_tensor   = torch.stack(types, dim=0)  # → [B]\n",
    "    return batched_graphs, types_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b720a06-be85-4f97-be52-6b898286cff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/js/697gbr4j3lzdh9ccy7qxb5ph0000gn/T/ipykernel_81157/900529685.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights, biases = torch.load(model_path, map_location=device)\n",
      "/var/folders/js/697gbr4j3lzdh9ccy7qxb5ph0000gn/T/ipykernel_81157/900529685.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w = torch.index_select(w, axis, torch.tensor(perm[p], device=w.device))\n",
      "/var/folders/js/697gbr4j3lzdh9ccy7qxb5ph0000gn/T/ipykernel_81157/3578387594.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.types        = torch.tensor(mlp_types, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Builds lists of permuted mnist and fmnist models\n",
    "ref_model, original_models, mnist_permuted_models = get_permuted_models_data(ref_point=0, model_type = \"MNIST\")\n",
    "ref_model, original_models, fmnist_permuted_models = get_permuted_models_data(ref_point=0, model_type = \"Fashion MNIST\")\n",
    "\n",
    "layer_size = [196, 32, 32, 10]\n",
    "\n",
    "mnist_graph_dataset = LabeledMLPGraphDataset(mnist_permuted_models, torch.full( [len(mnist_permuted_models)], 0), layer_size)\n",
    "fmnist_graph_dataset = LabeledMLPGraphDataset(fmnist_permuted_models, torch.full( [len(fmnist_permuted_models)], 1), layer_size)\n",
    "\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "# 3) Create a PyG DataLoader with our custom collate_fn\n",
    "batch_size = 64\n",
    "graph_loader = PyGDataLoader(\n",
    "    ConcatDataset([mnist_graph_dataset, fmnist_graph_dataset]),\n",
    "    batch_size   = batch_size,\n",
    "    shuffle      = True,\n",
    "    collate_fn   = graph_collate_fn,\n",
    "    # drop_last    = True,   # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db441848-44c7-41e3-b31f-8c263834538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFM has 2454098 parameters\n"
     ]
    }
   ],
   "source": [
    "num_layers = 3 # number of layers in the dataset MLPs \n",
    "graph_model = GraphFlowNet(\n",
    "    node_in_channels    = 1 + (num_layers + 1),\n",
    "    edge_in_channels    = 1 + num_layers,\n",
    "    num_classes         = 2,\n",
    "    t_embed_dim         = 8,\n",
    "    class_embed_dim     = 32,\n",
    "    hidden_dim          = 512,\n",
    "    num_message_passing_layers = 2,\n",
    "    heads               = 1,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "cfm = ConditionalGraphCFM(\n",
    "    targetloader = graph_loader,\n",
    "    model        = graph_model,\n",
    "    layer_sizes  = layer_size,  # for flattening if needed\n",
    "    device       = device,\n",
    "    source_std   = 1e-1,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(graph_model.parameters(), lr=1e-4)\n",
    "# cfm.train(n_iters=1000, optimizer=optimizer)\n",
    "\n",
    "print(f\"CFM has {count_parameters(graph_model)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3958aac-238a-47a5-93d4-6c4bed80d541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter   100/ 2000]  Loss = 0.000004, took: 1528.47s\n",
      "[Iter   200/ 2000]  Loss = 0.000001, took: 1530.81s\n",
      "[Iter   300/ 2000]  Loss = 0.000012, took: 1502.36s\n",
      "[Iter   400/ 2000]  Loss = 0.000001, took: 1500.07s\n",
      "[Iter   500/ 2000]  Loss = 0.000001, took: 1522.99s\n",
      "[Iter   600/ 2000]  Loss = 0.000000, took: 1526.65s\n",
      "[Iter   700/ 2000]  Loss = 0.000103, took: 1516.96s\n",
      "[Iter   800/ 2000]  Loss = 0.000001, took: 1520.38s\n",
      "[Iter   900/ 2000]  Loss = 0.000000, took: 1535.99s\n",
      "[Iter  1000/ 2000]  Loss = 0.000001, took: 1533.07s\n",
      "[Iter  1100/ 2000]  Loss = 0.000350, took: 1545.76s\n",
      "[Iter  1200/ 2000]  Loss = 0.000003, took: 1579.50s\n",
      "[Iter  1300/ 2000]  Loss = 0.000000, took: 1581.85s\n",
      "[Iter  1400/ 2000]  Loss = 0.000015, took: 1585.14s\n",
      "[Iter  1500/ 2000]  Loss = 0.000233, took: 1580.35s\n",
      "[Iter  1600/ 2000]  Loss = 0.000000, took: 1615.83s\n",
      "[Iter  1700/ 2000]  Loss = 0.000001, took: 1604.75s\n",
      "[Iter  1800/ 2000]  Loss = 0.000049, took: 1622.08s\n",
      "[Iter  1900/ 2000]  Loss = 0.000054, took: 1622.19s\n",
      "[Iter  2000/ 2000]  Loss = 0.000001, took: 1627.75s\n"
     ]
    }
   ],
   "source": [
    "cfm.train(n_iters=2000, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "614f130c-e77e-4e27-8e8c-63907a79f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load last best saved\n",
    "\n",
    "\n",
    "# checkpoint_dir = 'checkpoints'\n",
    "\n",
    "# load_it = 200\n",
    "# last_checkpoint = os.path.join(checkpoint_dir, f'graph_cfm_{load_it}.pth')\n",
    "\n",
    "\n",
    "# num_layers = 3 # number of layers in the dataset MLPs \n",
    "# graph_model = GraphFlowNet(\n",
    "#     node_in_channels    = 1 + (num_layers + 1),\n",
    "#     edge_in_channels    = 1 + num_layers,\n",
    "#     num_classes         = 2,\n",
    "#     t_embed_dim         = 16,\n",
    "#     class_embed_dim     = 16,\n",
    "#     hidden_dim          = 64,\n",
    "#     num_message_passing_layers = 4,\n",
    "#     heads               = 1,\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "# graph_model.load_state_dict(torch.load(last_checkpoint))\n",
    "# graph_model.eval()\n",
    "# print(f\"Loaded checkpoint from '{last_checkpoint}'\")\n",
    "\n",
    "# cfm = ConditionalGraphCFM(\n",
    "#     targetloader = graph_loader,\n",
    "#     model        = graph_model,\n",
    "#     layer_sizes  = layer_size,  # for flattening if needed\n",
    "#     device       = device,\n",
    "#     source_std   = 1e-1,\n",
    "# )\n",
    "\n",
    "# optimizer = torch.optim.Adam(graph_model.parameters(), lr=1e-4)\n",
    "# # cfm.train(n_iters=1000, optimizer=optimizer)\n",
    "\n",
    "# print(f\"CFM has {count_parameters(graph_model)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ccbd705-f654-4530-a83c-3936de6a6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to handle test_loader selection correctly. \n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((14, 14)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset with the downsampling transform\n",
    "mnist_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n",
    "mnist_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform)\n",
    "\n",
    "# Create data loaders\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train_dataset, batch_size=64, shuffle=True)\n",
    "mnist_test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Resize (14x14) and Normalize Fashion MNIST images from torch.vision then create dataset and dataloader\n",
    "fashion_mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((14, 14)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset with the downsampling transform\n",
    "fashion_mnist_train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=fashion_mnist_transform)\n",
    "fashion_mnist_test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=fashion_mnist_transform)\n",
    "\n",
    "# Create data loaders\n",
    "fashion_mnist_train_loader = torch.utils.data.DataLoader(fashion_mnist_train_dataset, batch_size=64, shuffle=True)\n",
    "fashion_mnist_test_loader = torch.utils.data.DataLoader(fashion_mnist_test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b284b2c9-0191-449e-a507-06be7ce2ff9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:09:30 INFO: Generated Fashion MNIST MLP 0 accuracy: 4.98%\n",
      "10:09:31 INFO: Generated Fashion MNIST MLP 1 accuracy: 9.24%\n",
      "10:09:32 INFO: Generated Fashion MNIST MLP 2 accuracy: 7.52%\n",
      "10:09:33 INFO: Generated Fashion MNIST MLP 3 accuracy: 4.24%\n",
      "10:09:34 INFO: Generated Fashion MNIST MLP 4 accuracy: 14.84%\n",
      "10:09:35 INFO: Generated Fashion MNIST MLP 5 accuracy: 4.76%\n",
      "10:09:35 INFO: Generated Fashion MNIST MLP 6 accuracy: 9.32%\n",
      "10:09:36 INFO: Generated Fashion MNIST MLP 7 accuracy: 11.68%\n",
      "10:09:37 INFO: Generated Fashion MNIST MLP 8 accuracy: 9.85%\n",
      "10:09:37 INFO: Generated Fashion MNIST MLP 9 accuracy: 8.65%\n"
     ]
    }
   ],
   "source": [
    "generate_type = 1\n",
    "B          = 10\n",
    "source_std = 1e-1 #cfm.source_std\n",
    "n_steps     = 1\n",
    "noise_scale = source_std\n",
    "\n",
    "if generate_type == 0:\n",
    "    test_loader = mnist_test_loader\n",
    "    template_dataset = mnist_graph_dataset\n",
    "else: \n",
    "    test_loader = fashion_mnist_test_loader\n",
    "    template_dataset = fmnist_graph_dataset\n",
    "\n",
    "# ── (A) Grab one template to copy topology + one-hots ──\n",
    "template_graph, template_type = template_dataset[0] # test with type 0 \n",
    "template_graph = template_graph.to(device)\n",
    "template_type  = template_type.to(device).long()\n",
    "\n",
    "# ── (B) Build a batch of B random‐noise graphs with the same topology/one-hots ──\n",
    "random_graphs = []\n",
    "for _ in range(B):\n",
    "    G0 = template_graph.clone().to(device)\n",
    "\n",
    "    # overwrite node biases\n",
    "    N0 = G0.num_nodes\n",
    "    node_one_hot = G0.x[:, 1:].clone()                 # [N0, num_layers+1]\n",
    "    rand_bias    = torch.randn(N0, 1, device=device) * source_std\n",
    "    G0.x         = torch.cat([rand_bias, node_one_hot], dim=-1)\n",
    "\n",
    "    # overwrite edge weights\n",
    "    E0 = G0.num_edges\n",
    "    edge_one_hot = G0.edge_attr[:, 1:].clone()          # [E0, num_layers]\n",
    "    rand_weight  = torch.randn(E0, 1, device=device) * source_std\n",
    "    G0.edge_attr  = torch.cat([rand_weight, edge_one_hot], dim=-1)\n",
    "\n",
    "    # G0.edge_weight\n",
    "    random_graphs.append(G0)\n",
    "\n",
    "random_graph_batch = Batch.from_data_list(random_graphs).to(device)\n",
    "\n",
    "# ── (C) Choose a class c for all B graphs ──\n",
    "target_type = template_type.item()   # e.g. 0\n",
    "types = torch.full((B,), target_type, dtype=torch.long, device=device)\n",
    "\n",
    "# ── (D) Run the learned flow: integrate from t=0→1 ──\n",
    "final_graph_batch = cfm.map(\n",
    "    random_graph_batch = random_graph_batch,\n",
    "    types              = types,\n",
    "    n_steps            = n_steps,\n",
    "    noise_scale        = noise_scale,\n",
    ")\n",
    "\n",
    "# ── (E) Optionally flatten to [B, flat_dim] if you want raw weight‐vectors ──\n",
    "flat_batch = flatten_batch_params(\n",
    "    batch_data  = final_graph_batch,\n",
    "    node_flows  = final_graph_batch.x[:, [0]],        # [N_total,1]\n",
    "    edge_flows  = final_graph_batch.edge_attr[:, [0]],# [E_total,1]\n",
    "    layer_sizes = layer_size\n",
    ")\n",
    "# # flat_batch.shape == [B, flat_dim]\n",
    "\n",
    "# # Now each flat_batch[i] can be converted back to a WeightSpaceObject, or used for evaluation.\n",
    "\n",
    "# Convert to MLP weights and save\n",
    "accuracies = []\n",
    "for i in range(B):\n",
    "    new_wso = WeightSpaceObject.from_flat(\n",
    "        flat_batch[i], \n",
    "        layers=np.array(layer_size), \n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    expected_weight_shapes = [(32, 196), (32, 32), (10, 32)]\n",
    "    expected_bias_shapes = [(32,), (32,), (10,)]\n",
    "\n",
    "    assert len(new_wso.weights) == 3, f\"Expected 3 weight matrices, got {len(new_wso.weights)}\"\n",
    "    assert len(new_wso.biases) == 3, f\"Expected 3 bias vectors, got {len(new_wso.biases)}\"\n",
    "    \n",
    "    # Check each weight and bias shape\n",
    "    for j, (w, expected_shape) in enumerate(zip(new_wso.weights, expected_weight_shapes)):\n",
    "        assert w.shape == expected_shape, f\"Weight {j} has shape {w.shape}, expected {expected_shape}\"\n",
    "    \n",
    "    for j, (b, expected_shape) in enumerate(zip(new_wso.biases, expected_bias_shapes)):\n",
    "        assert b.shape == expected_shape, f\"Bias {j} has shape {b.shape}, expected {expected_shape}\"\n",
    "\n",
    "    # Save the generated weights\n",
    "    # torch.save(\n",
    "    #     (new_wso.weights, new_wso.biases),\n",
    "    #     f\"generated_mlp_weights_{i}.pt\"\n",
    "    # )\n",
    "\n",
    "    # Create and test model\n",
    "    model = MLP()\n",
    "    model.fc1.weight.data = new_wso.weights[0].clone()\n",
    "    model.fc1.bias.data = new_wso.biases[0].clone()\n",
    "    model.fc2.weight.data = new_wso.weights[1].clone()\n",
    "    model.fc2.bias.data = new_wso.biases[1].clone()\n",
    "    model.fc3.weight.data = new_wso.weights[2].clone()\n",
    "    model.fc3.bias.data = new_wso.biases[2].clone()\n",
    "\n",
    "    acc = test_mlp(model, test_loader)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    title_type = 0\n",
    "    if generate_type == 0:  title_type = f\"MNIST\" \n",
    "    else: title_type = f\"Fashion MNIST\"\n",
    "        \n",
    "    logging.info(f\"Generated { title_type } MLP {i} accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32dd4187-76e0-4098-ba44-25fc3515e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 9.46% and std 3.21%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(accuracies)\n",
    "print(f\"mean {np.mean(accuracies):.2f}% and std {np.std(accuracies):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e33d13-ee65-4dcc-87c9-e6d33caa588d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1bf56-087a-4729-8974-77748588f9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af7a7b-f6ad-407e-98b8-e40c23541625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
