{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6ae30c-e017-4738-87d3-02d74771a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.optim import Adam\n",
    "from collections import defaultdict\n",
    "import numpy.ma as ma\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from nn.models import DWSModel\n",
    "from typing import Tuple, NamedTuple\n",
    "from nn.layers import BN, DWSLayer,InvariantLayer, Dropout, ReLU\n",
    "from nn.layers.base import BaseLayer,GeneralSetLayer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1790b3d2-d288-4b5e-9339-1707cf7c267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are identical: True\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "def permute_layer_weights(layer, perm):\n",
    "    \"\"\"Permutes weights and biases of a layer based on the given permutation matrix.\"\"\"\n",
    "    weight = layer.weight.data.clone()\n",
    "    bias = layer.bias.data.clone()\n",
    "\n",
    "    # Permute rows of weights (output dimension)\n",
    "    layer.weight.data = weight[perm, :]\n",
    "\n",
    "    # Permute bias (output dimension)\n",
    "    layer.bias.data = bias[perm]\n",
    "    return layer\n",
    "\n",
    "def permute_model(model, permutations):\n",
    "    \"\"\"Applies a list of permutations to a model's layers.\"\"\"\n",
    "    permute_layer_weights(model.hidden1, permutations[0])\n",
    "    permute_layer_weights(model.hidden2, permutations[1])\n",
    "\n",
    "    # Adjust input weights of the second hidden layer\n",
    "    model.hidden2.weight.data = model.hidden2.weight.data[:, permutations[0]]\n",
    "    # Adjust input weights of the output layer\n",
    "    model.output.weight.data = model.output.weight.data[:, permutations[1]]\n",
    "    return model\n",
    "\n",
    "def generate_permutation_matrix(hidden_dim):\n",
    "    \"\"\"Generates a random permutation matrix.\"\"\"\n",
    "    perm = np.random.permutation(hidden_dim)\n",
    "    return torch.tensor(perm, dtype=torch.long)\n",
    "\n",
    "# Example Usage\n",
    "input_dim = 10\n",
    "hidden_dim = 16\n",
    "output_dim = 5\n",
    "\n",
    "# Initialize two identical models\n",
    "model_a = SimpleMLP(input_dim, hidden_dim, output_dim)\n",
    "model_b = SimpleMLP(input_dim, hidden_dim, output_dim)\n",
    "model_b.load_state_dict(model_a.state_dict())  # Ensure identical initial weights\n",
    "\n",
    "# Generate random permutations for each hidden layer\n",
    "perm1 = generate_permutation_matrix(hidden_dim)\n",
    "perm2 = generate_permutation_matrix(hidden_dim)\n",
    "\n",
    "# Apply permutations to model B\n",
    "model_c = permute_model(model_b, [perm1, perm1]) # dont need to be seperate perms\n",
    "\n",
    "# Verify functionality is preserved\n",
    "x = torch.rand(1, input_dim)\n",
    "output_a = model_a(x)\n",
    "output_b = model_b(x)\n",
    "output_c = model_c(x)\n",
    "\n",
    "print(\"Outputs are identical:\", torch.allclose(output_a, output_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f014adf-b457-4dfe-9026-6aaf2f2992f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2754,  0.1488,  0.1450,  0.0545,  0.0156]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.2754,  0.1488,  0.1450,  0.0545,  0.0156]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.2754,  0.1488,  0.1450,  0.0545,  0.0156]],\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_a, output_b, output_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1828cb62-23d0-4af6-80a8-290777ad53b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1146,  0.1517,  0.0849, -0.1838, -0.2705,  0.3162,  0.2603,  0.0251,\n",
      "          0.3071, -0.2898],\n",
      "        [-0.1530,  0.2477,  0.1027,  0.2589, -0.2825,  0.2156, -0.0958,  0.2503,\n",
      "          0.1518, -0.3014],\n",
      "        [-0.0836, -0.1677,  0.2058, -0.2556, -0.1140,  0.1546,  0.2420,  0.0414,\n",
      "         -0.0385, -0.0745],\n",
      "        [ 0.2134,  0.2503, -0.0340,  0.2803, -0.2951, -0.2324, -0.2870, -0.0131,\n",
      "          0.0861, -0.2488],\n",
      "        [-0.0777, -0.1802,  0.0702, -0.0501,  0.2609,  0.0517,  0.0555, -0.1036,\n",
      "         -0.2032, -0.2833],\n",
      "        [-0.0096,  0.1308,  0.3123,  0.1388,  0.0583, -0.0722,  0.1712, -0.0098,\n",
      "         -0.2590, -0.1907],\n",
      "        [-0.0919, -0.1051, -0.2803, -0.0928, -0.0947,  0.1869, -0.1440, -0.2760,\n",
      "         -0.2720,  0.2293],\n",
      "        [ 0.0651, -0.0656, -0.1989,  0.0404,  0.2409, -0.1033,  0.0823,  0.1227,\n",
      "          0.2240,  0.0412],\n",
      "        [-0.0895, -0.0282,  0.2695, -0.2857,  0.1852, -0.0996, -0.2727, -0.1151,\n",
      "          0.1198, -0.1558],\n",
      "        [ 0.1114, -0.2235,  0.2442, -0.2317,  0.1501,  0.1223,  0.0101, -0.0513,\n",
      "         -0.3002,  0.3142],\n",
      "        [-0.0876,  0.0203, -0.1113, -0.0459, -0.2158, -0.0721,  0.1974,  0.0325,\n",
      "          0.0497,  0.0655],\n",
      "        [ 0.1899, -0.1269,  0.1942, -0.0556, -0.0682, -0.0782, -0.1732, -0.0059,\n",
      "          0.2157, -0.0096],\n",
      "        [ 0.2267, -0.1168, -0.2147, -0.2599,  0.0416,  0.0828, -0.2591, -0.0569,\n",
      "         -0.1012, -0.2804],\n",
      "        [ 0.2778, -0.0560, -0.2478, -0.2948, -0.2691,  0.2053,  0.0455,  0.1710,\n",
      "         -0.1400, -0.0757],\n",
      "        [ 0.3083, -0.0350,  0.0527,  0.2350,  0.1415, -0.0161,  0.1777, -0.0959,\n",
      "         -0.2326,  0.1382],\n",
      "        [-0.1033, -0.0011,  0.2597,  0.1204, -0.1104, -0.1090,  0.0613, -0.1926,\n",
      "         -0.3153, -0.0248]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print( model_a.hidden1.weight )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6af1284b-0ceb-4277-b44e-99ee5852cc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0836, -0.1677,  0.2058, -0.2556, -0.1140,  0.1546,  0.2420,  0.0414,\n",
      "         -0.0385, -0.0745],\n",
      "        [-0.0777, -0.1802,  0.0702, -0.0501,  0.2609,  0.0517,  0.0555, -0.1036,\n",
      "         -0.2032, -0.2833],\n",
      "        [ 0.2267, -0.1168, -0.2147, -0.2599,  0.0416,  0.0828, -0.2591, -0.0569,\n",
      "         -0.1012, -0.2804],\n",
      "        [-0.0096,  0.1308,  0.3123,  0.1388,  0.0583, -0.0722,  0.1712, -0.0098,\n",
      "         -0.2590, -0.1907],\n",
      "        [ 0.1146,  0.1517,  0.0849, -0.1838, -0.2705,  0.3162,  0.2603,  0.0251,\n",
      "          0.3071, -0.2898],\n",
      "        [-0.0919, -0.1051, -0.2803, -0.0928, -0.0947,  0.1869, -0.1440, -0.2760,\n",
      "         -0.2720,  0.2293],\n",
      "        [ 0.2134,  0.2503, -0.0340,  0.2803, -0.2951, -0.2324, -0.2870, -0.0131,\n",
      "          0.0861, -0.2488],\n",
      "        [-0.1530,  0.2477,  0.1027,  0.2589, -0.2825,  0.2156, -0.0958,  0.2503,\n",
      "          0.1518, -0.3014],\n",
      "        [ 0.1899, -0.1269,  0.1942, -0.0556, -0.0682, -0.0782, -0.1732, -0.0059,\n",
      "          0.2157, -0.0096],\n",
      "        [-0.1033, -0.0011,  0.2597,  0.1204, -0.1104, -0.1090,  0.0613, -0.1926,\n",
      "         -0.3153, -0.0248],\n",
      "        [ 0.3083, -0.0350,  0.0527,  0.2350,  0.1415, -0.0161,  0.1777, -0.0959,\n",
      "         -0.2326,  0.1382],\n",
      "        [ 0.1114, -0.2235,  0.2442, -0.2317,  0.1501,  0.1223,  0.0101, -0.0513,\n",
      "         -0.3002,  0.3142],\n",
      "        [-0.0876,  0.0203, -0.1113, -0.0459, -0.2158, -0.0721,  0.1974,  0.0325,\n",
      "          0.0497,  0.0655],\n",
      "        [ 0.0651, -0.0656, -0.1989,  0.0404,  0.2409, -0.1033,  0.0823,  0.1227,\n",
      "          0.2240,  0.0412],\n",
      "        [ 0.2778, -0.0560, -0.2478, -0.2948, -0.2691,  0.2053,  0.0455,  0.1710,\n",
      "         -0.1400, -0.0757],\n",
      "        [-0.0895, -0.0282,  0.2695, -0.2857,  0.1852, -0.0996, -0.2727, -0.1151,\n",
      "          0.1198, -0.1558]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print( model_b.hidden1.weight )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89283edb-5f2b-4d22-a817-2b43da3e3a02",
   "metadata": {},
   "source": [
    "# Apply to MLP() dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5be7e23e-118d-4a69-b3ff-e368650e9566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MLP class for MNIST classification\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, init_type='xavier', seed=None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)  # Set a unique seed for reproducibility\n",
    "\n",
    "        self.init_weights(init_type)\n",
    "\n",
    "    def init_weights(self, init_type):\n",
    "        if init_type == 'xavier':\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "            nn.init.xavier_uniform_(self.fc2.weight)\n",
    "            nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        elif init_type == 'he':\n",
    "            nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "        else:\n",
    "            nn.init.normal_(self.fc1.weight)\n",
    "            nn.init.normal_(self.fc2.weight)\n",
    "            nn.init.normal_(self.fc3.weight)\n",
    "        \n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bfdf41d-bd1e-4bb5-bf0a-ab9f69a66bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data loader for MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_data = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "test_data = datasets.MNIST('.', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# MLP management functions: \n",
    "\n",
    "def train_mlp(model, epochs=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def test_mlp(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to compute gradients for evaluation\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "model = MLP()\n",
    "model = train_mlp(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2a66438-0aff-4573-8520-23b7c1fd908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_layer_weights(layer, perm):\n",
    "    \"\"\"Permutes weights and biases of a layer based on the given permutation matrix.\"\"\"\n",
    "    weight = layer.weight.data.clone()\n",
    "    bias = layer.bias.data.clone()\n",
    "\n",
    "    # Permute rows of weights (output dimension)\n",
    "    layer.weight.data = weight[perm, :]\n",
    "\n",
    "    # Permute bias (output dimension)\n",
    "    layer.bias.data = bias[perm]\n",
    "    return layer\n",
    "\n",
    "def permute_model(model, permutations):\n",
    "    \"\"\"Applies a list of permutations to a model's layers.\"\"\"\n",
    "    permute_layer_weights(model.fc1, permutations[0])\n",
    "    permute_layer_weights(model.fc2, permutations[1])\n",
    "\n",
    "    # Adjust input weights of the second hidden layer\n",
    "    model.fc2.weight.data = model.fc2.weight.data[:, permutations[0]]\n",
    "    # Adjust input weights of the output layer\n",
    "    model.fc3.weight.data = model.fc3.weight.data[:, permutations[1]]\n",
    "    return model\n",
    "\n",
    "def generate_permutation_matrix(hidden_dim):\n",
    "    \"\"\"Generates a random permutation matrix.\"\"\"\n",
    "    perm = np.random.permutation(hidden_dim)\n",
    "    return torch.tensor(perm, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a313f24b-0f88-4c44-b0e4-85651a1f57fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are identical: True\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "input_dim = 784\n",
    "hidden_dim = 32\n",
    "output_dim = 10\n",
    "\n",
    "# Generate random permutations for each hidden layer\n",
    "perm1 = generate_permutation_matrix(hidden_dim)\n",
    "perm2 = generate_permutation_matrix(hidden_dim)\n",
    "\n",
    "# Apply permutations to model B\n",
    "model_c = permute_model(model, [perm1, perm1]) # dont need to be seperate perms\n",
    "\n",
    "# Verify functionality is preserved\n",
    "x = torch.ones(1, input_dim)\n",
    "output_a = model(x)\n",
    "output_c = model_c(x)\n",
    "\n",
    "print(\"Outputs are identical:\", torch.allclose(output_a, output_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18364873-09fb-48d0-8cc8-ba220c4a2f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94.64, 94.64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mlp(model, test_loader), test_mlp(model_c, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8109aa-bc36-4ae7-ac7c-392d845b15f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
