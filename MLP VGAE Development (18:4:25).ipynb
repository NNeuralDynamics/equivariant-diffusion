{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9b67ac1d-342e-4aa0-b29d-0a7989d9acdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch_geometric.data import Data, DataLoader  # PyG Data and loader\n",
    "from torch.optim import Adam, SGD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4ba8db29-7ea6-4f7a-978b-628ec6073fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled MNIST tensor shape: torch.Size([14, 14])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the transform to downsample the images to 14x14 pixels\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((14, 14)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset with the downsampling transform\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# # Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_features, test_labels = next(iter(test_loader))\n",
    "true = test_features[0].squeeze()\n",
    "plt.imshow(true, cmap=\"gray\")\n",
    "print(f\"Downsampled MNIST tensor shape: {true.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cbc54ef6-f1ca-4d51-820a-5d501e18e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MNIST classifier MLP class for dataset\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, init_type='xavier', seed=None):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(196, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)  # Set a unique seed for reproducibility\n",
    "\n",
    "        self.init_weights(init_type)\n",
    "\n",
    "    def init_weights(self, init_type):\n",
    "        if init_type == 'xavier':\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "            nn.init.xavier_uniform_(self.fc2.weight)\n",
    "            nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        elif init_type == 'he':\n",
    "            nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "            nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n",
    "        else:\n",
    "            nn.init.normal_(self.fc1.weight)\n",
    "            nn.init.normal_(self.fc2.weight)\n",
    "            nn.init.normal_(self.fc3.weight)\n",
    "        \n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 196)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def test_mlp(model, test_loader, device = device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to compute gradients for evaluation\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train_mlp(model, epochs=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c44a27a7-9090-47f8-82d3-d37c86553caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "###############################\n",
    "# 1. Dataset: Neuron-level Graph Representation\n",
    "###############################\n",
    "class MLPGraphDatasetNeurons(torch.utils.data.Dataset):\n",
    "    def __init__(self, model_folder):\n",
    "        # Collect all .pt files in the folder.\n",
    "        self.model_paths = [os.path.join(model_folder, fname) \n",
    "                            for fname in os.listdir(model_folder) if fname.endswith('.pt')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.model_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Each file stores (weights, biases) using weights_only=True.\n",
    "        weights, biases = torch.load(self.model_paths[idx], weights_only=True)\n",
    "        # --- Build Node Features ---\n",
    "        # Input layer: use the input dimension from the first weight matrix.\n",
    "        input_dim = weights[0].shape[1]\n",
    "        input_feats = torch.zeros(input_dim, 1)\n",
    "        node_features = [input_feats]\n",
    "        for b in biases:\n",
    "            # Each bias is a 1D tensor; reshape it as a column vector.\n",
    "            node_feats = b.view(-1, 1)\n",
    "            node_features.append(node_feats)\n",
    "        x = torch.cat(node_features, dim=0)\n",
    "\n",
    "        # --- Build Edges and Edge Attributes ---\n",
    "        edge_index_list = []\n",
    "        edge_attr_list = []\n",
    "        offset = 0  # starting index for current layer's nodes\n",
    "        for w in weights:\n",
    "            in_dim = w.shape[1]    # nodes in source layer\n",
    "            out_dim = w.shape[0]   # nodes in destination layer\n",
    "            src_offset = offset\n",
    "            dst_offset = offset + in_dim\n",
    "            for i_out in range(out_dim):\n",
    "                for j_in in range(in_dim):\n",
    "                    src = src_offset + j_in\n",
    "                    dst = dst_offset + i_out\n",
    "                    edge_index_list.append([src, dst])\n",
    "                    edge_attr_list.append([w[i_out, j_in].item()])\n",
    "            offset += in_dim\n",
    "        # Convert lists into tensors.\n",
    "        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "def vgae_to_mlp(generated_data):\n",
    "    \"\"\"\n",
    "    generated_data: a Data object that has the reconstructed node features (x)\n",
    "    and edge attributes (edge_attr).  \n",
    "    The original graph was built as:\n",
    "      - First 196 nodes: input layer,\n",
    "      - Next 32 nodes: fc1 neurons,\n",
    "      - Next 32 nodes: fc2 neurons,\n",
    "      - Next 10 nodes: fc3 neurons.\n",
    "    Edge ordering is:\n",
    "      - fc1: edges from input (196 nodes) to fc1 (32 nodes) in a nested loop:\n",
    "             for i in range(32): for j in range(196)\n",
    "      - fc2: edges from fc1 (32 nodes) to fc2 (32 nodes)\n",
    "      - fc3: edges from fc2 (32 nodes) to fc3 (10 nodes)\n",
    "    This function instantiates a new MLP and sets its weights and biases from the generated outputs.\n",
    "    \"\"\"\n",
    "    x_rec = generated_data.x.squeeze()  # shape: (196+32+32+10,)\n",
    "    edge_attr_rec = generated_data.edge_attr  # shape: (32*196 + 32*32 + 10*32,)\n",
    "\n",
    "    # Extract biases.\n",
    "    fc1_bias = x_rec[196:196+32]\n",
    "    fc2_bias = x_rec[196+32:196+32+32]\n",
    "    fc3_bias = x_rec[196+32+32:196+32+32+10]\n",
    "\n",
    "    # Extract edge weights.\n",
    "    # For fc1: first 32*196 values.\n",
    "    fc1_weight = edge_attr_rec[:32*196].view(32, 196)\n",
    "    # For fc2: next 32*32 values.\n",
    "    start_fc2 = 32*196\n",
    "    fc2_weight = edge_attr_rec[start_fc2:start_fc2+32*32].view(32, 32)\n",
    "    # For fc3: remaining 10*32 values.\n",
    "    start_fc3 = start_fc2 + 32*32\n",
    "    fc3_weight = edge_attr_rec[start_fc3:start_fc3+10*32].view(10, 32)\n",
    "\n",
    "    # Create a new instance of the MLP.\n",
    "    new_mlp = MLP()\n",
    "    # Assign weights and biases.\n",
    "    with torch.no_grad():\n",
    "        new_mlp.fc1.weight.copy_(fc1_weight)\n",
    "        new_mlp.fc1.bias.copy_(fc1_bias)\n",
    "        new_mlp.fc2.weight.copy_(fc2_weight)\n",
    "        new_mlp.fc2.bias.copy_(fc2_bias)\n",
    "        new_mlp.fc3.weight.copy_(fc3_weight)\n",
    "        new_mlp.fc3.bias.copy_(fc3_bias)\n",
    "    return new_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "206434e5-d05c-4a16-8ccd-49d2f3d4fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, BatchNorm, JumpingKnowledge, GraphNorm, GATConv, GAT\n",
    "\n",
    "class GraphBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mode = \"max\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        self.gc1 = GCNConv(in_channels, in_channels)\n",
    "        self.gc2 = GCNConv(in_channels, in_channels)\n",
    "        self.gc3 = GCNConv(in_channels, in_channels)\n",
    "\n",
    "        self.jk = JumpingKnowledge(mode = mode)\n",
    "\n",
    "        self.gn1 = GraphNorm(in_channels) # mode != max broken atm \n",
    "        \n",
    "        self.out = GCNConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        acts = []\n",
    "\n",
    "        h1 = F.elu(self.gc1(x, edge_index, edge_weight))\n",
    "        acts.append(h1)\n",
    "\n",
    "        h2 = F.elu(self.gc2(h1, edge_index, edge_weight))\n",
    "        acts.append(h2)\n",
    "\n",
    "        h3 = F.elu(self.gc3(h2, edge_index, edge_weight))\n",
    "        acts.append(h3)\n",
    "\n",
    "        h4 = F.elu(self.jk(acts))\n",
    "        h5 = self.gn1(h4)\n",
    "\n",
    "        out = F.elu(self.out(h5, edge_index, edge_weight))\n",
    "        \n",
    "        return out\n",
    "\n",
    "###############################\n",
    "# 2. Encoder: GCN-based with Edge Weight Processing\n",
    "###############################\n",
    "class GCNEncoderEdge(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, latent_dim):\n",
    "        super(GCNEncoderEdge, self).__init__()\n",
    "        \n",
    "        # self.gat = GAT(1, 1, 3, 1) \n",
    "        self.gat = GATConv(1, 1)\n",
    "        \n",
    "        self.conv1 = GraphBlock(in_channels, hidden_channels)\n",
    "        self.out = GCNConv(hidden_channels, 2 * latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "    \n",
    "        # Pass edge_weight and apply ReLU.\n",
    "        gat = self.gat(x, edge_index, edge_weight)\n",
    "        x = self.conv1(gat, edge_index, edge_weight)\n",
    "        out = self.out(x, edge_index, edge_weight)\n",
    "        \n",
    "        mu, logvar = torch.chunk(out, 2, dim=1)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "###############################\n",
    "# 3. Decoders:\n",
    "#   (a) EdgeDecoderMLP: Takes concatenated latent vectors of source & target\n",
    "#       and predicts the edge weight.\n",
    "#   (b) NodeDecoderMLP: Decodes each node latent vector into a reconstructed\n",
    "#       node feature (here, the neuron bias).\n",
    "###############################\n",
    "class EdgeDecoderMLP(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=64):\n",
    "        super(EdgeDecoderMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim*2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "\n",
    "        \n",
    "        # Get the latent representations for source and target nodes.\n",
    "        z_src = z[edge_index[0]]\n",
    "        z_dst = z[edge_index[1]]\n",
    "        \n",
    "        # Concatenate the source and destination latent vectors.\n",
    "        z_cat = torch.cat([z_src, z_dst], dim=1)\n",
    "        \n",
    "        h1 = F.relu(self.fc1(z_cat))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        out = self.fc3(h2)\n",
    "        \n",
    "        return out.squeeze()  # shape: (num_edges,)\n",
    "\n",
    "class NodeDecoderMLP(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=64, out_dim=1):\n",
    "        super(NodeDecoderMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        h1 = F.relu(self.fc1( z ))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        out = self.fc3(h2)\n",
    "\n",
    "        return out.squeeze()  # shape: (num_nodes,)\n",
    "\n",
    "###############################\n",
    "# 4. Full VGAE Model (Generative Network)\n",
    "###############################\n",
    "class CustomVGAE(nn.Module):\n",
    "    def __init__(self, encoder, edge_decoder, node_decoder, kl_weight=1e-3):\n",
    "        super(CustomVGAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.edge_decoder = edge_decoder\n",
    "        self.node_decoder = node_decoder\n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "    def encode(self, x, edge_index, edge_weight):\n",
    "        mu, logvar = self.encoder(x, edge_index, edge_weight)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Clamp logvar to avoid extreme values.\n",
    "#         logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        std = torch.exp(0.5 * logvar) + 1e-8\n",
    "        if self.training:\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode_edges(self, z, edge_index):\n",
    "        return self.edge_decoder(z, edge_index)\n",
    "\n",
    "    def decode_nodes(self, z, edge_index):\n",
    "        return self.node_decoder(z, edge_index)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr.view(-1)\n",
    "        mu, logvar = self.encode(x, edge_index, edge_weight)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def compute_loss(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr.view(-1)\n",
    "        mu, logvar = self.encode(x, edge_index, edge_weight)\n",
    "        z = self.reparameterize( torch.nan_to_num(mu), torch.nan_to_num(logvar) )\n",
    "        # z = torch.nan_to_num(z)\n",
    "        # print(f\"Z shape: {z.shape} n/ Z: {z}\")\n",
    "        \n",
    "        # Decode edges and nodes.\n",
    "        pred_edge_weight = self.decode_edges(z, edge_index)\n",
    "        pred_node_features = self.decode_nodes(z, edge_index)\n",
    "        # print(f\"Pred weights: {pred_edge_weight}, pred nodes: {pred_node_features}\")\n",
    "        \n",
    "        # Reconstruction losses.\n",
    "        loss_edge = F.mse_loss(pred_edge_weight, edge_weight) + 1e-8\n",
    "        # print(f\"Edge loss: {loss_edge}\")\n",
    "        \n",
    "        # Data.x is of shape (num_nodes,1), so squeeze to match (num_nodes,)\n",
    "        loss_node = F.mse_loss(pred_node_features, x.squeeze()) + 1e-8\n",
    "        # print(f\"Node loss: {loss_node}\")\n",
    "        \n",
    "        # KL divergence loss, averaged over nodes.\n",
    "        kl_loss = -0.5 * torch.mean(torch.sum(1 + torch.nan_to_num(logvar) - torch.nan_to_num(mu).pow(2) - torch.exp(torch.nan_to_num(logvar)) + 1e-8, dim=1))\n",
    "        # print(f\"KL loss: {kl_loss}\")\n",
    "        \n",
    "        return loss_edge + loss_node + self.kl_weight * kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2be7aa4a-f7e2-4c36-9534-56eed48eb4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, Loss: 33.4938\n",
      "Epoch 002, Loss: 4.1251\n",
      "Epoch 003, Loss: 1.3073\n",
      "Epoch 004, Loss: 0.1684\n",
      "Epoch 005, Loss: 0.0506\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# 6. Setting Up DataLoader and Training\n",
    "###############################\n",
    "dataset = MLPGraphDatasetNeurons(\"models\")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Hyperparameters.\n",
    "in_channels = 1          # Each node feature is one scalar.\n",
    "hidden_channels = 1\n",
    "latent_dim = 512\n",
    "hidden_dim = 2048\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Instantiate components.\n",
    "encoder = GCNEncoderEdge(in_channels, hidden_channels, latent_dim)\n",
    "edge_decoder = EdgeDecoderMLP(latent_dim, hidden_dim=hidden_dim)\n",
    "node_decoder = NodeDecoderMLP(latent_dim, hidden_dim=hidden_dim, out_dim=in_channels)\n",
    "model = CustomVGAE(encoder, edge_decoder, node_decoder, kl_weight=1e-3)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "###############################\n",
    "# 7. Training Loop\n",
    "###############################\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.compute_loss(data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train()\n",
    "    print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1bbc41ca-bb01-4e88-8bc4-171bbe0a6f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGAE has 11548689 parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Function: Count the learnable params in a portion of the model\n",
    "    Args: model segment (ie, encoder, decoder, entire model)\n",
    "    Returns: the number of learnable params in that model/segment\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "print(f\"VGAE has {count_parameters(model)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d6069903-0097-4baf-b7cc-ecd467f773fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.8"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################\n",
    "# 8. Testing the Converted MLP on MNIST Data\n",
    "###############################\n",
    "# Assume you have a test_mlp function defined as:\n",
    "# def test_mlp(mlp, device, test_loader): ...\n",
    "#\n",
    "# After training, you can run the VGAE over one graph, decode, and convert the output to an MLP.\n",
    "# For example:\n",
    "#\n",
    "model.eval()\n",
    "data = dataset[0]  # take one example\n",
    "with torch.no_grad():\n",
    "  z, mu, logvar = model(data)\n",
    "  rec_edge = model.edge_decoder(z, data.edge_index)\n",
    "  rec_node = model.node_decoder(z, data.edge_index)\n",
    "\n",
    "# Build a new Data object with the reconstructed node features and edge attributes:\n",
    "rec_data = Data(x=rec_node.unsqueeze(1), edge_index=data.edge_index, edge_attr=rec_edge.unsqueeze(1))\n",
    "# Convert to an MLP.\n",
    "generated_mlp = vgae_to_mlp(rec_data)\n",
    "\n",
    "# Then, test with:\n",
    "test_mlp(generated_mlp, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "642e95d7-e003-4914-8106-1239218c2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "scores = []\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    data = dataset[n]  # take one example\n",
    "    with torch.no_grad():\n",
    "      z, mu, logvar = model(data)\n",
    "      rec_edge = model.edge_decoder(z, data.edge_index)\n",
    "      rec_node = model.node_decoder(z, data.edge_index)\n",
    "    \n",
    "    # Build a new Data object with the reconstructed node features and edge attributes:\n",
    "    rec_data = Data(x=rec_node.unsqueeze(1), edge_index=data.edge_index, edge_attr=rec_edge.unsqueeze(1))\n",
    "    # Convert to an MLP.\n",
    "    generated_mlp = vgae_to_mlp(rec_data)\n",
    "    \n",
    "    # Then, test with:\n",
    "    score = test_mlp(generated_mlp, test_loader)\n",
    "    scores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8f58e489-4604-470f-b4da-f8b211b26c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.8, 9.8, 9.8, 9.8, 9.8]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "5d1d1d7b-4255-4269-bce9-0c3d4ac6a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "674ed8bb-ab3f-493b-b185-46d48fd6c2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[8640, 1], edge_index=[2, 243712], edge_attr=[243712, 1], batch=[8640], ptr=[33])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd6fce-24aa-4b51-82e5-0b1caa1cdc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
